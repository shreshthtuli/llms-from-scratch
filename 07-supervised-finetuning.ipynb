{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Supervised Fine-Tuning\n",
    "\n",
    "> âš¡Compute Note: I recommend running this notebook on a node with 1x H200 GPU. \n",
    "\n",
    "This notebook fine-tunes the pretrained ShrayGPT model from notebook 06 into an instruction-following assistant using supervised fine-tuning (SFT). We will:\n",
    "\n",
    "1. Extend the `r50k_base` tokenizer with conversation-specific special tokens.\n",
    "2. Build an instruction dataset by blending Alpaca, Dolly v2, and UltraChat samples.\n",
    "3. Pack the conversations into fixed-length training chunks that match the model's block size.\n",
    "4. Fine-tune the base checkpoint with PyTorch Lightning and sample the resulting instruct model.\n",
    "\n",
    "SFT allows us to tune the model on Q&A style datasets, and make the responses more useful to a chatbot-like conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import interleave_datasets, load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n",
    "\n",
    "from src.shraygpt import ShrayGPT\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "model = ShrayGPT.load_from_checkpoint(\"checkpoints/shraygpt-base.ckpt\", map_location=\"cpu\")\n",
    "\n",
    "model.hparams.learning_rate_adamw = 1e-4\n",
    "model.hparams.learning_rate_muon = 5e-4\n",
    "model.hparams.aux_loss_weight = 5e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475c7c9",
   "metadata": {},
   "source": [
    "We shall use the Alpaca and Dolly SFT datasets from Huggingface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe0ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Alpaca Dataset Examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Give three tips for staying healthy.\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exerci\n",
      "\n",
      "--- Example 2 ---\n",
      "What are the three primary colors?\n",
      "The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB).\n",
      "\n",
      "--- Example 3 ---\n",
      "Describe the structure of an atom.\n",
      "An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\n",
      "\n",
      "The nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains mos\n",
      "\n",
      "--- Dolly Dataset Examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "When did Virgin Australia start operating?\n",
      "Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n",
      "\n",
      "--- Example 2 ---\n",
      "Which is a species of fish? Tope or Rope\n",
      "Tope\n",
      "\n",
      "--- Example 3 ---\n",
      "Why can camels survive for long without water?\n",
      "Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.\n"
     ]
    }
   ],
   "source": [
    "# Load a sample of the Alpaca dataset\n",
    "alpaca = load_dataset(\"yahma/alpaca-cleaned\", split='train')\n",
    "\n",
    "print(\"--- Alpaca Dataset Examples ---\")\n",
    "for i, example in enumerate(iter(alpaca.take(3))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(example['instruction'][:500])\n",
    "    print(example['output'][:500])\n",
    "\n",
    "# Load a sample of the Dolly datase\n",
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\", split='train')\n",
    "\n",
    "print(\"\\n--- Dolly Dataset Examples ---\")\n",
    "for i, example in enumerate(iter(dolly.take(3))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(example['instruction'][:500])\n",
    "    print(example['response'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3546d8",
   "metadata": {},
   "source": [
    "Now let's format the conversations and fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_PREFIXES = {\n",
    "    \"system\": 'System:',\n",
    "    \"user\": \"User:\",\n",
    "    \"assistant\": \"Assistant:\",\n",
    "}\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are ShrayGPT, a helpful and concise AI assistant.\"\n",
    "\n",
    "\n",
    "def format_conversation(messages: Sequence[Tuple[str, str]], default_system: str = DEFAULT_SYSTEM_PROMPT) -> Optional[str]:\n",
    "    '''Convert a list of (role, content) pairs into a single conversation string.'''\n",
    "    formatted: List[str] = []\n",
    "    has_system = any(role == \"system\" for role, _ in messages)\n",
    "    if not has_system:\n",
    "        formatted.append(f\"{ROLE_PREFIXES['system']} {default_system.strip()}\")\n",
    "\n",
    "    for role, content in messages:\n",
    "        prefix = ROLE_PREFIXES.get(role)\n",
    "        text = content.strip()\n",
    "        if prefix is None:\n",
    "            continue\n",
    "        formatted.append(f\"{prefix} {text}\")\n",
    "\n",
    "    if not formatted:\n",
    "        return None\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def preprocess_dataset(example: Dict[str, str]) -> Dict[str, Optional[str]]:\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example.get(\"input\", \"\").strip()\n",
    "    response = example[\"output\"].strip() if \"output\" in example else example[\"response\"].strip()\n",
    "\n",
    "    if input_text:\n",
    "        user_prompt = f\"{instruction}\\n\\n{input_text}\"\n",
    "    else:\n",
    "        user_prompt = instruction\n",
    "\n",
    "    conversation = [\n",
    "        (\"system\", DEFAULT_SYSTEM_PROMPT),\n",
    "        (\"user\", user_prompt),\n",
    "        (\"assistant\", response),\n",
    "    ]\n",
    "    return {\"text\": format_conversation(conversation)}\n",
    "\n",
    "alpaca_sft = alpaca.map(preprocess_dataset, remove_columns=alpaca.column_names)\n",
    "dolly_sft = dolly.map(preprocess_dataset, remove_columns=dolly.column_names)\n",
    "\n",
    "# Filter out any examples where text is None\n",
    "alpaca_sft = alpaca_sft.filter(lambda x: x[\"text\"] is not None)\n",
    "dolly_sft = dolly_sft.filter(lambda x: x[\"text\"] is not None)\n",
    "\n",
    "combined_sft = interleave_datasets([alpaca_sft, dolly_sft], probabilities=[0.5, 0.5], seed=42, stopping_strategy=\"all_exhausted\")\n",
    "train_test_sft = combined_sft.train_test_split(test_size=0.01, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "BLOCK_SIZE = 1024  # SFT can use a shorter context than the 06 pretraining run.\n",
    "PAD_ID = 0                     # or whichever id you want as pad\n",
    "LABEL_PAD_ID = -100            # CrossEntropy ignore_index\n",
    "\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, hf_dataset, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def _rank_world(self):\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            return dist.get_rank(), dist.get_world_size()\n",
    "        return 0, 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        rank, world = self._rank_world()\n",
    "        ds = self.hf_dataset\n",
    "        if hasattr(ds, \"shard\"):\n",
    "            try:\n",
    "                ds = ds.shard(num_shards=world, index=rank, contiguous=True)\n",
    "            except IndexError:\n",
    "                return\n",
    "\n",
    "        buffer = []\n",
    "        for item in ds:\n",
    "            if 'text' in item and item['text'] is not None:\n",
    "                toks = self.tokenizer.encode(item['text'])\n",
    "                # add an end marker if you like; otherwise omit the following line\n",
    "                # toks += [self.tokenizer.eot_token]\n",
    "                buffer.extend(toks)\n",
    "\n",
    "                # full blocks (no padding needed)\n",
    "                while len(buffer) >= self.block_size + 1:\n",
    "                    x = torch.tensor(buffer[:self.block_size], dtype=torch.long)\n",
    "                    y = torch.tensor(buffer[1:self.block_size+1], dtype=torch.long)\n",
    "                    yield x, y\n",
    "                    buffer = buffer[self.block_size:]\n",
    "\n",
    "        # tail: right-pad and ignore in loss\n",
    "        if len(buffer) > 1:\n",
    "            seq_len = min(self.block_size, len(buffer) - 1)  # number of *real* positions\n",
    "            x = torch.full((self.block_size,), PAD_ID, dtype=torch.long)\n",
    "            y = torch.full((self.block_size,), LABEL_PAD_ID, dtype=torch.long)  # ignore by default\n",
    "\n",
    "            # fill the real part\n",
    "            real_x = torch.tensor(buffer[:seq_len], dtype=torch.long)\n",
    "            real_y = torch.tensor(buffer[1:seq_len+1], dtype=torch.long)\n",
    "\n",
    "            x[:seq_len] = real_x\n",
    "            y[:seq_len] = real_y\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "train_dataset_full = IterableTextDataset(tokenizer, train_test_sft.get(\"train\"), BLOCK_SIZE)\n",
    "val_dataset_full = IterableTextDataset(tokenizer, train_test_sft.get(\"test\"), BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_full, \n",
    "    batch_size=14, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    drop_last=True,\n",
    "    pin_memory=True # Helps speed up data transfer to the GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_full, \n",
    "    batch_size=14, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    drop_last=True,\n",
    "    pin_memory=True # Helps speed up data transfer to the GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e51557",
   "metadata": {},
   "source": [
    "Now let's tune the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HF_API_TOKEN\")) \n",
    "\n",
    "class GenerateTextCallback(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to generate text samples at the end of each validation epoch.\"\"\"\n",
    "    def __init__(self, prompts, tokenizer, every_n_steps=100):\n",
    "        super().__init__()\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "        if not trainer.is_global_zero:\n",
    "            return  # only rank 0 prints/logs text\n",
    "        pl_module.print(f\"\\n\\n--- Generating text at step {trainer.global_step} ---\")\n",
    "        tb = getattr(trainer.logger, \"experiment\", None)\n",
    "        \n",
    "        for i, prompt in enumerate(self.prompts):\n",
    "            formatted_prompt = format_conversation([\n",
    "                                    (\"system\", DEFAULT_SYSTEM_PROMPT),\n",
    "                                    (\"user\", prompt),\n",
    "                                    (\"assistant\", \"\"),\n",
    "                                ])\n",
    "            start_tokens = self.tokenizer.encode(formatted_prompt)\n",
    "            context = torch.tensor(start_tokens, dtype=torch.long, device=pl_module.device).unsqueeze(0)\n",
    "            generated_tokens = pl_module.generate_nocache(context, max_new_tokens=100, temperature=0.7, top_k=20)\n",
    "            generated_text = self.tokenizer.decode(generated_tokens[0].tolist())\n",
    "            pl_module.print(f\"PROMPT: '{prompt}'\")\n",
    "            pl_module.print(f\"GENERATED: {generated_text}\\n\")\n",
    "            if tb is not None and hasattr(tb, \"add_text\"):\n",
    "                tb.add_text(f\"samples/prompt_{i}\", f\"**Prompt:** {prompt}\\n\\n**Generated:** {generated_text}\",\n",
    "                            global_step=trainer.global_step)\n",
    "\n",
    "callback = GenerateTextCallback(prompts=[\"What are the primary colors?\", \"What is 1 + 1?\", \"What is the color of the sky?\"], \n",
    "    tokenizer=tokenizer, every_n_steps=1000)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"shraygpt-{epoch:02d}-{step:05d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = L.Trainer(max_steps=200_000, accelerator='auto', devices=1, precision='bf16-mixed', strategy='auto', \n",
    "                    num_sanity_val_steps=0, limit_train_batches=1000, limit_val_batches=100,\n",
    "                    callbacks=[callback, checkpoint_cb, lr_monitor],\n",
    "                    logger=L.pytorch.loggers.TensorBoardLogger(\"logs/\"), log_every_n_steps=1) \n",
    "\n",
    "model.automatic_optimization = False\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2b26e",
   "metadata": {},
   "source": [
    "The training curve is shown here.\n",
    "\n",
    "![image.png](assets/sft.png)\n",
    "\n",
    "Now let's see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f5972c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are ShrayGPT, a helpful and concise AI assistant.\n",
      "\n",
      "User: Describe ways to reduce air pollution.\n",
      "\n",
      "Assistant: \n",
      "1. Encouraging the use of public transportation: By reducing the number of vehicles on the road, you can significantly reduce the number of vehicles on the road, which in turn reduces pollution caused by harmful emissions.\n",
      "\n",
      "2. Being more responsible: Consider the impact of measures that need to be taken to reduce air pollution, such as green spaces, fresh air, and renewable sources of energy such as wind, solar, and hydro power.\n",
      "\n",
      "3. Use public transportation: In addition\n"
     ]
    }
   ],
   "source": [
    "model = ShrayGPT.load_from_checkpoint(\"checkpoints/shraygpt-instruct.ckpt\")\n",
    "prompt = \"Describe ways to reduce air pollution.\"\n",
    "formatted_prompt = format_conversation([\n",
    "                        (\"system\", DEFAULT_SYSTEM_PROMPT),\n",
    "                        (\"user\", prompt),\n",
    "                        (\"assistant\", \"\"),\n",
    "                    ])\n",
    "start_tokens = tokenizer.encode(formatted_prompt)\n",
    "context = torch.tensor(start_tokens, dtype=torch.long, device=model.device).unsqueeze(0)\n",
    "generated_tokens = model.generate_nocache(context, max_new_tokens=100, temperature=0.7, top_k=20)\n",
    "generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558425f",
   "metadata": {},
   "source": [
    "Pretty neat!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
