{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Supervised Fine-Tuning\n",
    "\n",
    "This notebook fine-tunes the pretrained ShrayGPT model from notebook 06 into an instruction-following assistant using supervised fine-tuning (SFT). We will:\n",
    "\n",
    "1. Extend the `r50k_base` tokenizer with conversation-specific special tokens.\n",
    "2. Build an instruction dataset by blending Alpaca, Dolly v2, and UltraChat samples.\n",
    "3. Pack the conversations into fixed-length training chunks that match the model's block size.\n",
    "4. Fine-tune the base checkpoint with PyTorch Lightning and sample the resulting instruct model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import interleave_datasets, load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tiktoken\n",
    "\n",
    "from src.shraygpt import ShrayGPT\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "SPECIAL_TOKENS = [\"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|end|>\"]\n",
    "\n",
    "# Extend the GPT-2 tokenizer with chat-specific tokens used in the SFT recipes below.\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "tokenizer = tokenizer.merge_special_tokens(SPECIAL_TOKENS)\n",
    "ALLOWED_SPECIAL = set(SPECIAL_TOKENS)\n",
    "SPECIAL_TOKEN_IDS: Dict[str, int] = {\n",
    "    tok: tokenizer.encode(tok, allowed_special={tok})[0] for tok in SPECIAL_TOKENS\n",
    "}\n",
    "\n",
    "print(f\"Tokenizer vocab size after adding specials: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_CHECKPOINT_PATH = \"checkpoints/shraygpt-pretrain.ckpt\"\n",
    "\n",
    "BASE_CONFIG = dict(\n",
    "    vocab_size=tokenizer.n_vocab,\n",
    "    block_size=4096,\n",
    "    d_model=1024,\n",
    "    n_head=16,\n",
    "    d_head=64,\n",
    "    n_layers=16,\n",
    "    num_experts=4,\n",
    "    num_experts_per_tok=2,\n",
    "    dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "def load_base_model() -> ShrayGPT:\n",
    "    '''Load the pretrained checkpoint from notebook 06 or fall back to a fresh instance.'''\n",
    "    try:\n",
    "        model = ShrayGPT.load_from_checkpoint(BASE_CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "        print(f\"Loaded checkpoint from {BASE_CHECKPOINT_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Checkpoint not found. Initializing a fresh model with base configuration.\")\n",
    "        model = ShrayGPT(**BASE_CONFIG)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resize_token_embeddings(model: ShrayGPT, new_vocab_size: int) -> None:\n",
    "    '''Resize token embeddings and LM head when new special tokens are added.'''\n",
    "    old_vocab_size = model.tok_emb.weight.size(0)\n",
    "    if new_vocab_size <= old_vocab_size:\n",
    "        return\n",
    "\n",
    "    device = model.tok_emb.weight.device\n",
    "    d_model = model.tok_emb.weight.size(1)\n",
    "\n",
    "    new_tok_emb = nn.Embedding(new_vocab_size, d_model, device=device)\n",
    "    new_head = nn.Linear(d_model, new_vocab_size, bias=False, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        new_tok_emb.weight[:old_vocab_size] = model.tok_emb.weight\n",
    "        new_head.weight[:old_vocab_size] = model.head.weight\n",
    "        nn.init.normal_(new_tok_emb.weight[old_vocab_size:], mean=0.0, std=0.02)\n",
    "        nn.init.normal_(new_head.weight[old_vocab_size:], mean=0.0, std=0.02)\n",
    "\n",
    "    model.tok_emb = new_tok_emb\n",
    "    model.head = new_head\n",
    "    print(f\"Resized embeddings from {old_vocab_size} to {new_vocab_size}\")\n",
    "\n",
    "\n",
    "model = load_base_model()\n",
    "resize_token_embeddings(model, tokenizer.n_vocab)\n",
    "\n",
    "# Lower learning rates for the instruct tuning phase and keep manual optimization.\n",
    "model.hparams.learning_rate_adamw = 1e-4\n",
    "model.hparams.learning_rate_muon = 5e-4\n",
    "model.hparams.aux_loss_weight = 5e-4\n",
    "model.automatic_optimization = False"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "SYSTEM_TOKEN = \"<|system|>\"\n",
    "USER_TOKEN = \"<|user|>\"\n",
    "ASSISTANT_TOKEN = \"<|assistant|>\"\n",
    "END_TOKEN = \"<|end|>\"\n",
    "ROLE_TOKENS = {\n",
    "    \"system\": SYSTEM_TOKEN,\n",
    "    \"user\": USER_TOKEN,\n",
    "    \"assistant\": ASSISTANT_TOKEN,\n",
    "}\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are ShrayGPT, a helpful and concise AI assistant.\"\n",
    "\n",
    "\n",
    "def format_conversation(messages: Sequence[Tuple[str, str]], default_system: str = DEFAULT_SYSTEM_PROMPT) -> Optional[str]:\n",
    "    '''Convert a list of (role, content) pairs into a single conversation string.'''\n",
    "    formatted: List[str] = []\n",
    "    has_system = any(role == \"system\" for role, _ in messages)\n",
    "    if not has_system:\n",
    "        formatted.append(f\"{SYSTEM_TOKEN}{default_system.strip()}{END_TOKEN}\")\n",
    "\n",
    "    for role, content in messages:\n",
    "        token = ROLE_TOKENS.get(role)\n",
    "        text = content.strip()\n",
    "        if token is None or not text:\n",
    "            continue\n",
    "        formatted.append(f\"{token}{text}{END_TOKEN}\")\n",
    "\n",
    "    if not formatted:\n",
    "        return None\n",
    "    return \"\".join(formatted)\n",
    "\n",
    "\n",
    "def preprocess_alpaca(example: Dict[str, str]) -> Dict[str, Optional[str]]:\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    input_text = example.get(\"input\", \"\").strip()\n",
    "    response = example[\"output\"].strip()\n",
    "\n",
    "    if input_text:\n",
    "        user_prompt = f\"{instruction}\n",
    "\n",
    "{input_text}\"\n",
    "    else:\n",
    "        user_prompt = instruction\n",
    "\n",
    "    conversation = [\n",
    "        (\"system\", DEFAULT_SYSTEM_PROMPT),\n",
    "        (\"user\", user_prompt),\n",
    "        (\"assistant\", response),\n",
    "    ]\n",
    "    return {\"text\": format_conversation(conversation)}\n",
    "\n",
    "\n",
    "def preprocess_dolly(example: Dict[str, str]) -> Dict[str, Optional[str]]:\n",
    "    instruction = example[\"instruction\"].strip()\n",
    "    context = example.get(\"context\", \"\").strip()\n",
    "    response = example[\"response\"].strip()\n",
    "\n",
    "    if context:\n",
    "        user_prompt = f\"{instruction}\n",
    "\n",
    "Context:\n",
    "{context}\"\n",
    "    else:\n",
    "        user_prompt = instruction\n",
    "\n",
    "    conversation = [\n",
    "        (\"system\", DEFAULT_SYSTEM_PROMPT),\n",
    "        (\"user\", user_prompt),\n",
    "        (\"assistant\", response),\n",
    "    ]\n",
    "    return {\"text\": format_conversation(conversation)}\n",
    "\n",
    "\n",
    "def preprocess_ultrachat(example: Dict[str, List[Dict[str, str]]]) -> Dict[str, Optional[str]]:\n",
    "    messages = []\n",
    "    for message in example[\"messages\"]:\n",
    "        role = message.get(\"role\")\n",
    "        content = message.get(\"content\", \"\").strip()\n",
    "        if role not in ROLE_TOKENS or not content:\n",
    "            continue\n",
    "        messages.append((role, content))\n",
    "\n",
    "    if not messages or messages[-1][0] != \"assistant\":\n",
    "        return {\"text\": None}\n",
    "\n",
    "    return {\"text\": format_conversation(messages)}\n",
    "\n",
    "\n",
    "alpaca = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "ultrachat = load_dataset(\"HuggingFaceH4/ultrachat_200k\", \"messages\", split=\"train_sft\")\n",
    "\n",
    "alpaca_sft = alpaca.map(preprocess_alpaca, remove_columns=alpaca.column_names)\n",
    "dolly_sft = dolly.map(preprocess_dolly, remove_columns=dolly.column_names)\n",
    "ultrachat_sft = ultrachat.map(preprocess_ultrachat, remove_columns=ultrachat.column_names)\n",
    "\n",
    "alpaca_sft = alpaca_sft.filter(lambda ex: ex[\"text\"] is not None)\n",
    "dolly_sft = dolly_sft.filter(lambda ex: ex[\"text\"] is not None)\n",
    "ultrachat_sft = ultrachat_sft.filter(lambda ex: ex[\"text\"] is not None)\n",
    "\n",
    "combined = interleave_datasets(\n",
    "    [alpaca_sft, dolly_sft, ultrachat_sft],\n",
    "    probabilities=[0.4, 0.3, 0.3],\n",
    "    seed=42,\n",
    ")\n",
    "train_val = combined.train_test_split(test_size=0.02, seed=42)\n",
    "train_texts = train_val[\"train\"].shuffle(seed=42)\n",
    "val_texts = train_val[\"test\"].shuffle(seed=1234)\n",
    "\n",
    "MAX_TRAIN = 20000\n",
    "MAX_VAL = 512\n",
    "if train_texts.num_rows > MAX_TRAIN:\n",
    "    train_texts = train_texts.select(range(MAX_TRAIN))\n",
    "if val_texts.num_rows > MAX_VAL:\n",
    "    val_texts = val_texts.select(range(MAX_VAL))\n",
    "\n",
    "print(f\"Training examples: {train_texts.num_rows}\")\n",
    "print(f\"Validation examples: {val_texts.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 1024  # SFT can use a shorter context than the 06 pretraining run.\n",
    "END_ID = SPECIAL_TOKEN_IDS[END_TOKEN]\n",
    "\n",
    "\n",
    "def tokenize_texts(texts: Iterable[str]) -> List[List[int]]:\n",
    "    sequences: List[List[int]] = []\n",
    "    for text in texts:\n",
    "        ids = tokenizer.encode(text, allowed_special=ALLOWED_SPECIAL)\n",
    "        if not ids:\n",
    "            continue\n",
    "        if ids[-1] != END_ID:\n",
    "            ids.append(END_ID)\n",
    "        sequences.append(ids)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def pack_sequences(sequences: Sequence[Sequence[int]], block_size: int) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    packed: List[Tuple[torch.Tensor, torch.Tensor]] = []\n",
    "    buffer: List[int] = []\n",
    "    for seq in sequences:\n",
    "        buffer.extend(seq)\n",
    "        while len(buffer) >= block_size + 1:\n",
    "            chunk = buffer[: block_size + 1]\n",
    "            x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "            y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "            packed.append((x, y))\n",
    "            buffer = buffer[block_size:]\n",
    "    return packed\n",
    "\n",
    "\n",
    "train_sequences = tokenize_texts(train_texts[\"text\"])\n",
    "val_sequences = tokenize_texts(val_texts[\"text\"])\n",
    "\n",
    "train_pairs = pack_sequences(train_sequences, BLOCK_SIZE)\n",
    "val_pairs = pack_sequences(val_sequences, BLOCK_SIZE)\n",
    "\n",
    "random.shuffle(train_pairs)\n",
    "\n",
    "\n",
    "class PackedPairDataset(Dataset):\n",
    "    def __init__(self, pairs: Sequence[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        self.pairs = list(pairs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.pairs[idx]\n",
    "\n",
    "\n",
    "train_dataset = PackedPairDataset(train_pairs)\n",
    "val_dataset = PackedPairDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"{len(train_dataset)} training chunks, {len(val_dataset)} validation chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "\n",
    "class GenerateAfterValidation(L.Callback):\n",
    "    def __init__(self, prompts: Sequence[str], tokenizer: tiktoken.Encoding, every_n_steps: int = 200):\n",
    "        super().__init__()\n",
    "        self.prompts = list(prompts)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "        if not trainer.is_global_zero:\n",
    "            return\n",
    "        for prompt in self.prompts:\n",
    "            response = chat(model=pl_module, tokenizer=self.tokenizer, prompt=prompt)\n",
    "            pl_module.print(f\"Prompt: {prompt}\n",
    "Response: {response}\n",
    "\")\n",
    "\n",
    "\n",
    "def chat(model: ShrayGPT, tokenizer: tiktoken.Encoding, prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "         max_new_tokens: int = 256, temperature: float = 0.7, top_k: Optional[int] = 50) -> str:\n",
    "    prefix = (\n",
    "        f\"{SYSTEM_TOKEN}{system_prompt}{END_TOKEN}\"\n",
    "        f\"{USER_TOKEN}{prompt.strip()}{END_TOKEN}\"\n",
    "        f\"{ASSISTANT_TOKEN}\"\n",
    "    )\n",
    "    prompt_tokens = tokenizer.encode(prefix, allowed_special=ALLOWED_SPECIAL)\n",
    "    prompt_tokens = prompt_tokens[-model.block_size:]\n",
    "    context = torch.tensor(prompt_tokens, dtype=torch.long, device=model.device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate_nocache(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "\n",
    "    generated_ids = generated[0].tolist()\n",
    "    new_tokens = generated_ids[len(prompt_tokens):]\n",
    "    text = tokenizer.decode(new_tokens)\n",
    "    if END_TOKEN in text:\n",
    "        text = text.split(END_TOKEN)[0]\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"shraygpt-sft-{epoch:02d}-{step:05d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=2,\n",
    "    save_last=True,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "generate_cb = GenerateAfterValidation(\n",
    "    prompts=[\n",
    "        \"Summarize the Water Cycle in two sentences.\",\n",
    "        \"Write a short Python function that checks if a string is a palindrome.\",\n",
    "    ],\n",
    "    tokenizer=tokenizer,\n",
    "    every_n_steps=200,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_steps=2000,\n",
    "    accumulate_grad_batches=4,\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"bf16-mixed\" if torch.cuda.is_available() else \"32-true\",\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[checkpoint_cb, lr_monitor, generate_cb],\n",
    "    limit_val_batches=50,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate a response with the tuned model once training has completed.\n",
    "example_prompt = \"Explain how to write a unit test in Python using pytest.\"\n",
    "print(chat(model, tokenizer, example_prompt))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}