{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate code to run trained models\n",
    "\n",
    "> âš¡Compute Note: I recommend running this notebook on a node with 1x H200 GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "from src.shraygpt import ShrayGPT\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "R0_CHECKPOINT_PATH = \"checkpoints/shraygpt-r0.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShrayGPT(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x Block(\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "      (mha): CausalSelfAttentionMLA(\n",
       "        (wq): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (wk_lat): Linear(in_features=1024, out_features=64, bias=False)\n",
       "        (wv_lat): Linear(in_features=1024, out_features=64, bias=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (moe): MoE(\n",
       "        (gate): Linear(in_features=1024, out_features=6, bias=False)\n",
       "        (experts): ModuleList(\n",
       "          (0-5): 6 x SwiGLU(\n",
       "            (w1): Linear(in_features=1024, out_features=2744, bias=False)\n",
       "            (w2): Linear(in_features=1024, out_features=2744, bias=False)\n",
       "            (w3): Linear(in_features=2744, out_features=1024, bias=False)\n",
       "            (act): SiLU()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (shared_expert): SwiGLU(\n",
       "          (w1): Linear(in_features=1024, out_features=2744, bias=False)\n",
       "          (w2): Linear(in_features=1024, out_features=2744, bias=False)\n",
       "          (w3): Linear(in_features=2744, out_features=1024, bias=False)\n",
       "          (act): SiLU()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): RMSNorm()\n",
       "  (head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "def generate_response(model: ShrayGPT, prompt: str, max_new_tokens: int) -> str:\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    context = torch.tensor(prompt_tokens, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate_nocache(\n",
    "            context,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.2,\n",
    "            top_k=20,\n",
    "        )\n",
    "        new_tokens = generated[0].tolist()[len(prompt_tokens):]\n",
    "    response = tokenizer.decode(new_tokens)\n",
    "    response, _, _ = response.partition(\"System:\")\n",
    "    response = response.replace(\"Assistant:\", \"\")\n",
    "    return response.strip()\n",
    "\n",
    "ckpt = torch.load(R0_CHECKPOINT_PATH, map_location=DEVICE)\n",
    "state = ckpt[\"state_dict\"]; hparams = ckpt.get(\"hyper_parameters\")\n",
    "model = ShrayGPT(**hparams)\n",
    "model.load_state_dict(state, strict=False)\n",
    "model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the depths of solitude,\n",
      "Where and for a journey of spirit,\n",
      "Are treasures upon the icy serenity\n",
      "Where the sun sets,\n",
      "The sorrow of the storm\n",
      "The wind that dies,\n",
      "So through the charm of life,\n",
      "So let us return to the place of civilization,\n",
      "With the bounties and grandeur,\n",
      "And listen to the voice of the ocean,\n",
      "For though the sea is only a drop by the breath of life,\n",
      "For sure cannot erase it,\n",
      "There's hope, the power of nature,\n",
      "So let the sun shine its sweetest wonder,\n",
      "For though the sea might not give it,\n",
      "For though the sea may yet remain dreary,\n",
      "And the mightiest enchanted serene,\n",
      "In comfort, of heart and small temples,\n",
      "And always, in harmony,\n",
      "Without warmth and light,\n",
      "And the night, though the light may not give,\n",
      "For though the sea may still seem empty,\n",
      "And though the sea is not chafing,\n",
      "It's a never respite, yet true.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(model, \"Write a short poem about the sea:\", 256)\n",
    "response, _, _ = response.partition(\"System:\")\n",
    "response = response.replace('Assistant:', '').strip()\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
