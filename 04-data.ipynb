{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83690eea",
   "metadata": {},
   "source": [
    "# 04: The Art and Science of Data for LLMs\n",
    "\n",
    "Welcome to the fourth part of our series! So far, we have built and optimized a GPT model from scratch. However, a powerful model architecture is only half of the equation. The other, arguably more important, half is the **data** it's trained on.\n",
    "\n",
    "The principle of \"garbage in, garbage out\" has never been more true than in the age of LLMs. The quality, diversity, and cleanliness of your training data directly determine your model's capabilities, its biases, and its failure modes. In this notebook, we will explore:\n",
    "\n",
    "1.  **What large-scale datasets look like**: We'll load a subset of a massive web-scraped dataset.\n",
    "2.  **Common data quality issues**: We'll inspect raw data to find boilerplate, code, and other artifacts.\n",
    "3.  **Filtering and cleaning techniques**: We'll discuss and implement simple heuristics to improve data quality.\n",
    "4.  **The impact of curation**: We'll compare our raw dataset to a highly-filtered one to see the difference.\n",
    "\n",
    "For this tutorial, we'll rely heavily on the ðŸ¤— `datasets` library, which is the standard for accessing and processing massive datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2c785",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First, let's install the necessary libraries. We need `datasets` to download and process our data, and `matplotlib` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7b5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRESHTH\\Desktop\\llms-from-scratch\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7724b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Set default figure size for plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d221e",
   "metadata": {},
   "source": [
    "### 1. Exploring a Raw Web Dataset: C4\n",
    "\n",
    "The **Colossal Cleaned Common Crawl (C4)** dataset was created by Google for training their T5 models. It's a massive scrape of the public internet, which has undergone some basic cleaning (like removing offensive words and deduplication). However, it's still considered relatively \"raw\" compared to more modern, heavily curated datasets.\n",
    "\n",
    "Let's load a small part of the C4 dataset to see what it looks like. We'll use `streaming=True` to avoid downloading the entire dataset, which is several terabytes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the C4 dataset in streaming mode\n",
    "c4_dataset = load_dataset(\"c4\", \"en\", streaming=True, split='train')\n",
    "\n",
    "# Let's look at the first few examples\n",
    "print(\"--- Raw C4 Dataset Examples ---\")\n",
    "for i, example in enumerate(iter(c4_dataset.take(5))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    # Print the first 500 characters of the text\n",
    "    print(example['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7966624",
   "metadata": {},
   "source": [
    "### 2. Identifying Data Quality Issues\n",
    "\n",
    "As you look through the examples above, you might notice some problems:\n",
    "- **Boilerplate Text**: Phrases like \"log in,\" \"terms of use,\" or cookie consent notices.\n",
    "- **Code and Markup**: Snippets of JavaScript, HTML, or CSS that are not natural language.\n",
    "- **Strange Formatting**: Excessive line breaks, weird characters, or garbled text.\n",
    "- **Non-Prose Content**: Lists, tables, or other structured data that doesn't read like a book.\n",
    "\n",
    "Training a model on this kind of data can teach it to generate undesirable content. Our goal is to filter the dataset to keep only high-quality, natural language prose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d206006b",
   "metadata": {},
   "source": [
    "### 3. Data Filtering Techniques\n",
    "\n",
    "Data filtering is a deep and complex field, but we can apply some simple yet powerful heuristics. Here are a few common ones:\n",
    "\n",
    "1.  **Length Filtering**: Remove documents that are too short or too long.\n",
    "2.  **Character Filtering**: Remove documents with a high percentage of non-alphanumeric characters.\n",
    "3.  **Boilerplate Removal**: Remove documents containing common web boilerplate phrases (e.g., \"JavaScript is disabled\").\n",
    "4.  **Repetition Removal**: Remove documents with highly repetitive lines or n-grams.\n",
    "\n",
    "Let's create a simple filtering function that combines a few of these ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52fa9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_high_quality(example):\n",
    "    \"\"\"A simple heuristic-based filter for data quality.\"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    # 1. Length filter\n",
    "    if len(text) < 200 or len(text) > 100000:\n",
    "        return False\n",
    "    \n",
    "    # 2. Boilerplate filter\n",
    "    boilerplate_phrases = [\n",
    "        \"terms of use\", \"privacy policy\", \"cookie policy\", \n",
    "        \"subscribe to our newsletter\", \"enable javascript\"\n",
    "    ]\n",
    "    if any(phrase in text.lower() for phrase in boilerplate_phrases):\n",
    "        return False\n",
    "        \n",
    "    # 3. Character filter (check for high proportion of non-alphanumeric chars)\n",
    "    # This can be a proxy for code or heavily formatted text\n",
    "    alphanumeric_chars = sum(c.isalnum() for c in text)\n",
    "    if alphanumeric_chars / len(text) < 0.75:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "# The .filter() method applies our function to each example\n",
    "filtered_c4 = c4_dataset.filter(is_high_quality)\n",
    "\n",
    "print(\"--- Filtered C4 Dataset Examples ---\")\n",
    "for i, example in enumerate(iter(filtered_c4.take(5))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(example['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f3836",
   "metadata": {},
   "source": [
    "While our simple filter helps, professional dataset creation involves much more sophisticated pipelines. Let's look at a dataset that has already undergone this process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc1df0",
   "metadata": {},
   "source": [
    "### 4. A Look at a Highly Curated Dataset: FineWeb\n",
    "\n",
    "**FineWeb**, created by the Hugging Face team, is a great example of a state-of-the-art, highly filtered dataset. It starts from Common Crawl but applies a rigorous filtering and deduplication pipeline, resulting in over 15 trillion tokens of high-quality text.\n",
    "\n",
    "Let's load a sample of FineWeb and compare it to the raw C4 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of the FineWeb dataset\n",
    "# We'll use a smaller 10B token sample for demonstration\n",
    "fineweb_dataset = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10B\", streaming=True, split='train')\n",
    "\n",
    "print(\"--- FineWeb Dataset Examples ---\")\n",
    "for i, example in enumerate(iter(fineweb_dataset.take(5))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(example['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8620b8",
   "metadata": {},
   "source": [
    "### Conclusion: Quality Over Quantity\n",
    "\n",
    "Comparing the raw C4 examples with the FineWeb examples, the difference is clear. The FineWeb text is much cleaner, reads more like natural prose, and is free of the distracting artifacts common in raw web data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-from-scratch (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
