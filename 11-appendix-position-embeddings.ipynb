{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Advanced Positional Embeddings for Long-Context Generalization\n",
    "\n",
    "This appendix extends the material from Chapters 01\u201310 by surveying three modern positional embedding strategies designed to enhance transformer performance on long contexts: **No Position Embedding (NoPE)**, **Rotary Position Embedding (RoPE)**, and **Yet another RoPE extension (YaRN)**. We cover the intuition, mathematical formulation, and practical considerations for each method, and we conclude with minimal reference implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Alternative Positional Embeddings\n",
    "\n",
    "Classical transformers rely on either learned absolute position embeddings or deterministic sinusoidal embeddings (Vaswani et al., 2017). These approaches bind each token to a unique absolute index, which limits extrapolation beyond the context window seen during training. Long-context applications\u2014retrieval-augmented generation, document understanding, and code completion\u2014demand inductive biases that extrapolate gracefully when sequences exceed training lengths.\n",
    "\n",
    "Modern position embedding schemes focus on either *relative* or *functionally continuous* encodings, preserving translational invariance or enabling smooth extension to longer contexts. Below we detail three representative methods that illustrate these principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. No Position Embedding (NoPE)\n",
    "\n",
    "NoPE (Press et al., 2021) removes **explicit** positional encodings and relies purely on learned attention biases. The key insight is that attention itself can capture sequential structure when the model is trained on tasks with causal masking and left-to-right decoding.\n",
    "\n",
    "### Theory\n",
    "For a standard transformer layer, the attention logits are\n",
    "\n",
    "$$\text{Attn}(Q, K) = \f",
    "rac{QK^\top}{\\sqrt{d_k}} + B,$$\n",
    "\n",
    "where $B$ optionally encodes relative position biases. In NoPE, there is no positional encoding added to the token embeddings. Instead, an untied learned bias term $b_{ij}$ is added directly to the attention logits:\n",
    "\n",
    "$$\text{Attn}(Q, K) = \f",
    "rac{QK^\top}{\\sqrt{d_k}} + b_{ij}.$$\n",
    "\n",
    "The bias matrix $b_{ij}$ depends only on the relative distance $(i - j)$, enabling extrapolation to longer contexts when combined with appropriate parameter sharing (e.g., ALiBi; Press et al., 2021).\n",
    "\n",
    "### Practical Considerations\n",
    "* Works best when paired with monotonic attention biases such as ALiBi.\n",
    "* Requires carefully initializing or regularizing biases to prevent degenerate solutions where positional information is lost.\n",
    "* Enables models to generalize beyond training lengths because the learned bias function can be evaluated at larger relative distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rotary Position Embedding (RoPE)\n",
    "\n",
    "RoPE (Su et al., 2021) introduces relative positions by rotating query and key vectors in complex space. Each dimension pair is treated as a complex number and rotated by an angle proportional to the token index.\n",
    "\n",
    "### Theory\n",
    "Given position $n$ and feature index $2i$, define frequencies $\theta_i = \\omega^{2i/d}$ with $\\omega$ as a base (commonly $10{,}000$). Represent a 2-D feature pair $(x_{2i}, x_{2i+1})$ as $z_i = x_{2i} + j x_{2i+1}$. RoPE applies a rotation:\n",
    "\n",
    "$$\text{RoPE}(z_i, n) = z_i \\cdot e^{j n \theta_i}.$$\n",
    "\n",
    "The rotated queries and keys yield attention logits:\n",
    "\n",
    "$$\text{Attn}(q_n, k_m) = \\Re\\left[ \text{RoPE}(q_n, n) \\cdot \\overline{\text{RoPE}(k_m, m)} \r",
    "ight].$$\n",
    "\n",
    "Because the attention depends on $n-m$, RoPE implicitly encodes relative positions. Moreover, extending to longer contexts only requires evaluating the rotation for larger $n$.\n",
    "\n",
    "### Practical Considerations\n",
    "* Maintains rotational invariance and preserves dot-product magnitudes.\n",
    "* Supports interpolation and extrapolation by rescaling angles (e.g., NTK-aware scaling).\n",
    "* Widely adopted in GPT-NeoX, LLaMA, and other open-source models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YaRN: Yet another RoPE extensioN\n",
    "\n",
    "YaRN (Peng et al., 2023) refines RoPE by combining extrapolation-friendly rescaling with interpolation for shorter contexts. It blends multiple rotation scales to reduce phase distortion when sequences exceed the training window.\n",
    "\n",
    "### Theory\n",
    "YaRN introduces two scaling factors: an *interpolation* factor $\u0007lpha$ applied within the training window and an *extrapolation* factor $\beta$ for longer contexts. For a position $n$, frequencies are scaled as\n",
    "\n",
    "$$\tilde{\theta}_i(n) = \begin{cases}\n",
    "\u0007lpha \theta_i, & n \\leq N_{\text{train}} \\\n",
    "\beta \theta_i, & n > N_{\text{train}}\n",
    "\\end{cases}$$\n",
    "\n",
    "and the rotations become\n",
    "\n",
    "$$\text{YaRN}(z_i, n) = z_i \\cdot e^{j n \tilde{\theta}_i(n)}.$$\n",
    "\n",
    "The method smoothly transitions between scales using interpolation weights, ensuring continuity at $N_{\text{train}}$. This approach stabilizes training while allowing evaluation on contexts far beyond what the model has seen.\n",
    "\n",
    "### Practical Considerations\n",
    "* Choose $(\u0007lpha, \beta)$ to preserve attention spectra (e.g., $\u0007lpha < 1$ for better fit on short contexts, $\beta > 1$ for long contexts).\n",
    "* Implementation can share the same kernel as RoPE with position-dependent scaling coefficients.\n",
    "* Demonstrated to extend LLaMA-2 from 4k to 128k tokens when combined with curriculum fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Implementations\n",
    "\n",
    "The following code snippets illustrate the mechanics of each approach using PyTorch. These are reference implementations for experimentation; production systems should use fused kernels for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_nope_bias(attn_scores, bias):\n",
    "    \"\"\"Add a relative bias matrix (NoPE-style).\n",
    "    attn_scores: (batch, heads, seq, seq)\n",
    "    bias: (seq, seq) where bias[i, j] depends on i - j\n",
    "    \"\"\"\n",
    "    return attn_scores + bias\n",
    "\n",
    "seq_len = 8\n",
    "distance = torch.arange(seq_len)[:, None] - torch.arange(seq_len)[None, :]\n",
    "bias = -torch.abs(distance).float()  # simple ALiBi-style slope\n",
    "scores = torch.zeros(1, 1, seq_len, seq_len)\n",
    "nope_scores = apply_nope_bias(scores, bias)\n",
    "nope_scores[0, 0, :3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def rotary_angles(dim, base=10000):\n",
    "    return torch.tensor([base ** (-2 * (i // 2) / dim) for i in range(dim)])\n",
    "\n",
    "def apply_rope(x, positions, base=10000):\n",
    "    dim = x.size(-1)\n",
    "    theta = rotary_angles(dim, base=base).to(x.device)\n",
    "    freqs = torch.einsum('n,d->nd', positions.float(), theta)\n",
    "    cos = torch.cos(freqs).unsqueeze(-1)\n",
    "    sin = torch.sin(freqs).unsqueeze(-1)\n",
    "    x_reshaped = x.view(*x.shape[:-1], dim // 2, 2)\n",
    "    x1, x2 = x_reshaped[..., 0], x_reshaped[..., 1]\n",
    "    rotated = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "    return rotated.view_as(x)\n",
    "\n",
    "q = torch.randn(1, 4, 64)  # (seq, head_dim)\n",
    "pos = torch.arange(q.size(0))\n",
    "rope_q = apply_rope(q, pos)\n",
    "rope_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_yarn(x, positions, base=10000, alpha=0.8, beta=1.2, train_len=2048):\n",
    "    dim = x.size(-1)\n",
    "    theta = rotary_angles(dim, base=base).to(x.device)\n",
    "    scale = torch.where(positions[:, None] <= train_len, alpha, beta)\n",
    "    freqs = torch.einsum('n,d->nd', positions.float(), theta) * scale\n",
    "    cos = torch.cos(freqs).unsqueeze(-1)\n",
    "    sin = torch.sin(freqs).unsqueeze(-1)\n",
    "    x_reshaped = x.view(*x.shape[:-1], dim // 2, 2)\n",
    "    x1, x2 = x_reshaped[..., 0], x_reshaped[..., 1]\n",
    "    rotated = torch.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "    return rotated.view_as(x)\n",
    "\n",
    "long_positions = torch.arange(4096)\n",
    "x = torch.randn(4096, 64)\n",
    "yarn_x = apply_yarn(x, long_positions)\n",
    "yarn_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Best Practices\n",
    "\n",
    "| Method | Core Idea | Strengths | Weaknesses | Typical Use Cases |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| NoPE | Remove explicit positional encodings; rely on biases | Simplicity, compatibility with ALiBi | Requires careful bias design; implicit structure | Autoregressive decoders, efficient inference |\n",
    "| RoPE | Complex rotations encoding relative positions | Smooth extrapolation, widely adopted | Requires even-dimensional head sizes | General-purpose LLMs (GPT-NeoX, LLaMA) |\n",
    "| YaRN | Scale RoPE frequencies for interpolation + extrapolation | Extends context without retraining from scratch | Additional hyperparameters, modest compute overhead | Long-context fine-tuning of pretrained RoPE models |\n",
    "\n",
    "**Implementation tips:**\n",
    "* Align head dimensions to multiples of 2 for RoPE/YaRN.\n",
    "* When extending context windows, adjust attention masking and KV-cache sizes accordingly.\n",
    "* Validate extrapolation empirically using synthetic tasks (e.g., copy or needle-in-a-haystack tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Press, I., Smith, N. A., & Levy, O. (2021). \"Train Short, Test Long: Attention with Linear Biases.\" *arXiv:2108.12409*.\n",
    "* Su, J., Lu, Y., Pan, S., & Wen, L. (2021). \"RoFormer: Enhanced Transformer with Rotary Position Embedding.\" *arXiv:2104.09864*.\n",
    "* Peng, B., et al. (2023). \"YaRN: Efficient Context Window Extension of Large Language Models.\" *arXiv:2309.00071*.\n",
    "* Vaswani, A., et al. (2017). \"Attention Is All You Need.\" *NeurIPS*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}