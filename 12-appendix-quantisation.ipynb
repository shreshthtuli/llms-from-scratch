{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12 (Appendix). Quantisation and Quantisation-Aware Training\n",
        "\n",
        "This tutorial-style appendix introduces the motivations, numerics, and tooling behind **model quantisation** with a focus on transformers. We provide conceptual explanations and concise PyTorch code snippets so you can reason about precision trade-offs without running large-scale LLM training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By the end of this appendix you should be able to:\n",
        "\n",
        "- Explain why quantisation is attractive for deployment and how it differs from pruning or distillation.\n",
        "- Describe the most common integer formats (e.g. int8, int4) and how scale/zero-point pairs represent floating-point ranges.\n",
        "- Compare post-training quantisation (PTQ) with quantisation-aware training (QAT).\n",
        "- Prototype a minimal PTQ and QAT workflow in PyTorch using toy data.\n",
        "\n",
        "An interesting video on quantisation is by [Julia Turc](https://youtu.be/qoQJq5UwV1c?si=LerRDQsX-N5mXbzY)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Why quantise large language models?\n",
        "\n",
        "Quantisation maps high-precision parameters and activations (usually 16-bit or 32-bit floats) to lower-precision integer representations. The key benefits are:\n",
        "\n",
        "- **Latency:** Integer matrix multiplications are faster on CPUs and accelerators that ship with dedicated int8/int4 instructions.\n",
        "- **Memory footprint:** Reducing precision from 16-bit to 8-bit halves the storage requirements for weights, activations, and gradients.\n",
        "- **Bandwidth:** Smaller tensors move more quickly across device memory hierarchies, improving throughput for autoregressive decoding.\n",
        "\n",
        "The trade-off is reduced representational fidelity. Effective quantisation strategies aim to minimise task degradation by carefully choosing calibration data, numeric formats, and training procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantisation versus other efficiency techniques\n",
        "\n",
        "| Technique | Core idea | Typical gain | Trade-offs |\n",
        "|-----------|-----------|--------------|------------|\n",
        "| Pruning | Remove parameters entirely | Smaller models, sparse compute | Requires sparse kernels, may harm accuracy |\n",
        "| Distillation | Train a smaller student on teacher outputs | Compact model with similar behaviour | Needs extra training, may miss rare behaviours |\n",
        "| Quantisation | Lower the precision of weights/activations | Faster + smaller with same architecture | Numerical noise can degrade quality |\n",
        "\n",
        "Quantisation composes well with pruning and distillation, but each solves a distinct optimisation problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantisation fundamentals\n",
        "\n",
        "A uniform affine quantiser represents a real value $x$ with:\n",
        "\n",
        "$$\\hat{x} = \\text{clip}\\left(\\left\\lfloor \\frac{x}{s} \\right\\rceil + z, q_{\\min}, q_{\\max} \\right),$$\n",
        "\n",
        "where $s$ is the **scale**, $z$ is the **zero-point**, and $q_{\\min}, q_{\\max}$ bound the integer range (e.g. $[-128, 127]$ for signed int8).\n",
        "\n",
        "To recover an approximate float, we dequantise via:\n",
        "\n",
        "$$x \\approx s \\cdot (\\hat{x} - z).$$\n",
        "\n",
        "Two practical choices dominate transformer quantisation:\n",
        "\n",
        "- **Per-tensor vs. per-channel scales:** LayerNorm and attention projections often benefit from per-channel scaling to preserve dynamic ranges across attention heads.\n",
        "- **Symmetric vs. asymmetric:** Symmetric quantisers fix $z = 0$ and work well when data are zero-centred (common for weights). Asymmetric quantisers add a zero-point to better capture skewed activation distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing bit-widths\n",
        "\n",
        "| Bit-width | Common usage | Notes |\n",
        "|-----------|--------------|-------|\n",
        "| 16-bit (FP16/BF16/FP8) | Training and mixed-precision inference | Floating formats preserve wide dynamic range. |\n",
        "| 8-bit int | Widely adopted for weights + activations | Best hardware support across CPU/GPU/TPU. |\n",
        "| 4-bit int | Aggressive compression for decoder-only LLMs | Needs more careful calibration, often weight-only. |\n",
        "| 2-bit / Binary | Specialised research | Requires custom kernels; accuracy drops are larger. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PyTorch quantisation toolchain overview\n",
        "\n",
        "PyTorch (since 2.0) exposes quantisation APIs under `torch.ao.quantization`. The typical workflow is:\n",
        "\n",
        "1. **Define** a float32 model.\n",
        "2. **Prepare** it with a quantisation configuration (qconfig) that describes observer types and backend kernels.\n",
        "3. **Calibrate** or **train** to collect activation statistics.\n",
        "4. **Convert** to a quantised model with integer kernels.\n",
        "\n",
        "Depending on whether calibration happens after or during training we distinguish PTQ and QAT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Post-training quantisation (PTQ)\n",
        "\n",
        "PTQ transforms a pre-trained float model to a quantised one using a small calibration dataset. No gradient updates are required, making PTQ attractive when the original training data are unavailable or expensive to reproduce.\n",
        "\n",
        "Below we quantise a toy multilayer perceptron. Although tiny, the code mirrors what you would do with transformer blocks: prepare, calibrate, convert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training (just a few epochs for demo)...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "import torch.optim as optim\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool  = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.pool(self.relu2(self.conv2(x)))\n",
        "        # SAFE FLATTEN (works for channels-last too)\n",
        "        x = torch.flatten(x, 1)              # <-- changed from reshape\n",
        "        # or: x = x.contiguous().view(x.size(0), -1)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "model_fp32 = SimpleCNN()\n",
        "model_fp32.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_fp32.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Training (just a few epochs for demo)...\")\n",
        "for epoch in range(1):  # Just 1 epoch for time; increase for better accuracy!\n",
        "    model_fp32.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_fp32(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea75be03",
      "metadata": {},
      "source": [
        "Fusing Conv + ReLU (or Conv+BN+ReLU) is important as it combines them into one operation, improving both the accuracy and speed of quantized models. We do this before quantization.\n",
        "\n",
        "We then set the quantization config. It tells PyTorch how to observe and quantize the model. 'fbgemm' is preferred for x86 CPUs. For ARM CPUs, use 'qnnpack'.\n",
        "\n",
        "We then calibrate. This step feeds real data through the model. Observers collect min/max values to determine how to map floats to int8 (scale/zero-point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2d20e903",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibrating...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_316898/4183642918.py:9: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  torch.quantization.prepare(model_fp32, inplace=True)\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_316898/4183642918.py:18: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.convert(model_fp32, inplace=False)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SimpleCNN(\n",
              "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (conv1): QuantizedConvReLU2d(1, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.013325857929885387, zero_point=0, padding=(1, 1))\n",
              "  (relu1): Identity()\n",
              "  (conv2): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.02389773167669773, zero_point=0, padding=(1, 1))\n",
              "  (relu2): Identity()\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): QuantizedLinearReLU(in_features=6272, out_features=128, scale=0.18056835234165192, zero_point=0, qscheme=torch.per_channel_affine)\n",
              "  (relu3): Identity()\n",
              "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.2785327434539795, zero_point=60, qscheme=torch.per_channel_affine)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def fuse_model(model):\n",
        "    torch.quantization.fuse_modules(model,\n",
        "        [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3']], inplace=True)\n",
        "fuse_model(model_fp32)\n",
        "\n",
        "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "model_fp32.cpu()  # Quantization is CPU-only in PyTorch\n",
        "torch.quantization.prepare(model_fp32, inplace=True)\n",
        "\n",
        "print(\"Calibrating...\")\n",
        "model_fp32.eval()\n",
        "with torch.no_grad():\n",
        "    for images, _ in train_loader:\n",
        "        model_fp32(images)\n",
        "        break  # In practical cases, use more calibration data; here just a few for demo\n",
        "\n",
        "quantized_model = torch.quantization.convert(model_fp32, inplace=False)\n",
        "quantized_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a167cb4c",
      "metadata": {},
      "source": [
        "Let's see how this model performs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5040f196",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating original (float32) model:\n",
            "Accuracy: 97.84 %\n",
            "\n",
            "Evaluating quantized model:\n",
            "Accuracy: 97.78 %\n",
            "\n",
            "Model size (float32): 3209.8 KB\n",
            "Model size (quantized): 802.0 KB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def evaluate(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print('Accuracy:', 100.0 * correct / total, '%')\n",
        "    return correct / total\n",
        "\n",
        "print(\"\\nEvaluating original (float32) model:\")\n",
        "evaluate(model_fp32, test_loader)  \n",
        "print(\"\\nEvaluating quantized model:\")\n",
        "evaluate(quantized_model, test_loader)\n",
        "\n",
        "torch.save(model_fp32.state_dict(), \"float_model.pth\")\n",
        "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
        "float_size = os.path.getsize(\"float_model.pth\") / 1024\n",
        "quant_size = os.path.getsize(\"quantized_model.pth\") / 1024\n",
        "os.remove(\"float_model.pth\"); os.remove(\"quantized_model.pth\")\n",
        "\n",
        "print(f\"\\nModel size (float32): {float_size:.1f} KB\")\n",
        "print(f\"Model size (quantized): {quant_size:.1f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even without additional fine-tuning, int8 PTQ often introduces only mild accuracy degradation (only 0.1% here!) and significantly reduce model size (by 75% here!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quantisation-aware training (QAT)\n",
        "\n",
        "PTQ struggles when activation distributions shift significantly between calibration data and real workloads, or when you push to 4-bit precision. **Quantisation-aware training** mitigates this by simulating quantisation effects during fine-tuning.\n",
        "\n",
        "The high-level idea:\n",
        "\n",
        "1. Insert fake-quantisation modules that emulate quant/dequant in the forward pass.\n",
        "2. Backpropagate through straight-through estimators so gradients flow to float weights.\n",
        "3. After training, convert to real integer kernels.\n",
        "\n",
        "Below we fine-tune our toy network for a few gradient steps with QAT. Instead of optimising a real task, we match a random target tensor just to illustrate the mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_316898/2062197539.py:39: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared_qat = prepare_qat_fx(fused_float, qconfig_mapping, example_inputs)\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1 done. Last batch loss: 0.0231\n",
            "\n",
            "Eval float (QAT-prepared) before convert:\n",
            "Accuracy: 98.19 %\n",
            "GraphModule(\n",
            "  (conv1): QuantizedConvReLU2d(1, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.015383397229015827, zero_point=0, padding=(1, 1))\n",
            "  (conv2): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.020312752574682236, zero_point=0, padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): QuantizedLinearReLU(in_features=6272, out_features=128, scale=0.34092840552330017, zero_point=0, qscheme=torch.per_channel_affine)\n",
            "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.26298820972442627, zero_point=112, qscheme=torch.per_channel_affine)\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x):\n",
            "    conv1_input_scale_0 = self.conv1_input_scale_0\n",
            "    conv1_input_zero_point_0 = self.conv1_input_zero_point_0\n",
            "    quantize_per_tensor = torch.quantize_per_tensor(x, conv1_input_scale_0, conv1_input_zero_point_0, torch.quint8);  x = conv1_input_scale_0 = conv1_input_zero_point_0 = None\n",
            "    conv1 = self.conv1(quantize_per_tensor);  quantize_per_tensor = None\n",
            "    conv2 = self.conv2(conv1);  conv1 = None\n",
            "    pool = self.pool(conv2);  conv2 = None\n",
            "    flatten = torch.flatten(pool, 1);  pool = None\n",
            "    fc1 = self.fc1(flatten);  flatten = None\n",
            "    fc2 = self.fc2(fc1);  fc1 = None\n",
            "    dequantize_6 = fc2.dequantize();  fc2 = None\n",
            "    return dequantize_6\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n",
            "\n",
            "Quantized int8 accuracy:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_316898/2062197539.py:76: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = convert_fx(prepared_qat.cpu())   # convert on CPU\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 98.25 %\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "import torchvision, torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.ao.quantization import get_default_qat_qconfig, QConfigMapping\n",
        "from torch.ao.quantization.quantize_fx import fuse_fx, prepare_qat_fx, convert_fx\n",
        "\n",
        "class SimpleCNNFX(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool  = nn.MaxPool2d(2, 2)        # -> 14x14\n",
        "        self.fc1   = nn.Linear(32 * 14 * 14, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2   = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        x = self.pool(self.relu2(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)               # safe flatten (channels-last OK)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "float_model = SimpleCNNFX().to(device)\n",
        "\n",
        "# Example input for FX tracing\n",
        "example_inputs = (torch.randn(1, 1, 28, 28),)\n",
        "\n",
        "# Fuse known patterns: (conv, relu), (linear, relu)\n",
        "# This modifies module structure to ConvReLU / LinearReLU where applicable\n",
        "fused_float = float_model.cpu()\n",
        "backend = \"fbgemm\"                                          # x86 backend\n",
        "qconfig = get_default_qat_qconfig(backend)\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "\n",
        "prepared_qat = prepare_qat_fx(fused_float, qconfig_mapping, example_inputs)\n",
        "prepared_qat.to(device)\n",
        "prepared_qat.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(prepared_qat.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 1\n",
        "num_steps_freeze = 200    # after N steps disable observers & freeze BN (QAT best-practice)\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    prepared_qat.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = prepared_qat(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "        # Best practice: after a warmup, disable observers + freeze BN stats\n",
        "        if step == num_steps_freeze:\n",
        "            def _toggle(module):\n",
        "                if hasattr(module, 'apply_disable_observer'):\n",
        "                    module.apply(torch.ao.quantization.disable_observer)\n",
        "                if hasattr(module, 'apply_freeze_bn'):\n",
        "                    module.apply(torch.ao.quantization.freeze_bn_stats)\n",
        "            _toggle(prepared_qat)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} done. Last batch loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nEval float (QAT-prepared) before convert:\")\n",
        "_ = evaluate(prepared_qat, test_loader, device=device)\n",
        "\n",
        "prepared_qat.eval()\n",
        "quantized_model = convert_fx(prepared_qat.cpu())   # convert on CPU\n",
        "print(quantized_model)\n",
        "\n",
        "print(\"\\nQuantized int8 accuracy:\")\n",
        "_ = evaluate(quantized_model, test_loader, device='cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a real LLM fine-tuning session you would:\n",
        "\n",
        "- Start from a float checkpoint, insert QAT observers in linear/attention modules, and resume training with a small learning rate.\n",
        "- Use task-representative prompts so the model adapts to quantisation noise where it matters most (e.g. long-context decoding).\n",
        "- Optionally freeze embedding layers or layer norms if they destabilise under fake-quantisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical tips\n",
        "\n",
        "- **Gradient scaling:** Lower precisions amplify rounding error, so gradient clipping and slightly higher weight decay can stabilise QAT.\n",
        "- **Progressive bit-width schedules:** Begin with int8 fake quantisers and anneal to int4 once the model adapts.\n",
        "- **Layer-wise decisions:** Some components (e.g. logits projection) may remain in higher precision to safeguard perplexity.\n",
        "- **Evaluation:** Always compare quantised and float baselines on downstream metrics (perplexity, accuracy) and monitor decoding latency to ensure the trade-off is worthwhile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Beyond standard QAT\n",
        "\n",
        "Research is rapidly evolving:\n",
        "\n",
        "- **GPTQ/AWQ:** Optimise weight quantisation with second-order approximations or activation-aware scaling.\n",
        "- **LLM.int8():** Hybrid scheme keeping outlier activations in FP16 while quantising the rest.\n",
        "- **FP8 training/inference:** Uses floating-point formats with learned scaling, now supported on some GPUs.\n",
        "- **Structured sparsity + quantisation:** Combining 2:4 sparsity with int8 kernels for maximal throughput.\n",
        "\n",
        "These techniques build upon the PTQ/QAT primitives described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key takeaways\n",
        "\n",
        "- Quantisation trades numerical precision for latency and memory gains, making it crucial for serving LLMs on cost-sensitive hardware.\n",
        "- PTQ is simple and works well when you have decent calibration data and stick to 8-bit weights/activations.\n",
        "- QAT injects quantisation noise during training, enabling more aggressive bit-widths or challenging domains at the cost of additional fine-tuning.\n",
        "- PyTorch's `torch.ao.quantization` module provides batteries-included utilities to prepare, calibrate, and convert models, so you can experiment without rewriting your architecture."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
