{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix: Quantisation and Quantisation-Aware Training\n",
        "\n",
        "This tutorial-style appendix introduces the motivations, numerics, and tooling behind **model quantisation** with a focus on transformers. We provide conceptual explanations and concise PyTorch code snippets so you can reason about precision trade-offs without running large-scale LLM training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning goals\n",
        "\n",
        "By the end of this appendix you should be able to:\n",
        "\n",
        "- Explain why quantisation is attractive for deployment and how it differs from pruning or distillation.\n",
        "- Describe the most common integer formats (e.g. int8, int4) and how scale/zero-point pairs represent floating-point ranges.\n",
        "- Compare post-training quantisation (PTQ) with quantisation-aware training (QAT).\n",
        "- Prototype a minimal PTQ and QAT workflow in PyTorch using toy data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Why quantise large language models?\n",
        "\n",
        "Quantisation maps high-precision parameters and activations (usually 16-bit or 32-bit floats) to lower-precision integer representations. The key benefits are:\n",
        "\n",
        "- **Latency:** Integer matrix multiplications are faster on CPUs and accelerators that ship with dedicated int8/int4 instructions.\n",
        "- **Memory footprint:** Reducing precision from 16-bit to 8-bit halves the storage requirements for weights, activations, and gradients.\n",
        "- **Bandwidth:** Smaller tensors move more quickly across device memory hierarchies, improving throughput for autoregressive decoding.\n",
        "\n",
        "The trade-off is reduced representational fidelity. Effective quantisation strategies aim to minimise task degradation by carefully choosing calibration data, numeric formats, and training procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantisation versus other efficiency techniques\n",
        "\n",
        "| Technique | Core idea | Typical gain | Trade-offs |\n",
        "|-----------|-----------|--------------|------------|\n",
        "| Pruning | Remove parameters entirely | Smaller models, sparse compute | Requires sparse kernels, may harm accuracy |\n",
        "| Distillation | Train a smaller student on teacher outputs | Compact model with similar behaviour | Needs extra training, may miss rare behaviours |\n",
        "| Quantisation | Lower the precision of weights/activations | Faster + smaller with same architecture | Numerical noise can degrade quality |\n",
        "\n",
        "Quantisation composes well with pruning and distillation, but each solves a distinct optimisation problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantisation fundamentals\n",
        "\n",
        "A uniform affine quantiser represents a real value $x$ with:\n",
        "\n",
        "$$\\hat{x} = \\text{clip}\\left(\\left\\lfloor \\frac{x}{s} \\right\\rceil + z, q_{\\min}, q_{\\max} \\right),$$\n",
        "\n",
        "where $s$ is the **scale**, $z$ is the **zero-point**, and $q_{\\min}, q_{\\max}$ bound the integer range (e.g. $[-128, 127]$ for signed int8).\n",
        "\n",
        "To recover an approximate float, we dequantise via:\n",
        "\n",
        "$$x \\approx s \\cdot (\\hat{x} - z).$$\n",
        "\n",
        "Two practical choices dominate transformer quantisation:\n",
        "\n",
        "- **Per-tensor vs. per-channel scales:** LayerNorm and attention projections often benefit from per-channel scaling to preserve dynamic ranges across attention heads.\n",
        "- **Symmetric vs. asymmetric:** Symmetric quantisers fix $z = 0$ and work well when data are zero-centred (common for weights). Asymmetric quantisers add a zero-point to better capture skewed activation distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing bit-widths\n",
        "\n",
        "| Bit-width | Common usage | Notes |\n",
        "|-----------|--------------|-------|\n",
        "| 16-bit (FP16/BF16/FP8) | Training and mixed-precision inference | Floating formats preserve wide dynamic range. |\n",
        "| 8-bit int | Widely adopted for weights + activations | Best hardware support across CPU/GPU/TPU. |\n",
        "| 4-bit int | Aggressive compression for decoder-only LLMs | Needs more careful calibration, often weight-only. |\n",
        "| 2-bit / Binary | Specialised research | Requires custom kernels; accuracy drops are larger. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. PyTorch quantisation toolchain overview\n",
        "\n",
        "PyTorch (since 2.0) exposes quantisation APIs under `torch.ao.quantization`. The typical workflow is:\n",
        "\n",
        "1. **Define** a float32 model.\n",
        "2. **Prepare** it with a quantisation configuration (qconfig) that describes observer types and backend kernels.\n",
        "3. **Calibrate** or **train** to collect activation statistics.\n",
        "4. **Convert** to a quantised model with integer kernels.\n",
        "\n",
        "Depending on whether calibration happens after or during training we distinguish PTQ and QAT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Post-training quantisation (PTQ)\n",
        "\n",
        "PTQ transforms a pre-trained float model to a quantised one using a small calibration dataset. No gradient updates are required, making PTQ attractive when the original training data are unavailable or expensive to reproduce.\n",
        "\n",
        "Below we quantise a toy multilayer perceptron. Although tiny, the code mirrors what you would do with transformer blocks: prepare, calibrate, convert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# A toy float32 model (e.g. mimicking an MLP block in a transformer)\n",
        "float_model = nn.Sequential(\n",
        "    nn.Linear(16, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 16),\n",
        ")\n",
        "\n",
        "float_model.eval()\n",
        "example_inputs = torch.randn(64, 16)\n",
        "\n",
        "# Baseline inference\n",
        "with torch.inference_mode():\n",
        "    float_outputs = float_model(example_inputs)\n",
        "print(f\"Float model dtype: {float_outputs.dtype}, shape: {float_outputs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Configure PTQ using the fbgemm backend (appropriate for x86 CPUs)\n",
        "qconfig = torch.ao.quantization.get_default_qconfig(\"fbgemm\")\n",
        "prepared_model = torch.ao.quantization.prepare(float_model, qconfig)\n",
        "\n",
        "# Calibration pass: run a few batches to collect activation statistics\n",
        "with torch.inference_mode():\n",
        "    prepared_model(example_inputs)\n",
        "\n",
        "# Convert to a quantised model\n",
        "quantized_model = torch.ao.quantization.convert(prepared_model)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    quantized_outputs = quantized_model(example_inputs)\n",
        "\n",
        "print(quantized_model)\n",
        "print(f\"Quantised output dtype: {quantized_outputs.dtype}\")\n",
        "print(f\"Max absolute diff vs float32: {(float_outputs - quantized_outputs.float()).abs().max():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even without additional fine-tuning, int8 PTQ often introduces only mild accuracy degradation for decoder-only LLMs when you calibrate with a few hundred representative prompts. For activation-heavy modules (e.g. attention scores), per-channel observers and smooth quantisation transformations can further reduce error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quantisation-aware training (QAT)\n",
        "\n",
        "PTQ struggles when activation distributions shift significantly between calibration data and real workloads, or when you push to 4-bit precision. **Quantisation-aware training** mitigates this by simulating quantisation effects during fine-tuning.\n",
        "\n",
        "The high-level idea:\n",
        "\n",
        "1. Insert fake-quantisation modules that emulate quant/dequant in the forward pass.\n",
        "2. Backpropagate through straight-through estimators so gradients flow to float weights.\n",
        "3. After training, convert to real integer kernels.\n",
        "\n",
        "Below we fine-tune our toy network for a few gradient steps with QAT. Instead of optimising a real task, we match a random target tensor just to illustrate the mechanics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "qat_model = nn.Sequential(\n",
        "    nn.Linear(16, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 16),\n",
        ")\n",
        "\n",
        "qat_model.train()\n",
        "\n",
        "qat_qconfig = torch.ao.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
        "prepared_qat = torch.ao.quantization.prepare_qat(qat_model, qat_qconfig)\n",
        "\n",
        "optimizer = torch.optim.SGD(prepared_qat.parameters(), lr=1e-2)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for step in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    inputs = torch.randn(32, 16)\n",
        "    targets = torch.randn(32, 16)\n",
        "    outputs = prepared_qat(inputs)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step == 0 or (step + 1) % 2 == 0:\n",
        "        print(f\"Step {step+1}: loss={loss.item():.4f}\")\n",
        "\n",
        "prepared_qat.eval()\n",
        "quantized_qat_model = torch.ao.quantization.convert(prepared_qat)\n",
        "\n",
        "print(quantized_qat_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a real LLM fine-tuning session you would:\n",
        "\n",
        "- Start from a float checkpoint, insert QAT observers in linear/attention modules, and resume training with a small learning rate.\n",
        "- Use task-representative prompts so the model adapts to quantisation noise where it matters most (e.g. long-context decoding).\n",
        "- Optionally freeze embedding layers or layer norms if they destabilise under fake-quantisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical tips\n",
        "\n",
        "- **Gradient scaling:** Lower precisions amplify rounding error, so gradient clipping and slightly higher weight decay can stabilise QAT.\n",
        "- **Progressive bit-width schedules:** Begin with int8 fake quantisers and anneal to int4 once the model adapts.\n",
        "- **Layer-wise decisions:** Some components (e.g. logits projection) may remain in higher precision to safeguard perplexity.\n",
        "- **Evaluation:** Always compare quantised and float baselines on downstream metrics (perplexity, accuracy) and monitor decoding latency to ensure the trade-off is worthwhile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Beyond standard QAT\n",
        "\n",
        "Research is rapidly evolving:\n",
        "\n",
        "- **GPTQ/AWQ:** Optimise weight quantisation with second-order approximations or activation-aware scaling.\n",
        "- **LLM.int8():** Hybrid scheme keeping outlier activations in FP16 while quantising the rest.\n",
        "- **FP8 training/inference:** Uses floating-point formats with learned scaling, now supported on some GPUs.\n",
        "- **Structured sparsity + quantisation:** Combining 2:4 sparsity with int8 kernels for maximal throughput.\n",
        "\n",
        "These techniques build upon the PTQ/QAT primitives described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Key takeaways\n",
        "\n",
        "- Quantisation trades numerical precision for latency and memory gains, making it crucial for serving LLMs on cost-sensitive hardware.\n",
        "- PTQ is simple and works well when you have decent calibration data and stick to 8-bit weights/activations.\n",
        "- QAT injects quantisation noise during training, enabling more aggressive bit-widths or challenging domains at the cost of additional fine-tuning.\n",
        "- PyTorch's `torch.ao.quantization` module provides batteries-included utilities to prepare, calibrate, and convert models, so you can experiment without rewriting your architecture."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}