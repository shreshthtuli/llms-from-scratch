{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Pre-training\n",
    "\n",
    "The grand finale of all the ideas and concepts discussed thus far is pre-training a modern LLM on a pre-training dataset. We use the `r50k` tokenizer, use the advanced LLM we developed earlier and train it on the FineWeb-EDU dataset from HuggingFace. We use the learning rates we got from our scaling laws and pre-train the model. \n",
    "\n",
    "However, training an LLM is not as straightforward when you want to increase the number of parameters and use multiple GPUs. \n",
    "\n",
    "Let's start with creating a 1.5B parameter MoE model.\n",
    "I also very highly recommend running this notebook on a multi-GPU node, for instance 8xH200. Again, you can use Lightning AI studio for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1.58B\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from src.shraygpt import ShrayGPT\n",
    "torch.set_num_threads(1) # Prevents deadlocks with DataLoader and multiple workers\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "def get_total_param_count(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "d_model = 32*32\n",
    "n_head = 32\n",
    "d_head = 32\n",
    "n_layers = 24\n",
    "num_experts = 6\n",
    "num_experts_per_tok = 2\n",
    "block_size = 8192\n",
    "batch_size = 2\n",
    "\n",
    "model = ShrayGPT(\n",
    "    vocab_size=tokenizer.n_vocab, \n",
    "    block_size=block_size, \n",
    "    d_model=d_model,\n",
    "    n_head=n_head, \n",
    "    d_head=d_head, \n",
    "    n_layers=n_layers, \n",
    "    num_experts=num_experts, \n",
    "    num_experts_per_tok=num_experts_per_tok\n",
    ")\n",
    "# model = ShrayGPT.load_from_checkpoint(\"checkpoints/last-v2.ckpt\")\n",
    "model.hparams.learning_rate_adamw = 3e-4\n",
    "model.hparams.learning_rate_muon = 1.5e-3\n",
    "model.hparams.aux_loss_weight = 1e-2\n",
    "# model.compile(backend=\"inductor\", dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "params = get_total_param_count(model)\n",
    "print(f\"Total parameters: {params/1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a dataset and training and validation data loaders using the HuggingFace's Fineweb-EDU dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbc8479e01e4561973c2c09dbd1145b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3042a0fa284ceda69de4e62e6acb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, hf_dataset, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def _rank_world(self):\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            return dist.get_rank(), dist.get_world_size()\n",
    "        return 0, 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        rank, world = self._rank_world()\n",
    "\n",
    "        # Shard the HF streaming dataset so each rank reads a disjoint slice\n",
    "        ds = self.hf_dataset\n",
    "        if hasattr(ds, \"shard\"):\n",
    "            ds = ds.shard(num_shards=world, index=rank, contiguous=True)\n",
    "\n",
    "        buffer = []\n",
    "        for item in ds:\n",
    "            if 'text' in item:\n",
    "                tokenized = self.tokenizer.encode(item['text']) + [self.tokenizer.eot_token]\n",
    "                buffer.extend(tokenized)\n",
    "                while len(buffer) >= self.block_size + 1:\n",
    "                    x = torch.tensor(buffer[:self.block_size], dtype=torch.long)\n",
    "                    y = torch.tensor(buffer[1:self.block_size+1], dtype=torch.long)\n",
    "                    yield x, y\n",
    "                    buffer = buffer[self.block_size:]\n",
    "\n",
    "\n",
    "full_train_stream = load_dataset('HuggingFaceFW/fineweb-edu', name='sample-350BT', split='train', streaming=True)\n",
    "num_val_samples = 10000  # Let's reserve 10,000 samples for validation.\n",
    "\n",
    "val_stream_full = full_train_stream.take(num_val_samples)\n",
    "train_stream_full = full_train_stream.skip(num_val_samples)\n",
    "\n",
    "train_dataset_full = IterableTextDataset(tokenizer, train_stream_full, block_size)\n",
    "val_dataset_full = IterableTextDataset(tokenizer, val_stream_full, block_size)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_full, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    pin_memory=True # Helps speed up data transfer to the GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_full, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add code to evaluate our LLM on the Hellaswag dataset. HellaSwag is a multiple-choice benchmark designed to probe a language model's commonsense reasoning and narrative completion skills. Each example pairs a natural-language context (usually an incomplete story or instructional description) with four candidate endings that are grammatically plausible. The dataset was intentionally constructed to make superficial cues ineffective, so robust performance requires understanding real-world dynamics and linguistic nuance.\n",
    "\n",
    "Key characteristics include:\n",
    "- **Challenging distractors:** The incorrect endings are adversarially generated, so they are fluent and contextually plausible.\n",
    "- **Narrative and procedural diversity:** Prompts draw from activity descriptions, WikiHow instructions, and grounded video captions, encouraging broad coverage of everyday situations.\n",
    "- **Focus on generative scoring:** Although evaluation is cast as multiple choice, models must internally score free-form completions to select the most coherent ending.\n",
    "\n",
    "We load the HellaSwag validation split and a matching accuracy metric from the Hugging Face `datasets` and `evaluate` libraries, optionally subsampling for quicker experiments. Each validation example supplies a context and four candidate endings. For every ending, we concatenate it with the context, tokenize the result, and run it through the model in evaluation mode to obtain the autoregressive loss. The ending with the lowest loss becomes the model's prediction, which we compare against the gold label to accumulate accuracy. This routine aligns the evaluation with our training objective—better language modeling likelihood should translate directly into higher HellaSwag accuracy.\n",
    "\n",
    "During longer training runs we call this evaluation hook periodically, logging the resulting accuracy so it can be tracked alongside training metrics. In this way, HellaSwag serves as an end-to-end proxy for the model's ability to apply learned knowledge to structured reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HellaSwag validation set and accuracy metric...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading HellaSwag validation set and accuracy metric...\")\n",
    "hellaswag_val = load_dataset(\"hellaswag\", split=\"validation\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# For a quick demonstration, let's use a small subset of the validation set.\n",
    "# A full evaluation would run on the entire set.\n",
    "subset_size = 2000\n",
    "hellaswag_subset = hellaswag_val.select(range(subset_size))\n",
    "\n",
    "# 2. Create an evaluation function\n",
    "def evaluate_on_hellaswag(model, tokenizer, dataset):\n",
    "    \"\"\"\n",
    "    Evaluates a model on the HellaSwag dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # The core idea is to calculate the loss for the context concatenated with each ending.\n",
    "    # The ending that results in the lowest loss is the model's prediction.\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Evaluating HellaSwag\"):\n",
    "        context = example['ctx']\n",
    "        endings = example['endings']\n",
    "        correct_label = int(example['label'])\n",
    "        \n",
    "        context_tokens = tokenizer.encode(context)\n",
    "        \n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for ending in endings:\n",
    "                # Create the full input by combining context and the current ending\n",
    "                full_text_tokens = context_tokens + tokenizer.encode(ending)\n",
    "                \n",
    "                # Prepare input and target tensors\n",
    "                x = torch.tensor([full_text_tokens[:-1]], dtype=torch.long, device=model.device)\n",
    "                y = torch.tensor([full_text_tokens[1:]], dtype=torch.long, device=model.device)\n",
    "                \n",
    "                # Get the loss for this specific continuation\n",
    "                logits, _, aux_loss_ = model(x)\n",
    "                total_loss, main_loss, aux_loss = model._calculate_loss(logits, y.to(model.device), aux_loss_)\n",
    "                losses.append(main_loss.item())\n",
    "        \n",
    "        # The prediction is the index of the ending with the minimum loss\n",
    "        prediction = torch.argmin(torch.tensor(losses)).item()\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(correct_label)\n",
    "        \n",
    "    # 3. Compute the final score\n",
    "    print(\"Computing final accuracy...\")\n",
    "    results = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "    return results\n",
    "\n",
    "# hellaswag_results = evaluate_on_hellaswag(model, tokenizer, hellaswag_subset)\n",
    "# hellaswag_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce the concepts of mult-GPU and mixed-precision training. \n",
    "\n",
    "Training large language models efficiently requires spreading computation across multiple devices while maintaining numerical stability. This guide summarizes the major forms of GPU parallelism, how PyTorch Lightning orchestrates multi-GPU training, and why mixed precision is a default optimization in this project.\n",
    "Different parallelism strategies partition work across accelerators in complementary ways. Modern systems often compose multiple techniques.\n",
    "\n",
    "### Data parallelism\n",
    "- **Concept:** Replicate the full model on each GPU, split every batch of data, and aggregate gradients after each backward pass.\n",
    "- **When to use:** Fits when the model parameters comfortably fit into individual GPU memory and input batches are large.\n",
    "- **Trade-offs:** Communication cost for synchronizing gradients grows with parameter count, but scaling is straightforward and stateless.\n",
    "\n",
    "PyTorch's `DistributedDataParallel` (DDP) is the de facto data-parallel implementation. Each process hosts a model replica and maintains local optimizer state while `all_reduce` operations keep gradients synchronized.\n",
    "\n",
    "### Model parallelism\n",
    "When the model itself is too large for a single device, we split its parameters.\n",
    "\n",
    "- **Tensor (intra-layer) parallelism:** Each layer's tensors are sharded across GPUs. Collective operations (e.g., `all_gather`, `reduce_scatter`) coordinate matrix multiplications. Tensor parallelism reduces per-GPU activation and weight memory but introduces more frequent communication.\n",
    "- **Pipeline parallelism:** Consecutive layers are assigned to stages on different devices. Micro-batches move through the pipeline in staggered fashion to keep stages busy. While communication is limited to stage boundaries, pipeline bubbles can reduce utilization if micro-batch counts are small.\n",
    "- **Sequence/context parallelism:** Long contexts are split across devices, sharing attention key/value memory. This is useful for inference or training with very long sequences.\n",
    "\n",
    "Model-parallel strategies are often combined with data parallelism (\"3D\" parallelism) to balance memory constraints and throughput.\n",
    "\n",
    "### Parameter and optimizer sharding\n",
    "Techniques like ZeRO (Zero Redundancy Optimizer) partition optimizer states, gradients, and sometimes parameters across data-parallel ranks. This reduces memory duplication while keeping a data-parallel programming model.\n",
    "\n",
    "## Multi-GPU training in PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning abstracts the distributed setup via **strategies** that configure process launch, distributed collectives, and accelerator plugins. Key capabilities for multi-GPU runs include:\n",
    "\n",
    "- **DDP strategy:** Wraps PyTorch DDP, automatically launching one process per GPU (`Trainer(strategy=\"ddp\")`). Lightning handles dataset sharding via `DistributedSampler`, synchronizes batch norms, and coordinates gradient reduction.\n",
    "- **Fully Sharded Data Parallel (FSDP):** Activates parameter sharding to reduce memory footprints (`strategy=\"fsdp\"` with optional auto-wrap policies).\n",
    "- **DeepSpeed integration:** Enables ZeRO stages, gradient accumulation, and activation checkpointing via `strategy=\"deepspeed\"`.\n",
    "- **Accelerator plugins:** `Trainer(accelerator=\"gpu\", devices=n)` selects available GPUs, while Lightning manages device placement and precision configuration.\n",
    "\n",
    "Because Lightning centralizes the launch configuration, scripts stay close to single-GPU code. Callbacks and loggers remain device-agnostic, and synchronization barriers (e.g., `rank_zero_only`) ensure safe logging.\n",
    "\n",
    "## Mixed precision training\n",
    "\n",
    "### Why mixed precision helps\n",
    "- **Higher throughput:** FP16 or bfloat16 math uses tensor cores, dramatically increasing matrix multiply throughput on modern GPUs.\n",
    "- **Lower memory usage:** Half-precision activations and gradients halve memory consumption, enabling larger batches or models.\n",
    "- **Similar convergence:** With loss scaling to avoid underflow, models typically match FP32 accuracy while converging faster.\n",
    "\n",
    "### Mixed precision in PyTorch Lightning\n",
    "Lightning toggles precision via the `precision` flag, e.g., `Trainer(precision=\"16-mixed\")` for automatic mixed precision (AMP) or `precision=\"bf16-mixed\"` on hardware with bfloat16 support. Under the hood Lightning wraps forward passes in `torch.autocast`, applies dynamic loss scaling when using FP16, and ensures optimizer steps run in FP32 to preserve stability. Precision settings integrate with all distributed strategies, so mixed precision works seamlessly across multi-GPU runs.\n",
    "\n",
    "### When to fall back to full precision\n",
    "If gradients become unstable (e.g., persistent `NaN`s) or certain custom CUDA kernels lack half-precision support, switch to `precision=32` or selectively disable AMP for problematic modules. Lightning's strategy abstraction keeps the rest of the training loop unchanged, simplifying experimentation.\n",
    "\n",
    "By combining distributed strategies with mixed precision, we maximize throughput and keep experiments tractable even when scaling to many GPUs.\n",
    "\n",
    "I highly recommend watching the [Stanford CS336 Parallelism Lecture](https://www.youtube.com/watch?v=l1RJcDjzK8M&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=7). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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') \n",
    "\n",
    "class GenerateTextCallback(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to generate text samples at the end of each validation epoch.\"\"\"\n",
    "    def __init__(self, prompts, tokenizer, every_n_steps=100):\n",
    "        super().__init__()\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "        if not trainer.is_global_zero:\n",
    "            return  # only rank 0 prints/logs text\n",
    "        pl_module.print(f\"\\n\\n--- Generating text at step {trainer.global_step} ---\")\n",
    "        tb = getattr(trainer.logger, \"experiment\", None)\n",
    "        \n",
    "        for i, prompt in enumerate(self.prompts):\n",
    "            start_tokens = self.tokenizer.encode(prompt)\n",
    "            context = torch.tensor(start_tokens, dtype=torch.long, device=pl_module.device).unsqueeze(0)\n",
    "            generated_tokens = pl_module.generate_nocache(context, max_new_tokens=100, temperature=0.7, top_k=20)\n",
    "            generated_text = self.tokenizer.decode(generated_tokens[0].tolist())\n",
    "            pl_module.print(f\"PROMPT: '{prompt}'\")\n",
    "            pl_module.print(f\"GENERATED: {generated_text}\\n\")\n",
    "            if tb is not None and hasattr(tb, \"add_text\"):\n",
    "                tb.add_text(f\"samples/prompt_{i}\", f\"**Prompt:** {prompt}\\n\\n**Generated:** {generated_text}\",\n",
    "                            global_step=trainer.global_step)\n",
    "\n",
    "class EvaluateHellaSwag(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to evaluate the LLM.\"\"\"\n",
    "    def __init__(self, every_n_steps=1000):\n",
    "        super().__init__()\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "        # do heavy eval only on rank 0\n",
    "        if not trainer.is_global_zero:\n",
    "            return\n",
    "        pl_module.print(f\"\\n\\n--- Evaluating at step {trainer.global_step} ---\")\n",
    "        \n",
    "        hellaswag_results = evaluate_on_hellaswag(pl_module, tokenizer, hellaswag_subset)\n",
    "        acc = hellaswag_results['accuracy']\n",
    "        pl_module.print(f\"\\n\\n--- Accuracy: {acc} at step {trainer.global_step} ---\")\n",
    "        pl_module.log(\"hellaswag/accuracy\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=False)\n",
    "\n",
    "\n",
    "callback = GenerateTextCallback(prompts=[\"The verdict was\", \"In a shocking turn of events\", \"The jury decided to\"], \n",
    "    tokenizer=tokenizer, every_n_steps=1000)\n",
    "evalcallback = EvaluateHellaSwag(every_n_steps=1000)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"shraygpt-{epoch:02d}-{step:05d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = L.Trainer(max_steps=200_000, accelerator='auto', devices=8, precision='bf16-mixed', strategy='auto', \n",
    "                    num_sanity_val_steps=0, limit_train_batches=1000, limit_val_batches=100,\n",
    "                    callbacks=[callback, L.pytorch.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=200), evalcallback, checkpoint_cb, lr_monitor],\n",
    "                    logger=L.pytorch.loggers.TensorBoardLogger(\"logs/\"), log_every_n_steps=1) \n",
    "\n",
    "model.automatic_optimization = False\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify with some generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: 'Common medical symptoms of viral are'\n",
      "GENERATED: Common medical symptoms of viral are associated with intelina. Patients with MMR are likely to experience epileptococcal acute pancreatitis, and for people carrying juvenile enterocolaria, the infection may be severe. Although, not everybody responds to treatment, the disease can become both life-threatening and can progress to liver failure and death.\n",
      "What Is Surgery?\n",
      "Surgery is the only way to heal if it is better for you or your child. Surgery is most often used for the typical purpose of surgery (to prevent pneumonia\n",
      "\n",
      "PROMPT: 'An interesting political fact is'\n",
      "GENERATED: An interesting political fact is that slavery was abolished so completely in the 19th century. So, it would have been possible for one hundred years to reach this conclusion.\n",
      "Hundreds of people believed that slavery was a global problem, but some wonder if the very idea of black slavery still exists. And there is a direct evidence that black people around the world from all over the world testify to 1.5 million black people about one percent of all immigrants from Africa, which was carried with them to the “civil rights movement�\n",
      "\n",
      "PROMPT: 'The future of AI is'\n",
      "GENERATED: The future of AI is increasingly privat due to ongoing feedback from Cambodian partners and the associated mining company Ibhadra Sirra, Crista Mestra, said it will transition from an agricultural sector to a developed, independent, and environmentally responsible, partnership with a consortium of more than 20 countries to form The International Data Initiative comprising 25 countries will be supported to support the development of the Global North Gen package. This will be done under the Treaty’s patented green technology RIMO.\n",
      "The bid will bring about\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from src.shraygpt import ShrayGPT\n",
    "torch.set_num_threads(1) # Prevents deadlocks with DataLoader and multiple workers\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "# model = ShrayGPT.load_from_checkpoint(\"checkpoints/shraygpt-base.ckpt\")\n",
    "\n",
    "model.eval()\n",
    "prompts = [\"Common medical symptoms of viral are\", \"An interesting political fact is\", \"The future of AI is\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    start_tokens = tokenizer.encode(prompt)\n",
    "    context = torch.tensor(start_tokens, dtype=torch.long, device=model.device).unsqueeze(0)\n",
    "    generated_tokens = model.generate_nocache(context, max_new_tokens=100, temperature=0.6, top_k=20)\n",
    "    generated_text = tokenizer.decode(generated_tokens[0].tolist())\n",
    "    print(f\"PROMPT: '{prompt}'\")\n",
    "    print(f\"GENERATED: {generated_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
