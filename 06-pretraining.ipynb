{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2.67B\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from src.shraygpt import ShrayGPT\n",
    "torch.set_num_threads(1) # Prevents deadlocks with DataLoader and multiple workers\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "def get_total_param_count(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "d_model = 32*32\n",
    "n_head = 32\n",
    "d_head = 32\n",
    "n_layers = 32\n",
    "num_experts = 8\n",
    "num_experts_per_tok = 1\n",
    "block_size = 8192\n",
    "batch_size = 1\n",
    "lr = 1e-5\n",
    "\n",
    "model = ShrayGPT(\n",
    "    vocab_size=tokenizer.n_vocab, \n",
    "    block_size=block_size, \n",
    "    d_model=d_model,\n",
    "    n_head=n_head, \n",
    "    d_head=d_head, \n",
    "    n_layers=n_layers, \n",
    "    num_experts=num_experts, \n",
    "    num_experts_per_tok=num_experts_per_tok\n",
    ")\n",
    "model.hparams.learning_rate = lr\n",
    "model.hparams.aux_loss_weight = 1e-2\n",
    "# model.compile(backend=\"inductor\", dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "params = get_total_param_count(model)\n",
    "print(f\"Total parameters: {params/1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d822d6aa57194dd0a2b3dc3d3eab79ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a270b0d6e7204c208bdfe59030334f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, hf_dataset, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def _rank_world(self):\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            return dist.get_rank(), dist.get_world_size()\n",
    "        return 0, 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        rank, world = self._rank_world()\n",
    "\n",
    "        # Shard the HF streaming dataset so each rank reads a disjoint slice\n",
    "        ds = self.hf_dataset\n",
    "        if hasattr(ds, \"shard\"):\n",
    "            ds = ds.shard(num_shards=world, index=rank, contiguous=True)\n",
    "\n",
    "        buffer = []\n",
    "        for item in ds:\n",
    "            if 'text' in item:\n",
    "                tokenized = self.tokenizer.encode(item['text']) + [self.tokenizer.eot_token]\n",
    "                buffer.extend(tokenized)\n",
    "                while len(buffer) >= self.block_size + 1:\n",
    "                    x = torch.tensor(buffer[:self.block_size], dtype=torch.long)\n",
    "                    y = torch.tensor(buffer[1:self.block_size+1], dtype=torch.long)\n",
    "                    yield x, y\n",
    "                    buffer = buffer[self.block_size:]\n",
    "\n",
    "\n",
    "full_train_stream = load_dataset('HuggingFaceFW/fineweb-edu', name='sample-350BT', split='train', streaming=True)\n",
    "num_val_samples = 10000  # Let's reserve 10,000 samples for validation.\n",
    "\n",
    "val_stream_full = full_train_stream.take(num_val_samples)\n",
    "train_stream_full = full_train_stream.skip(num_val_samples)\n",
    "\n",
    "train_dataset_full = IterableTextDataset(tokenizer, train_stream_full, block_size)\n",
    "val_dataset_full = IterableTextDataset(tokenizer, val_stream_full, block_size)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_full, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    pin_memory=True # Helps speed up data transfer to the GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_full, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HellaSwag validation set and accuracy metric...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading HellaSwag validation set and accuracy metric...\")\n",
    "hellaswag_val = load_dataset(\"hellaswag\", split=\"validation\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# For a quick demonstration, let's use a small subset of the validation set.\n",
    "# A full evaluation would run on the entire set.\n",
    "subset_size = 100\n",
    "hellaswag_subset = hellaswag_val.select(range(subset_size))\n",
    "\n",
    "# 2. Create an evaluation function\n",
    "def evaluate_on_hellaswag(model, tokenizer, dataset):\n",
    "    \"\"\"\n",
    "    Evaluates a model on the HellaSwag dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # The core idea is to calculate the loss for the context concatenated with each ending.\n",
    "    # The ending that results in the lowest loss is the model's prediction.\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Evaluating HellaSwag\"):\n",
    "        context = example['ctx']\n",
    "        endings = example['endings']\n",
    "        correct_label = int(example['label'])\n",
    "        \n",
    "        context_tokens = tokenizer.encode(context)\n",
    "        \n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for ending in endings:\n",
    "                # Create the full input by combining context and the current ending\n",
    "                full_text_tokens = context_tokens + tokenizer.encode(ending)\n",
    "                \n",
    "                # Prepare input and target tensors\n",
    "                x = torch.tensor([full_text_tokens[:-1]], dtype=torch.long, device=model.device)\n",
    "                y = torch.tensor([full_text_tokens[1:]], dtype=torch.long, device=model.device)\n",
    "                \n",
    "                # Get the loss for this specific continuation\n",
    "                logits, _, aux_loss_ = model(x)\n",
    "                total_loss, main_loss, aux_loss = model._calculate_loss(logits, y.to(model.device), aux_loss_)\n",
    "                losses.append(total_loss.item())\n",
    "        \n",
    "        # The prediction is the index of the ending with the minimum loss\n",
    "        prediction = torch.argmin(torch.tensor(losses)).item()\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(correct_label)\n",
    "        \n",
    "    # 3. Compute the final score\n",
    "    print(\"Computing final accuracy...\")\n",
    "    results = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "    return results\n",
    "\n",
    "# hellaswag_results = evaluate_on_hellaswag(model, tokenizer, hellaswag_subset)\n",
    "# hellaswag_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrategies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDPStrategy\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcapture_scalar_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_float32_matmul_precision(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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') \n",
    "\n",
    "class GenerateTextCallback(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to generate text samples at the end of each validation epoch.\"\"\"\n",
    "    def __init__(self, prompts, tokenizer, every_n_steps=100):\n",
    "        super().__init__()\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "\n",
    "        if not trainer.is_global_zero:\n",
    "            return  # only rank 0 prints/logs text\n",
    "        pl_module.print(f\"\\n\\n--- Generating text at step {trainer.global_step + 1} ---\")\n",
    "        tb = getattr(trainer.logger, \"experiment\", None)\n",
    "        \n",
    "        for prompt in self.prompts:\n",
    "            start_tokens = self.tokenizer.encode(prompt)\n",
    "            context = torch.tensor(start_tokens, dtype=torch.long, device=pl_module.device).unsqueeze(0)\n",
    "            generated_tokens = pl_module.generate(context, max_new_tokens=100, temperature=0.8, top_k=20)\n",
    "            generated_text = self.tokenizer.decode(generated_tokens[0].tolist())\n",
    "            pl_module.print(f\"PROMPT: '{prompt}'\")\n",
    "            pl_module.print(f\"GENERATED: {generated_text}\\n\")\n",
    "            if tb is not None and hasattr(tb, \"add_text\"):\n",
    "                tb.add_text(f\"samples/prompt_{i}\", f\"**Prompt:** {prompt}\\n\\n**Generated:** {text}\",\n",
    "                            global_step=trainer.global_step)\n",
    "        # optional: flush\n",
    "        if tb is not None and hasattr(tb, \"flush\"):\n",
    "            tb.flush()\n",
    "\n",
    "class EvaluateHellaSwag(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to evaluate the LLM.\"\"\"\n",
    "    def __init__(self, every_n_steps=100):\n",
    "        super().__init__()\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "\n",
    "        # do heavy eval only on rank 0\n",
    "        if not trainer.is_global_zero:\n",
    "            return\n",
    "        pl_module.print(f\"\\n\\n--- Evaluating at step {trainer.global_step} ---\")\n",
    "        \n",
    "        hellaswag_results = evaluate_on_hellaswag(model, tokenizer, hellaswag_subset)\n",
    "        acc = hellaswag_results['accuracy']\n",
    "        pl_module.print(f\"\\n\\n--- Accuracy: {acc} at step {trainer.global_step} ---\")\n",
    "        pl_module.log(\"hellaswag/accuracy\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "        tb = getattr(trainer.logger, \"experiment\", None)\n",
    "        if tb is not None and hasattr(tb, \"add_scalar\"):\n",
    "            tb.add_scalar(\"hellaswag/accuracy\", acc, global_step=trainer.global_step)\n",
    "            if hasattr(tb, \"flush\"):\n",
    "                tb.flush()\n",
    "\n",
    "\n",
    "callback = GenerateTextCallback(prompts=[\"The verdict was\", \"In a shocking turn of events\", \"The jury decided to\"], \n",
    "    tokenizer=tokenizer, every_n_steps=100)\n",
    "evalcallback = EvaluateHellaSwag(every_n_steps=100)\n",
    "\n",
    "trainer = L.Trainer(max_steps=20_000, accelerator='auto', devices=8, precision='16-mixed', strategy='auto', \n",
    "                    num_sanity_val_steps=0, val_check_interval=100, check_val_every_n_epoch=None, limit_train_batches=100, limit_val_batches=100,\n",
    "                    callbacks=[callback, L.pytorch.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3), evalcallback],\n",
    "                    logger=L.pytorch.loggers.TensorBoardLogger(\"logs/\"), log_every_n_steps=1) \n",
    "\n",
    "model.automatic_optimization = False\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
