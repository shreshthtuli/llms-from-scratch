{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2.67B\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from src.shraygpt import ShrayGPT\n",
    "torch.set_num_threads(1) # Prevents deadlocks with DataLoader and multiple workers\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "def get_total_param_count(module):\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "d_model = 32*32\n",
    "n_head = 32\n",
    "d_head = 32\n",
    "n_layers = 32\n",
    "num_experts = 8\n",
    "num_experts_per_tok = 1\n",
    "block_size = 8192\n",
    "batch_size = 1\n",
    "lr = 3e-4\n",
    "\n",
    "model = ShrayGPT(\n",
    "    vocab_size=tokenizer.n_vocab, \n",
    "    block_size=block_size, \n",
    "    d_model=d_model,\n",
    "    n_head=n_head, \n",
    "    d_head=d_head, \n",
    "    n_layers=n_layers, \n",
    "    num_experts=num_experts, \n",
    "    num_experts_per_tok=num_experts_per_tok\n",
    ")\n",
    "model.hparams.learning_rate = lr\n",
    "model.hparams.aux_loss_weight = 1e-2\n",
    "# model.compile(backend=\"inductor\", dynamic=True, mode=\"reduce-overhead\")\n",
    "\n",
    "params = get_total_param_count(model)\n",
    "print(f\"Total parameters: {params/1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664441e53b134fdba91f56d762d58895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbbdd0c75724113bb0b57b211ab4a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "\n",
    "class IterableTextDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, hf_dataset, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def _rank_world(self):\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            return dist.get_rank(), dist.get_world_size()\n",
    "        return 0, 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        rank, world = self._rank_world()\n",
    "\n",
    "        # Shard the HF streaming dataset so each rank reads a disjoint slice\n",
    "        ds = self.hf_dataset\n",
    "        if hasattr(ds, \"shard\"):\n",
    "            ds = ds.shard(num_shards=world, index=rank, contiguous=True)\n",
    "\n",
    "        buffer = []\n",
    "        for item in ds:\n",
    "            if 'text' in item:\n",
    "                tokenized = self.tokenizer.encode(item['text']) + [self.tokenizer.eot_token]\n",
    "                buffer.extend(tokenized)\n",
    "                while len(buffer) >= self.block_size + 1:\n",
    "                    x = torch.tensor(buffer[:self.block_size], dtype=torch.long)\n",
    "                    y = torch.tensor(buffer[1:self.block_size+1], dtype=torch.long)\n",
    "                    yield x, y\n",
    "                    buffer = buffer[self.block_size:]\n",
    "\n",
    "\n",
    "full_train_stream = load_dataset('HuggingFaceFW/fineweb-edu', name='sample-350BT', split='train', streaming=True)\n",
    "num_val_samples = 10000  # Let's reserve 10,000 samples for validation.\n",
    "\n",
    "val_stream_full = full_train_stream.take(num_val_samples)\n",
    "train_stream_full = full_train_stream.skip(num_val_samples)\n",
    "\n",
    "train_dataset_full = IterableTextDataset(tokenizer, train_stream_full, block_size)\n",
    "val_dataset_full = IterableTextDataset(tokenizer, val_stream_full, block_size)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_full, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    pin_memory=True # Helps speed up data transfer to the GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_full, \n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,  \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HellaSwag validation set and accuracy metric...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading HellaSwag validation set and accuracy metric...\")\n",
    "hellaswag_val = load_dataset(\"hellaswag\", split=\"validation\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# For a quick demonstration, let's use a small subset of the validation set.\n",
    "# A full evaluation would run on the entire set.\n",
    "subset_size = 100\n",
    "hellaswag_subset = hellaswag_val.select(range(subset_size))\n",
    "\n",
    "# 2. Create an evaluation function\n",
    "def evaluate_on_hellaswag(model, tokenizer, dataset):\n",
    "    \"\"\"\n",
    "    Evaluates a model on the HellaSwag dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    # The core idea is to calculate the loss for the context concatenated with each ending.\n",
    "    # The ending that results in the lowest loss is the model's prediction.\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Evaluating HellaSwag\"):\n",
    "        context = example['ctx']\n",
    "        endings = example['endings']\n",
    "        correct_label = int(example['label'])\n",
    "        \n",
    "        context_tokens = tokenizer.encode(context)\n",
    "        \n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for ending in endings:\n",
    "                # Create the full input by combining context and the current ending\n",
    "                full_text_tokens = context_tokens + tokenizer.encode(ending)\n",
    "                \n",
    "                # Prepare input and target tensors\n",
    "                x = torch.tensor([full_text_tokens[:-1]], dtype=torch.long, device=model.device)\n",
    "                y = torch.tensor([full_text_tokens[1:]], dtype=torch.long, device=model.device)\n",
    "                \n",
    "                # Get the loss for this specific continuation\n",
    "                logits, _, aux_loss_ = model(x)\n",
    "                total_loss, main_loss, aux_loss = model._calculate_loss(logits, y.to(model.device), aux_loss_)\n",
    "                losses.append(total_loss.item())\n",
    "        \n",
    "        # The prediction is the index of the ending with the minimum loss\n",
    "        prediction = torch.argmin(torch.tensor(losses)).item()\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(correct_label)\n",
    "        \n",
    "    # 3. Compute the final score\n",
    "    print(\"Computing final accuracy...\")\n",
    "    results = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "    return results\n",
    "\n",
    "# hellaswag_results = evaluate_on_hellaswag(model, tokenizer, hellaswag_subset)\n",
    "# hellaswag_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/__init__.py:1615: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:45.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:751: Checkpoint directory /teamspace/studios/this_studio/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | tok_emb | Embedding  | 51.5 M | train\n",
      "1 | layers  | ModuleList | 2.6 B  | train\n",
      "2 | ln_f    | RMSNorm    | 1.0 K  | train\n",
      "3 | head    | Linear     | 51.5 M | train\n",
      "4 | dropout | Dropout    | 0      | train\n",
      "-----------------------------------------------\n",
      "2.7 B     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 B     Total params\n",
      "10,694.308Total estimated model params size (MB)\n",
      "2117      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f2b50cc3c44c8aabb9128f7c07d39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e400c196b54aadaac31a6ae3c2c891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 2000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was, who, and more likely, or more to be so as they can take the right-out.\n",
      "-term pressure will be used to create a long-up, and other, or an issue, including the most of a good, and is not to be able to use the system of the same. It will be required to be used to be more likely to be more about and, but when they are not to be given a problem is not the best to be to be a problem\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events--c-c.\n",
      "-cun, n.\n",
      "- H, B, R. (19. (1996(7.\n",
      "- J.\n",
      "- C, R. E/s, F. (2004): 5:10. (2009). \"A (2013).\n",
      "- \"The research is:\n",
      "- E-1:1.\n",
      "- \"M. (2006). \"G.\n",
      "- E. (10.\n",
      "- Klam, L.\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to, and, but in the Holy, and the world of the Jews, and the government.\n",
      "In the most important to the United States.\n",
      "The government\n",
      "The American government\n",
      "The government\n",
      "by in the\n",
      "The state and the government\n",
      "The public sector and the world.\n",
      "’s the “We”\n",
      "The government and more than the government\n",
      "”\n",
      "It’s\n",
      "The U.\n",
      "“We”\n",
      "“I\n",
      "“\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 2000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.22 at step 2000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('hellaswag/accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f821fbe70c9c4ce7894d2d0a72a655bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 4000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was but again again that he was dead, and they could not put, as, was a matter of the way to do it.\n",
      "The first thing of this was the one-to-day man, for the new man himself had done to the next morning, and the first Jew he had not heard it in the way of the Lord. He would say that \"The Lord was so impressed by the Lord, to his wife, the King of the Lord, and they had to be a very\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events.\n",
      "Dr, a team of scientists, who were doing himself, the team, the team of the group, and how he had the advantage he and told him. The team went on the team, and carried the team's decision.\n",
      "The team said the team was not a researcher who had the team that the team would.\n",
      "“I was asked, and how they wanted to work at that time. It had to take on a team to work on his team.\n",
      "“\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to study the first-hand-m-n-p-4, a--t-m-s------pl-----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 4000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.25 at step 4000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f42fd6fde7a409289d5e5a231cb2fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 6000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was in the late-century German newspaper, which included the original-based survey of 1819. It was also known that in the late 19th century, and was part of New York City, a library of the former library, which is also a collection of printed and printed in 1847 and 1842.\n",
      "On the early 1800s, the first newspaper, the newspaper of newspaper and newspaper, were also written in the newspaper. The newspaper was originally published in 1837, a newspaper in 18\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events, a highly-looking-growth technique of the ancient Greek of Greece, with a detailed report of its “unreal” of the Roman Empire, with the first known Roman Catholic, and a Roman Catholic in Greek (“The Greek) and a Roman Empire” (“the Greek”) by its first Greek, Greek, and Greek (ph.e. the Greek word for the Greek, and the Greek is also known as “Christian”, meaning\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to help your child understand a personal relationship for you. It is also important for your child to be involved in your child’s needs, while they may require an opportunity to make it clear to him as much as he or she has to do.\n",
      "In this case, you can also use your child to express your personal responsibility.\n",
      "Our social life is crucial in helping your children get involved in helping them learn to express their needs and be able to learn about what they need for.\n",
      "To\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 6000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.32 at step 6000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416c75c34e30499cb31df475c287b7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 8000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was in the way that the user would have to have the same touch to touch and see if they had a hand-held hand on the hand if they had the hand, and not the hand, but that this would have been very effective.\n",
      "This was the case in this story, which gave the user a different view of the button. If the user had to click, they were still working on the screen in the right way and to make them on the screen.\n",
      "This was not the case\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events is a reminder and a statue of our ancestors. He is a teacher and has a voice to live in. He is a teacher, a great teacher, and a teacher who has a strong, and is a teacher. He is a writer and author of a book with the idea of anoree to be a teacher.\n",
      "“The teacher is the speaker’s and the teacher is not a writer, but a teacher’s need.”\n",
      "It is a writer who\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to the next, he would still have the ability to play a game and play that are designed to be used as a game that can be designed to play with the game and play a play.\n",
      "“The game is a game that is played for play, playing and play games that could be played with play games.\n",
      "“Game play games are not only suited to players, but the game games are designed to play games that players play.”\n",
      "As you play is the most important\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 8000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.27 at step 8000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684774aac57f4be395f940080ac44a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 10000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was a sign and a member of the American civil rights organization, which was the way forward to a goal of their own rights.\n",
      "The first step in this step is to develop a strategy to tackle the issue of the National Park program, which included a group of people who were involved in the development of a plan that would be able to make a decision on the path of a new problem and in fact they would have to be taken. The first step would be to create a plan where a community would\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events in New York in New York’s New York City, New York, the first known place in New York City in the New City; The New York State Building in New York City, San Diego in New York Harbor, New York, New York City, New York, New York, New York, New York, New York, New York, New York in New York, New York and New York, New York, New York City, Cambridge, New York, New York, New Philadelphia\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to keep the attention of their bodies, to whom they were doing their duties and did not have their rights and rights, that were to be done by their parents and other interests.\n",
      "How did you know if I failed? Why did you learn that you knew where your son was?\n",
      "The first, what made you want was to go?\n",
      "1. When I saw the first question that I wanted to talk in previous posts, I realized that I had not taken a part in my life until I\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 10000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.27 at step 10000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f405ca260f624f0e958ac4fd32c8f9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 12000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was to take the old hand. To this point, the \"whites\") had been released for the \"hats\" before they started to create their \"junk\" in front of the \"k\" while the \"hose\" was moved.\n",
      "\"It's been a few months before the \"H\" was made, a \"Z\" in the \"K\" was called \"K\" because \"hairs\" of a \"Z\" was \"so\").\n",
      "\"The Sun was very\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events.\n",
      "Another way people play more important in their lives, as adults and children play a part in all walks of life. If one of these people is in crisis, why do some people play a part in helping the children live together?\n",
      "One of the most important benefits of this type of play is that it is an overall well being. For that reason, the more we work together to be a leader, we have to create a love of the world.\n",
      "The play is designed to teach\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to explain them, our minds can’t be solved in the future.”\n",
      "• It should be possible to solve a difficult problem, or to test every time a problem is possible.\n",
      "• The test should be conducted to determine if the problem remains unresolved or cannot be solved.\n",
      "• It should be treated, and possible to detect any mistakes.\n",
      "• It should be done in the computer system to get rid of errors and to be discovered.\n",
      "The test must be completed and tested\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 12000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.26 at step 12000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fa644a12fa41ab8a7ba5cef28a4493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 14000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was moved to help students not need.\n",
      "The team of the team is made from the DNA from the egg shell that it was processed to see how the DNA from the chicken could cause the infection.\n",
      "\"I understand the difference between the DNA that's going to get to the chicken meat or the chicken, and how it's going to get the information from its DNA,\" said study co-author Dr. Howard Follins, a medical doctor who tested the protein to get in the first place:\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events, and the “War” and “Power” music,” and the “Mrs” is an example of the way in which the music and music are organized in the early 19th century. It is a way of looking at the music that we are talking about in today.\n",
      "Music is not something that happens in the last two decades in the “Bottomstaff” or, but one can use, because music is not a music part of the whole\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to find the gods:\n",
      "- We shall be talking about other objects in the vicinity.\n",
      "- We should not forget that we are all beings who are made the objects of our own.\n",
      "- We shall never try to find the knowledge contained in the Bible.\n",
      "- It is difficult to understand how the Bible is translated.\n",
      "- It is important that we do not know when and it is to be taught.\n",
      "- We will not be able to teach a text. We will not be able\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 14000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.26 at step 14000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ef6d6cf98340459314ac388ba46d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 16000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was found at the X-4\n",
      "at the two-and-butaters were fired at the same time. The moon was thought to be much better and so the Lunar Age could be made more cragmented over a thousand years. The difference between the three-and-halfths is very unusual compared to the first four ever smaller moons, and the two-planet observations of both the two stars were slightly larger than the first two-planet.\n",
      "One other odd tidiest of the other two\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events, and a view of ideas.\n",
      "In the 18th century, Alexander believed that God, and his principles, had made him an angel who, according to his prophecy, “He was a God; and, in his sight, had the power to save the world bereveled by the world” (4:21; 16:22). As in Plato, God is a creator of perfect goodness, but the mystery of eternity is the ultimate redemption of pleasure.\n",
      "Jesus was\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to learn how to read such a large number of money distances\n",
      "In the United States alone many of the most diverse routes we can find to watch if they are at a distance. We are, therefore, not just that, but that's where we come from and how we can get there are.\n",
      "We are in a general sense of where we're looking at the same spot and the opportunity to compare ourselves with ourselves.\n",
      "- We're really on the lookout for us: we can see the same\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 16000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.26 at step 16000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909e61050c8340ec9d79175da1477b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 18000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was held in the 1950 century. It was not a question of the Jews in Palestine-based Palestine programme, but that they were also Jewish and wanted Jews to be deported. Many Jewish Jews were murdered in Palestine in 1967.\n",
      "Those who belonged to Jews were Jewish. Many of them went on the grounds of the Jewish king and his son-in-law, Simon Hass ibn Hale, were Jewish. There were Jews who were the Gentist Rabbilim al-Nissim (not the first\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events, the sun being of the sun is a part of the atmosphere, and the energy that we have been trying to measure the intensity and temperature we have so high.\n",
      "To measure the temperature, we need to measure the temperature of the atmosphere in order to measure the temperature at which we are measuring the values of the atmosphere. For instance, a temperature scale is one degree Celsius. A temperature difference of 0.2°C is a measure of the relationship between the temperature and the temperature obtained by the\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to recharge the average they will be rewarded if they have not learned how to manage their risk.\n",
      "- In fact, it is important to understand the difference between risk and risk factors that might increase your risk level and risk of developing cancers.\n",
      "- Identifying your risk factors and making sure your risk is strongly measured and treated.\n",
      "- Consider monitoring the risks and benefits to your treatment plan.\n",
      "- Medication and treatment plan should include:\n",
      "- Medication and prevention should include:\n",
      "- Speech\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 18000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.31 at step 18000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1ba4310dfb46b992e4e14c098e6eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 20000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was lost by the need to keep the user from the box. Once it was replaced, the other cards were to stop.\n",
      "7. Math Worksheets\n",
      "Grab the second puzzle on a page. Fold the page down onto the site for an interesting bit about the Foldit. Fold the page with a red ribbon. Fold each slide into another quadrant on each slide. Fold it up and glue it on the next slide to show the backresting. Fold it and cut the bleed. Fold\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events, and the ‘old’ness’ is to do the work of building a building on the building, the contractors used by architects who can do the project on their own.\n",
      "The builders of computer and computer who are looking for a \"work\" of computers can easily access.\n",
      "For your convenience, you can have the time to think about your computer, electronic computer and electronic devices, and have the same problem. You have to have hundreds of records to decide which stations to use\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to scale, as the haught around the region increases, as if they are emitting consistently as much as possible. One method for measuring minimum temperature is:\n",
      "Pulse used in measuring the percentage of ppts (g = ppt) is the recommended method of decreasing the level of water as this will increase the level of water available. For determining the percent of the water in this case, it is advised that 0.5 percent of the population will be unable to meet the expected level of water\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 20000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.25 at step 20000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9f2f7cd4564557bad023489b97e14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 22000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was put.\n",
      "\"We agree to explore the new vocabulary for the new schema, the concepts of grammar, and the concepts of the Bible were developed, and when we all began to develop our grammar and grammar and symbolism. I hope you feel more confident in your new words and thought, \"Look for the Standard Bible.\" (Munacqua)\n",
      "\"Nay\" is the desire to learn a wide range of purposes, including the use of repetition and study and study of new subjects such as\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events and other effects upon the course of human activity, with much of the awareness of the health of the body. These are known as the Kamm (San K). Kaury (Arizona: 800-649) are described in the overall literature.\n",
      "2. Rao Sidery (ed.) --Shivy, 17 (1985).\n",
      "3. Zohypyenexperience: \"The clinical interpretation of maternal mortality in relation to the overall picture of the health of the infant while\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to maximize the effects of low grade and lower rates. The most powerful predictive theory, especially that it is due to the theory behind what we deem in the North America—in the late 1776 essay, The Great Master, and the South—that the United States—then defined as the best alternative—but not to a certain extent.\n",
      "The book includes excerpts from the novel from an earlier paper by a former president, who later became a reference to the paper The Lost Love, a shorter book\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 22000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.3 at step 22000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1bd8ee164d4005a4026df798b54413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 24000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was used in 1963 to every day.\n",
      "March to the church on the May 15, 1816-4, first children were encouraged to learn about the ritual that came from the pulpit of the day. Finally, the children were encouraged to discuss their own rites because they wanted to remember all the other spectators that had not been lit up by. The play was a very important part of the ritual.\n",
      "To be a refresher, it is highly recommended that, after a thorough day, it is\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events and for the work. Here are all of this experience.\n",
      "To include all the details, this site should take you to a project that you have chosen to plan and submit. Here’s a bit of effort that has helped you as you sit and stand in a circle and think about how to present your work. I’m sure you have all the details from your mentor all to go and follow him’ to reach everyone.\n",
      "What are the key factors leading to my family\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to demonstrate damage,” he says. “It’s hard, I’m done right on the door and the shovel are impaired.”\n",
      "Ask the Scientist:\n",
      "“His Majesty Father Protects Must Takes Out the Bus\n",
      "Nestras have a wish for a way that they will save the world from perishing and doing away with the destruction of power and violence. But they have trouble finding that millions of human beings can’t see it.\n",
      "�\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 24000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.26 at step 24000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d2aaccbd0b4ce3880a0cea488f8c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 26000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was to describe the nature of the culture. In order to identify the culture and culture of the culture of culture, it is important to remember that some people are able to survive and that men may be ready to share their thoughts (without doubt, agrid). Therefore, some younger writers find this more plausible approach to the survival of the species of bacteria and this is referred to as \"youth\" (Acdo)\") – dealing with bacterial species.\n",
      "Here is an introduction to some of the concepts\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events, that’s not the case. There is not so many romantic statements that have been made on the basis of justifying Peace. The only reliable evidence that the lack of participation in leadership was the presence of infasteriquer in the political debate, and the right of support to the political party of the United States government in 1956 to the Senate and the House of Representatives approved the preliminary federal budget. The American Librarian proved that no action proposed by Congress directly to Congress if the President were elected\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to determine these\n",
      "of the forces that are\n",
      "for with the forces of E. Manch. Secretary deadlines that they are to be\n",
      "directable to strengthen research and to apply these to their work. To that end, they need to understand what they know about development and technological progress. We also challenge a simple set of hypotheses to predict which scientists understand\n",
      "turate, rejecting what we want to know.\n",
      "In an effort to solve a very complex problem it requires not just a small number of scientists\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 26000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.23 at step 26000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79b16270edc48cebca6e9c00ee44ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 28000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was built at the F bid which he became in existence of the Earth.\n",
      "The goal of Nature is to trace of an ancient cos of Earth’s largest Mediterranean. It was once thought that Kepler began to adopt the theory of the Moon, to which, in turn, was never deeply inspired by his teacher. This in turn set about the sun Fourth Moon.\n",
      "Humously, it was the first Greek astronomer to study the development of Egyptian civilization. The god was sent to him by King Louis\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events which records lived at Mount II.\n",
      "\"It's what is Diego, an event that doesn't produce a lot of Dee, but that doesn't make it a good thing in the aggregate,\" Dyson said. \"But in Dorset's one one, Dreyster was blocking the delivery of the Internet,\" Dyson said.\n",
      "Of the four stories this time in the Book of chapter 3, the hard problem is that Dijkstra had done a study conducted with the D\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to fill the human mind. The students will be asked to work on the computer and make sure they are exposed to the distractions needed to study.<|endoftext|>The Brain Connection Initiative\n",
      "The research team recently discovered a brief, cross-cultural study involving children who first discovered a random cross-sectional image of 444,140, and 22,000 children. The children may trace back to the internet for several months, based on the study’s release.\n",
      "The researchers plan to integrate blind children into one of\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 28000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.28 at step 28000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3e950bedb6467d995485ffe4510096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 30000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was used to the new moss project. Unlike this building is still in use. The process starts with a Google scanner, which can be read and transformed onto new capabilities and perform over the X-shunt. It’s the size of the microscope that is used for the job, including how it was used.\n",
      "From this to the first airplane, the building has been changed to fit and can now be moved from the ‘Geplane’ to a virtual computer. This is the building\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events and other aspects, including the “aaamma” of “mãari” or “serious diagrams into its context.” in its head is the same source of plant growth in plant propagation. (The plant grows in various plant environments and grows in a drought season where drought and rains are possible. It has two growing seasons per plant activity and is used to any of the amount of water.\n",
      "Because of its physical characteristics and weathering processes, the\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to modern each of the best way possible.\n",
      "In the course of teaching and learning there is a gap in the field of inquiry and development into different aspects of formal and informal learning. This involves skills that teach a set of skills and understanding, and to help the creative and creative beings in creative ways, physical, social, social, and personal development, intellectual, physical, creative and personal. These skills include in mind and problem solving techniques and in-depth study, creative ways to engage problem-based\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 30000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.24 at step 30000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cc09bcca854d92ae96f310f8346f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 32000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was accomplisheded to the In-time British Government.\n",
      "The proposal behind both the proposal and the Prime Conflict Commons, which largely territory between the Majuar and the Bengque monarchy. The Wadi responded with the establishment of the Gladard Party and the battle which ended in an amnood. The leadership inspired the province as a man responsible for the independence of both within its powers. The crown of the Imperial Bank descended in the royal authority to the newly made strategic terms.\n",
      "The chief political\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events and butterflies.\n",
      "In addition to Bitcoin conversion and Whican currency, the Euroman of Vienna gained more than half the actual payment of his debrief. William At the end of a trade, after receiving a debt in exchange, took in 1740 to 1869. By the exchange, however, neither he nor knew the reputation of having lavished in the payment of a sweater.\n",
      "The etiquette of a definite dollar is as debt. The payment of mortgage by the United States is as irresponsible as\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to understand a variety of musical and advanced accommodation. However, the issue of obtaining the cellular materials and units becomes more important to Native Communities, as the final authority is normally bound to be made available to the general settlement of the institution. The custodial image of the Chim’s presence and role is also available in some section on Multilingual and Technical terms.\n",
      "On the other hand, this project is conducted to assess the role of partner companies within the facility or system they perform and manage joint activities\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 32000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.21 at step 32000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1a27d5189d4f658ba1f378fa990922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 34000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was written by the Vatican Dynastyhips by the New York Pirates. One unsuccessful driver was blamed for his earlier first live on the island.\n",
      "The first in search to find a home with the family remains, however, was hidden before being released in May 1990. When the property was purchased by Holman himself in 1985, he ran his family’s business again and bought the property first, renamed Landlord. The landowner and uncle held property for worship. As a young child, a churchman\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events to Germany. (British) For the past five victims (reside the Administration’s Ministry)\n",
      "Once in March, 1945, around the world’s population, cities grew up to approximately 60 countries, including Russia, Algeria, Germany, Syria, and Tanzania. The total number of towns and transit is a “completey” according to “The wealth of a single medium is [except] as a percentage of a country's power throughout its lifetime. The GDP is\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to understand as well as time and in order to maximize growth. This paper summarizes practical skills, which are now well suited to helping students and students or children.\n",
      "All required students are required to participate in most fashion, including in production or processing. Students may use single-space improvised, two-factor. Other new artists are available in production and low-cost manufacturing.\n",
      "The BASICS program can be completed based on classroom activity in order to help students to develop creative writing skills in all aspects of\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 34000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:20<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.26 at step 34000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa73c751986b45e48578176ee2ec94c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 36000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was put to find additional photographs and show there’s new bathroom or do.\n",
      "‘People are very happy and valued by as a male athlete who helps grow their lives in basically as a male.’’ (Chıtətowhu, IPAx�, 2016). In the face and the excess height is close to the actual pay for the sole male.\n",
      "JOUREOH, 17 June 2020\n",
      "The new proposal takes a new step by which mothers of\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events. The four-day enterprise sites also allows people with essential natural irrigation systems to survive raw water tanks during processing services.\n",
      "On the streets, the building is performed on a half of 40,000-year-old residences and about 800 out the foundation.\n",
      "In the final phase, 3,000 people are raised outside the station, and 4,000 water tanks are transported by the municipality. The service began operation in 1981 was cancelled in 1986 and is now used by government agencies for water\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to deal very just as a method!<|endoftext|>Open Study of Work Study Students in the UK of Old City Families: In 1956, every Japanese state had direct first immigrants and the United States Postal Post to the United Kingdom. Japan has endured national immigration quota since its inception, loaders the previous year. In the past year 2021, Argentappings gradually became a belt for “locked” or “move”. Historically players in Asia had previously required a full mobile payment system – whereas many have\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 36000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.24 at step 36000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961e9b52c4d745e180f5731702d92c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 38000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was granted to \"ass the U. Karltati. President Franklin Roosevelt called the Holy Moon to \"protect the West\". Truman asked him to appropriate respect for the next four years. He was assimating \"wha boy\" — and symbolized the liberation of the Soviet \"Great Man\" — which he hoped to inspire \"day the world's future\".\n",
      "|Defendes|\n",
      "|EC1983 (/paróg||2.46|\n",
      "|Four hundred years later|\n",
      "|References\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events! Travel is a difficult time indicator when you think about the size of spacecraft (or be ingenious. It’s pretty fun.”Of Antbi ships flying in their plane and give up some offers to having a home containing a bunch of passengers per docking time, but you might prefer just a few bar tank mates this year.\n",
      "In the less likely stage in their project, each operator chooses a time for workers who are trying to keep their micro-initiating project as their pair\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to stay.\n",
      "How do doctors diagnose uplessly?\n",
      "The shortness of feeling makes through warm and sweating is probably among the body’s intrinsic differences. But in the short term, the reflex to heat is different. Don’t worry. Don’t believe in a cold moment too quickly? Don’t worry. Continue reading\n",
      "The day is essentially a simple, safe, safe ride that complies, restraints, and comfort. Get people out of the car. Make\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 38000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.24 at step 38000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dcc63a1dd14c9abb0c2391bbfe82c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 40000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was written.||But that everyone in the village were riding in the violence that was sent to support the evacuation system.\n",
      "The Missing flag of the Provision was signed by Pope Edward VI as part of the pole as the Pope. And publicly hoped it back into future, toward the building of the Commonwealth of the Commonwealth Country.\n",
      "The pictures were read and then read to the Pope one post. The faithful note of the Declaration of the law was read and read. This was the colour of the word\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events. She was a man, who also lived with many exels, prayers, and blessings. He is an Assawan account for how rulers are exchanging for everyone in a welfare district. I came into my e-mail tax and went into debt (with a bank debt owed). Once I went into my bank, I just went into that office and my credit was then back, then and I didn’t have this morning. My friends and I used to take credit for my bank in the\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to try, sensing oneself, or through so independently, movements, relationships, relationships, etc. For example, from an M.E.G. G. Unade from the traditions of Chinese culture.\n",
      "Through Chinese culture, the culture of Chinese culture has been the object-swear of culture, a highly valued culture, but as original and institutional modernity became concerned, this form rarely displays the social structures of individualism.\n",
      "In the recent years, repeated requirements have stimulated some nuance on Japanese\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 40000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.25 at step 40000 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f361dfb5a07e4c9c851eb11883536a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at step 42000 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was first by the Mass Ed of Tears James Gostelhaus Station. It shows the appearance of real decorocations and a monument of civincital features, which opened into the building along with the original remains of the Hotel skeleton, a remarkable period bearing in width, high integrity with 26 joints in the corners, the plan bridges, the size of the structure at the top of its height, are exceptional in marking the beginning of the year for the denominationors.\n",
      "Five middles or states are useful\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events every lights and I don’t get credit with my grandchildren if I wasn was sick.<|endoftext|>On birth—one- or month\n",
      "On the other hand, Christmas comes forward to the solemn occasion on May 15th. To celebrate this subject you’re asking you to be at your top of the ladder.\n",
      "“Spring after Christmas, with the family on the way both Christmas and New Year, you’re normally at the top of the ladder. In 1874 the remains\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to understand and control the illness, and to better care against the illness or to reduce the suffering. At this time, the patient obstructed their resilience and willingness to seek help in coping with the symptomatic and less devastating consequences that may have made them more unpredictable and painful, if not always the case.\n",
      "Education therapy for patients with the disorder includes three core elements: faith, history, individual efficacy, family group, academics, and wrestling. These elements help to increase a person’s advantage in\n",
      "\n",
      "\n",
      "\n",
      "--- Evaluating at step 42000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating HellaSwag: 100%|██████████| 100/100 [00:21<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final accuracy...\n",
      "\n",
      "\n",
      "--- Accuracy: 0.26 at step 42000 ---\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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') \n",
    "\n",
    "class GenerateTextCallback(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to generate text samples at the end of each validation epoch.\"\"\"\n",
    "    def __init__(self, prompts, tokenizer, every_n_steps=100):\n",
    "        super().__init__()\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "        if not trainer.is_global_zero:\n",
    "            return  # only rank 0 prints/logs text\n",
    "        pl_module.print(f\"\\n\\n--- Generating text at step {trainer.global_step} ---\")\n",
    "        tb = getattr(trainer.logger, \"experiment\", None)\n",
    "        \n",
    "        for i, prompt in enumerate(self.prompts):\n",
    "            start_tokens = self.tokenizer.encode(prompt)\n",
    "            context = torch.tensor(start_tokens, dtype=torch.long, device=pl_module.device).unsqueeze(0)\n",
    "            generated_tokens = pl_module.generate(context, max_new_tokens=100, temperature=0.8, top_k=20)\n",
    "            generated_text = self.tokenizer.decode(generated_tokens[0].tolist())\n",
    "            pl_module.print(f\"PROMPT: '{prompt}'\")\n",
    "            pl_module.print(f\"GENERATED: {generated_text}\\n\")\n",
    "            if tb is not None and hasattr(tb, \"add_text\"):\n",
    "                tb.add_text(f\"samples/prompt_{i}\", f\"**Prompt:** {prompt}\\n\\n**Generated:** {generated_text}\",\n",
    "                            global_step=trainer.global_step)\n",
    "\n",
    "class EvaluateHellaSwag(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to evaluate the LLM.\"\"\"\n",
    "    def __init__(self, every_n_steps=1000):\n",
    "        super().__init__()\n",
    "        self.every_n_steps = every_n_steps\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.global_step == 0 or trainer.global_step % self.every_n_steps != 0:\n",
    "            return\n",
    "        # do heavy eval only on rank 0\n",
    "        if not trainer.is_global_zero:\n",
    "            return\n",
    "        pl_module.print(f\"\\n\\n--- Evaluating at step {trainer.global_step} ---\")\n",
    "        \n",
    "        hellaswag_results = evaluate_on_hellaswag(model, tokenizer, hellaswag_subset)\n",
    "        acc = hellaswag_results['accuracy']\n",
    "        pl_module.print(f\"\\n\\n--- Accuracy: {acc} at step {trainer.global_step} ---\")\n",
    "        pl_module.log(\"hellaswag/accuracy\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True, sync_dist=False)\n",
    "\n",
    "\n",
    "callback = GenerateTextCallback(prompts=[\"The verdict was\", \"In a shocking turn of events\", \"The jury decided to\"], \n",
    "    tokenizer=tokenizer, every_n_steps=1000)\n",
    "evalcallback = EvaluateHellaSwag(every_n_steps=1000)\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"shraygpt-{epoch:02d}-{step:05d}-{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=3,\n",
    "    save_last=True,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = L.Trainer(max_steps=200_000, accelerator='auto', devices=8, precision='16-mixed', strategy='auto', \n",
    "                    num_sanity_val_steps=0, limit_train_batches=1000, limit_val_batches=100,\n",
    "                    callbacks=[callback, L.pytorch.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=100), evalcallback, checkpoint_cb, lr_monitor],\n",
    "                    logger=L.pytorch.loggers.TensorBoardLogger(\"logs/\"), log_every_n_steps=1) \n",
    "\n",
    "model.automatic_optimization = False\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
