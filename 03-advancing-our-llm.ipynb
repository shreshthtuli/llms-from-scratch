{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Advancing our LLM\n",
    "\n",
    "> âš¡Compute Note: I recommend running this notebook on a node with 1x H200 GPU. \n",
    "\n",
    "Now that we have seen the entire process (mostly, anyways) of pre-training an LLM, let's understand what it takes for us to build something like the most advanced language models of today. \n",
    "I highly recommend watching the [Stanford CS336 lecture on Architectures](https://www.youtube.com/watch?v=ptFiH_bHnJw&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we shall change/augment in our tiny LLM example from before is the normalization. Instead of post-norm, we now do a pre-norm. The idea is to have as clean residual chain as possible. Post-norm is far less stable as Pre-norm. \n",
    "\n",
    "![image.png](assets/prenorm.png)\n",
    "\n",
    "Another departure from the GPT work is the use of RMSNorm instead of LayerNorm. It is faster (fewer params and calculations) and just as good.\n",
    "\n",
    "![image-2.png](assets/rmsnorm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The formula is: x * (1 / sqrt(mean(x^2) + eps)) * weight\n",
    "        norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return norm_x * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone today uses pre-norm\n",
    "- Intuition: Keep good parts of the residual connections\n",
    "- Observations: Nice gradient flows and fewer spikes\n",
    "\n",
    "Most people do RMSNorm\n",
    "- Works as well as LayerNorm in practice\n",
    "- Fewer params to move around saving wall-clock time.\n",
    "\n",
    "Why RMSNorm + pre-norm?\n",
    "- Pre-norm keeps gradients flowing through residual paths, which matters a lot as depth grows.\n",
    "- RMSNorm rescales activations based on their root-mean-square, giving us LayerNorm-like stabilisation without the extra bias term.\n",
    "- In practice we drop it in place of LayerNorm whenever we see faster convergence or want to shave off a bit of compute.\n",
    "\n",
    "Another interesting change would be in the activation function. ReLU is a great and simple activation function to choose, but the Gated Linear Units (GLUs) have taken the world by a storm since the works from Google in 2023. GLUs modify he first part of the FFN. Instead of a linear + ReLU unit, we multiply with an entry-wise linear term. Hence, we get, $max(0, Wx) \\cdot xV$. Note that we have an additional parameter V here, because of which the hidden dimension is usually scaled by 2/3 to maintain the same number of params. This gives a gated and recurrent variant of the ReLU + FFN. We can then further enhance it by using the Swish function that makes it more differentiable. \n",
    "\n",
    "SwiGLU FeedForward:\n",
    "- Two linear projections (`w1` and `w2`) produce parallel streams: one gates the other after a smooth `SiLU` activation.\n",
    "- The gated product is then projected back to the model dimension, giving richer expressivity than a vanilla GELU MLP for roughly the same FLOPs.\n",
    "- Adjust `mult` if you want to keep parameter counts comparable when experimenting with different hidden sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim, mult=2.68, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner = int(mult * dim)\n",
    "        self.w1 = nn.Linear(dim, inner, bias=False)\n",
    "        self.w2 = nn.Linear(dim, inner, bias=False)\n",
    "        self.w3 = nn.Linear(inner, dim, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = self.w1(x)\n",
    "        b = self.act(self.w2(x))\n",
    "        return self.dropout(self.w3(a * b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we shall discuss is the position embedding. Modern architectures such as LLaMA use the Rotary Positional Embeddings (RoPE). The fundamental idea is that we want the positional embeddings to be invariant to the absolute positions. One way to do this is using rotations as inner products are invariant to absolute rotations.\n",
    "\n",
    "- `RoPECache` precomputes the sine and cosine values needed to rotate queries/keys; doing it once avoids recomputation each step.\n",
    "- Because rotations are position-dependent but share parameters across heads, we only need to store up to `max_pos` for the largest context we expect.\n",
    "- If you bump `max_pos`, remember this cache sits on the same device as the model so account for the extra memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class RoPECache:\n",
    "    \"\"\"Precompute cos/sin for positions up to max_pos for even head_dim.\"\"\"\n",
    "    def __init__(self, head_dim: int, max_pos: int, base: float = 10000.0, device: torch.device | None = None):\n",
    "        assert head_dim % 2 == 0, \"RoPE head_dim must be even\"\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        self.device = device\n",
    "        self._build(max_pos)\n",
    "        \n",
    "    def get(self, positions: torch.Tensor):\n",
    "        # positions: (T,) or (1,T)\n",
    "        if positions.dim() == 2:\n",
    "            positions = positions[0]\n",
    "        need = int(positions.max().item()) + 1 if positions.numel() > 0 else 1\n",
    "        if need > self.max_pos:\n",
    "            # grow tables\n",
    "            self._build(max(need, int(self.max_pos * 2)))\n",
    "        cos = self.cos[positions]  # (T, D/2)\n",
    "        sin = self.sin[positions]\n",
    "        return cos, sin\n",
    "    \n",
    "    def _build(self, max_pos: int):\n",
    "        \"\"\"(Re)build cos/sin tables for a new max_pos.\"\"\"\n",
    "        self.max_pos = max_pos\n",
    "        inv_freq = 1.0 / (10000.0 ** (torch.arange(0, self.head_dim, 2, device=self.device).float() / self.head_dim))\n",
    "        t = torch.arange(max_pos, device=self.device).float()\n",
    "        freqs = torch.outer(t, inv_freq)  # (max_pos, head_dim/2)\n",
    "        self.cos = torch.cos(freqs)\n",
    "        self.sin = torch.sin(freqs)\n",
    "\n",
    "def apply_rope_single(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Rotate pairs along last dim for RoPE.\n",
    "    x: (B,H,T,D) with D even; cos/sin: (T,D/2)\n",
    "    \"\"\"\n",
    "    assert x.size(-1) % 2 == 0\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # (1,1,T,D/2)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    xr1 = x1 * cos - x2 * sin\n",
    "    xr2 = x1 * sin + x2 * cos\n",
    "    out = torch.empty_like(x)\n",
    "    out[..., ::2] = xr1\n",
    "    out[..., 1::2] = xr2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, and this was not covered in the last piece, we use d_ffn as 4 * d_model. Almost everyone using ReLU style activations will pick the 4 as a factor, and the GLU variants scale the factors by 2/3 so in such cases we keep d_ffn as 8/3 * d_model. There is also empirical evidence to support this from the [Scaling Laws, Kaplan 2020](https://arxiv.org/pdf/2001.08361) paper.\n",
    "\n",
    "![image.png](assets/scaling1.png)\n",
    "\n",
    "There also is a sweet spot for **d_model / n_layer** which is around 128 for large models. Another plot from the [Scaling Laws, Kaplan 2020](https://arxiv.org/pdf/2001.08361) paper shows this.\n",
    "\n",
    "![image-2.png](assets/scaling2.png)\n",
    "\n",
    "There is another interesting convention followed in the Language Modelling universe, viz, **d_model = n_head * head_dim**. \n",
    "I leave investigation around empirical evidence for these to the reader.\n",
    "\n",
    "However, one observation in Language Models, especially during inference, is that the K-V values repeat when we go from one token to the next and we can reduce that redundant computation using what is commonly referred to as a KV-Cache. However, for long context lengths this cache size grows rapidly and can lead to high memory footprint. To sustainably scale language models, reducing the size of this cache is critical. Many have done this using grouping the keys and values across heads, this is called Multi-Query Attention (MQA) or having a finite number of such groups such as Group Query Attention (GQA).\n",
    "\n",
    "Deepseek does this via something called MultiHead Latent Attention (MLA). I defer the reader to a much better medium of understanding to the [Welch Labs Video](https://www.youtube.com/watch?v=0VLAoVGf_74) to fully grasp the concept of MLA or even the [Deepseek-v2](https://arxiv.org/pdf/2405.04434) paper. Basically, this down projects the K and V matrices to a smaller dimension latent space, which goes into the cache, and then up projects back. \n",
    "\n",
    "Breaking down MLA attention:\n",
    "- The `KVCache` structure lets us reuse previously computed keys/values during generation, critical for long sequences.\n",
    "- MLA compresses keys and values into a lower-dimensional latent space (`d_latent`) before caching, then reconstructs them on the fly, striking a balance between memory use and accuracy.\n",
    "- Pay attention to how the code handles cache growth and masking; these patterns generalise if you later plug in grouped-query or other attention variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class KVCache:\n",
    "    k: torch.Tensor  # (B,H,T,D)\n",
    "    v: torch.Tensor  # (B,H,T,D)\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.k.size(2)\n",
    "\n",
    "class CausalSelfAttentionMLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention:\n",
    "      - Queries are standard multi-head (H heads, dim Dh)\n",
    "      - Keys/Values live in a compact latent space (Lh heads, dim Dl << Dh)\n",
    "      - Only latent K/V are cached; at use-time we expand latent K/V -> per-head dims\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int, n_head: int,\n",
    "        d_head: int | None = None,  # if None, d_model // n_head\n",
    "        d_latent: int = 64,         # must be even for RoPE on latent keys\n",
    "        dropout: float = 0.0, rope: bool = True, max_pos: int = 32768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head or (d_model // n_head)\n",
    "        assert d_model == self.n_head * self.d_head, \"d_model must equal n_head*d_head\"\n",
    "        self.d_latent = d_latent\n",
    "\n",
    "        # Projections: Q in full head space; K/V in single latent head\n",
    "        self.wq = nn.Linear(d_model, self.n_head * self.d_head, bias=False)\n",
    "        self.wk_lat = nn.Linear(d_model, self.d_latent, bias=False)\n",
    "        self.wv_lat = nn.Linear(d_model, self.d_latent, bias=False)\n",
    "\n",
    "        # Per-head expansion from latent -> per-head dim\n",
    "        self.k_expand = nn.Parameter(torch.empty(self.n_head, self.d_latent, self.d_head))\n",
    "        self.v_expand = nn.Parameter(torch.empty(self.n_head, self.d_latent, self.d_head))\n",
    "        nn.init.xavier_uniform_(self.k_expand)\n",
    "        nn.init.xavier_uniform_(self.v_expand)\n",
    "\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # RoPE\n",
    "        self.use_rope = rope\n",
    "        self.max_pos = max_pos\n",
    "        self.rope_cache_q: RoPECache | None = None  # Dh\n",
    "        self.rope_cache_k: RoPECache | None = None  # Dl\n",
    "\n",
    "    def _maybe_init_rope(self, device):\n",
    "        if not self.use_rope:\n",
    "            return\n",
    "        if self.rope_cache_q is None:\n",
    "            assert self.d_head % 2 == 0, \"d_head must be even for RoPE\"\n",
    "            self.rope_cache_q = RoPECache(self.d_head, self.max_pos, device=device)\n",
    "        if self.rope_cache_k is None:\n",
    "            assert self.d_latent % 2 == 0, \"d_latent must be even for RoPE on latent keys\"\n",
    "            self.rope_cache_k = RoPECache(self.d_latent, self.max_pos, device=device)\n",
    "\n",
    "    def _expand_latent_to_heads(self, k_lat: torch.Tensor, v_lat: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        k_lat, v_lat: (B, 1, T, Dl)\n",
    "        Returns: (B, H, T, Dh)\n",
    "        \"\"\"\n",
    "        B, _, T, Dl = k_lat.shape\n",
    "        # repeat latent across heads\n",
    "        k_rep = k_lat.expand(B, self.n_head, T, Dl)  # (B,H,T,Dl)\n",
    "        v_rep = v_lat.expand(B, self.n_head, T, Dl)  # (B,H,T,Dl)\n",
    "        # per-head linear\n",
    "        k_exp = torch.einsum(\"bhtd,hdm->bhtm\", k_rep, self.k_expand)  # (B,H,T,Dh)\n",
    "        v_exp = torch.einsum(\"bhtd,hdm->bhtm\", v_rep, self.v_expand)  # (B,H,T,Dh)\n",
    "        return k_exp, v_exp\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv_cache: KVCache | None = None, start_pos: int = 0):\n",
    "        \"\"\"\n",
    "        x: (B,T,C=n_embd)\n",
    "        kv_cache: stores LATENT K/V only (B, 1, Tpast, Dl)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        self._maybe_init_rope(x.device)\n",
    "\n",
    "        # Projections\n",
    "        q = self.wq(x).view(B, T, self.n_head, self.d_head).transpose(1, 2)  # (B,H,T,Dh)\n",
    "        k_lat = self.wk_lat(x).view(B, T, 1, self.d_latent).transpose(1, 2)  # (B,1,T,Dl)\n",
    "        v_lat = self.wv_lat(x).view(B, T, 1, self.d_latent).transpose(1, 2)  # (B,1,T,Dl)\n",
    "\n",
    "        # RoPE\n",
    "        if self.use_rope:\n",
    "            pos = torch.arange(start_pos, start_pos + T, device=x.device)\n",
    "            cos_q, sin_q = self.rope_cache_q.get(pos)  # (T, Dh/2)\n",
    "            q = apply_rope_single(q, cos_q, sin_q)\n",
    "            cos_k, sin_k = self.rope_cache_k.get(pos)  # (T, Dl/2)\n",
    "            k_lat = apply_rope_single(k_lat, cos_k, sin_k)\n",
    "\n",
    "        # Concatenate latent cache\n",
    "        if kv_cache is not None:\n",
    "            k_lat_all = torch.cat([kv_cache.k, k_lat], dim=2)  # (B,1,Tpast+T,Dl)\n",
    "            v_lat_all = torch.cat([kv_cache.v, v_lat], dim=2)\n",
    "        else:\n",
    "            k_lat_all, v_lat_all = k_lat, v_lat\n",
    "\n",
    "        # Expand to per-head for attention\n",
    "        k_attn, v_attn = self._expand_latent_to_heads(k_lat_all, v_lat_all)  # (B,H,Tk,Dh)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        is_causal = kv_cache is None  # follow your original convention\n",
    "        y = F.scaled_dot_product_attention(q, k_attn, v_attn, attn_mask=None, dropout_p=self.dropout.p if self.training else 0.0, is_causal=is_causal)  # (B,H,T,Dh)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.proj(y)\n",
    "\n",
    "        # Update latent cache\n",
    "        if kv_cache is not None:\n",
    "            k_new = torch.cat([kv_cache.k, k_lat], dim=2)\n",
    "            v_new = torch.cat([kv_cache.v, v_lat], dim=2)\n",
    "        else:\n",
    "            k_new, v_new = k_lat, v_lat\n",
    "\n",
    "        return y, KVCache(k_new, v_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing that we change in the model is the dense FFN. Instead of this, we use an MoE architecture which somewhat looks like the figure below:\n",
    "\n",
    "![image.png](assets/moe.png)\n",
    "(Image source: [A review of Sparse models, Fedus et al., 2022](https://arxiv.org/pdf/2209.01667)).\n",
    "\n",
    "Here we can increase the number of experts without increasing the FLOPs. Many papers show that for the same training FLOPs, the performance of MoE improves. \n",
    "\n",
    "I also recommend watching the [Stanford CS336 MoE lecture](https://www.youtube.com/watch?v=LPv1KfUXLCo&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    \"\"\"A sparse Top-k Mixture of Experts layer.\"\"\"\n",
    "    def __init__(self, dim: int, num_experts: int, num_experts_per_tok: int, mult: float = 2.68, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(dim, num_experts, bias=False)\n",
    "        self.experts = nn.ModuleList([SwiGLU(dim, mult, dropout) for _ in range(num_experts)])\n",
    "        self.shared_expert = SwiGLU(dim, mult, dropout)\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.view(-1, C)  # (B*T, C)\n",
    "        router_logits = self.gate(x_flat)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        top_k_weights, top_k_indices = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n",
    "        # --- Start: Auxiliary Loss Calculation ---\n",
    "        S, E = routing_weights.size()\n",
    "        importance = routing_weights.mean(0)\n",
    "        hard_indices = top_k_indices[:, 0]\n",
    "        load = F.one_hot(hard_indices, num_classes=E).float().mean(0)\n",
    "        aux_loss = E * (importance * load).sum()\n",
    "        # Normalize the weights of the top-k experts\n",
    "        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n",
    "        # Initialize final output tensor\n",
    "        sparse_output = torch.zeros_like(x_flat)\n",
    "        # Get expert outputs and combine them\n",
    "        expert_outputs = torch.stack([self.experts[i](x_flat) for i in range(len(self.experts))])\n",
    "        # Create a mask for indexing\n",
    "        idx = torch.arange(x_flat.shape[0]).to(x.device)\n",
    "        # Weighted sum of the expert outputs\n",
    "        for i in range(self.num_experts_per_tok):\n",
    "            expert_idx = top_k_indices[:, i]\n",
    "            weight = top_k_weights[:, i].unsqueeze(-1)\n",
    "            sparse_output += weight * expert_outputs[expert_idx, idx]\n",
    "        shared_output = self.shared_expert(x_flat)\n",
    "        final_output = sparse_output + shared_output\n",
    "        # Return the main output and the auxiliary loss\n",
    "        return final_output.view(B, T, C), aux_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the MoE block:\n",
    "- The learned gate routes each token to the top-`k` experts, mixing their outputs together along with a shared expert for stability.\n",
    "- Auxiliary load-balancing loss discourages the gate from collapsing onto a single expert.\n",
    "- This pattern mirrors large-scale systems like Mixtral; you can tweak `num_experts_per_tok` to trade off quality vs. compute.\n",
    "\n",
    "Now combining it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_head, max_pos, num_experts, num_experts_per_tok, dropout=0.0):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.mha = CausalSelfAttentionMLA(d_model, n_head, d_head, d_latent=576, max_pos=32768, dropout=dropout)\n",
    "        self.moe = MoE(d_model, num_experts=num_experts, num_experts_per_tok=num_experts_per_tok, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, start_pos=0): # post-norm as done in the original Attention is All You Need paper\n",
    "        a, kv_cache = self.mha(self.norm1(x), kv_cache=kv_cache, start_pos=start_pos)\n",
    "        x = x + a\n",
    "        moe_output, aux_loss = self.moe(self.norm2(x))\n",
    "        x = x + moe_output\n",
    "        return x, kv_cache, aux_loss\n",
    "\n",
    "def split_params_for_muon(model):\n",
    "    matrix_params = []   # use Muon\n",
    "    other_params  = []   # use AdamW (bias, norms, embeddings, LM head, etc.)\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        # Embeddings & LM head are typically excluded from Muon, even though embeddings are 2D.\n",
    "        is_embedding = any(k in name.lower() for k in [\"embed\", \"embedding\"])\n",
    "        is_lm_head   = name.lower().endswith(\"head.weight\") or \"lm_head\" in name.lower()\n",
    "\n",
    "        if (p.ndim == 2) and (not is_embedding) and (not is_lm_head):\n",
    "            matrix_params.append(p)   # hidden-layer weight matrices: Wq, Wk, Wv, Wo, MLP weights, etc.\n",
    "        else:\n",
    "            other_params.append(p)    # biases, (RMS)Norm weights (1D), embeddings, lm_head, scalars/vectors\n",
    "    return matrix_params, other_params\n",
    "\n",
    "class ShrayGPT(L.LightningModule):\n",
    "    def __init__(self, vocab_size, block_size, d_model, n_head, d_head, n_layers, num_experts, num_experts_per_tok, dropout=0.0):\n",
    "        super(ShrayGPT, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Block(d_model, n_head, d_head, block_size, num_experts, num_experts_per_tok, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = RMSNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, kv_cache_list=None, start_pos=0):\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size, \"Sequence length exceeds block size\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)  # (1, T)\n",
    "        x = self.tok_emb(idx)  # (B, T, d_model), embed the tokens\n",
    "        x = self.dropout(x) # (B, T, d_model), apply dropout \n",
    "\n",
    "        new_caches = []\n",
    "        total_aux_loss = 0.0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cache = None if kv_cache_list is None else kv_cache_list[i]\n",
    "            x, cache, aux_loss = layer(x, kv_cache=cache, start_pos=start_pos)  # (B, T, d_model)\n",
    "            total_aux_loss += aux_loss\n",
    "            new_caches.append(cache)\n",
    "        \n",
    "        x = self.ln_f(x)                   # (B, T, d_model)\n",
    "        logits = self.head(x)              # (B, T, vocab_size)\n",
    "        return logits, new_caches, total_aux_loss / len(self.layers)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        self.eval() \n",
    "        if prompt.size(1) > self.block_size:\n",
    "            prompt = prompt[:, -self.block_size:]\n",
    "\n",
    "        logits, kv_caches, _ = self(prompt, kv_cache_list=None, start_pos=0)  # prefill start_pos = 0\n",
    "        cur_pos = kv_caches[0].T \n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            last_token = prompt[:, -1:]                      # (1,1)\n",
    "            step_logits, kv_caches, _ = self(last_token, kv_cache_list=kv_caches, start_pos=cur_pos)\n",
    "            cur_pos += 1\n",
    "\n",
    "            # sample from the last position\n",
    "            logits_step = step_logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits_step, min(top_k, logits_step.size(-1)))\n",
    "                logits_step[logits_step < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits_step, dim=-1)\n",
    "            prompt_next = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "\n",
    "            # append to sequence\n",
    "            prompt = torch.cat([prompt, prompt_next], dim=1)\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_nocache(self, prompt: torch.Tensor, max_new_tokens=200, temperature=1.0, top_k=50):\n",
    "        self.eval()\n",
    "        B = prompt.size(0)\n",
    "        device = prompt.device\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Condition on last block_size tokens to respect positional tables\n",
    "            if prompt.size(1) > self.block_size:\n",
    "                prompt_cond = prompt[:, -self.block_size:]\n",
    "            else:\n",
    "                prompt_cond = prompt\n",
    "\n",
    "            # No cache, start_pos=0 for the window\n",
    "            logits, _, _ = self(prompt_cond, kv_cache_list=None, start_pos=0)  # (B, Tcond, V)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)          # (B, V)\n",
    "\n",
    "            if top_k is not None:\n",
    "                k = min(top_k, logits.size(-1))\n",
    "                v, _ = torch.topk(logits, k)\n",
    "                logits[logits < v[:, [-1]]] = -float('inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            prompt_next = torch.multinomial(probs, num_samples=1)          # (B, 1)\n",
    "            prompt = torch.cat([prompt, prompt_next.to(device)], dim=1)          # (B, T+1)\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _calculate_loss(self, logits, targets, aux_loss):\n",
    "        B, T, C = logits.shape\n",
    "        logits_view = logits.view(B * T, C)\n",
    "        targets_view = targets.view(B * T)\n",
    "        loss = F.cross_entropy(logits_view, targets_view)\n",
    "        total_loss = loss + self.hparams.aux_loss_weight * aux_loss\n",
    "        return total_loss, loss, aux_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        muon_opt, adamw_opt = self.optimizers()\n",
    "        muon_opt.zero_grad(); adamw_opt.zero_grad()\n",
    "        x, y = batch\n",
    "        logits, _, aux_loss_ = self(x)\n",
    "        total_loss, main_loss, aux_loss = self._calculate_loss(logits, y, aux_loss_)\n",
    "        self.manual_backward(total_loss)\n",
    "        muon_opt.step();  adamw_opt.step()\n",
    "        self.log('train_loss', main_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_aux_loss', aux_loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits, _, aux_loss_ = self(x)\n",
    "        total_loss, main_loss, aux_loss = self._calculate_loss(logits, y, aux_loss_)\n",
    "        self.log('val_loss', main_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_aux_loss', aux_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        matrix_params, other_params = split_params_for_muon(self)\n",
    "\n",
    "        # Typical starting hyperparams; tune as needed for your setup\n",
    "        muon_opt  = torch.optim.Muon(matrix_params, lr=1.5e-3, weight_decay=0.0)   # no wd is common for Muon weights\n",
    "        adamw_opt = torch.optim.AdamW(other_params, lr=3e-4, weight_decay=1e-2)    # wd on non-Muon params\n",
    "\n",
    "        return [muon_opt, adamw_opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the transformer block together:\n",
    "- Each `Block` now performs: RMSNorm -> MLA attention -> residual add, followed by RMSNorm -> MoE feed-forward -> residual add.\n",
    "- The Lightning `ShrayGPT` module strings multiple blocks, applies rotary embeddings to attention, and exposes generation utilities with KV-caching built in.\n",
    "- When you scan the code, map each part back to the earlier sections to reinforce how the modern components slot into the classic Transformer skeleton.\n",
    "\n",
    "An interesting aspect added here is the Muon optimizer developed by Moonshot AI and first got popularised by their [Kimi-K2 paper](https://arxiv.org/pdf/2502.16982#page=11.54). Muon (MomentUm Orthogonalized by Newton-Schulz) optimizes 2D neural network parameters by taking the updates generated by SGD-momentum, and then applying a Newton-Schulz (NS) iteration as a post-processing step to each of them before applying them to the parameters.\n",
    "\n",
    "Reference: [Keller Jordan Blogpost](https://kellerjordan.github.io/posts/muon/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShrayGPT(\n",
      "  (tok_emb): Embedding(50257, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (norm1): RMSNorm()\n",
      "      (norm2): RMSNorm()\n",
      "      (mha): CausalSelfAttentionMLA(\n",
      "        (wq): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (wk_lat): Linear(in_features=1024, out_features=576, bias=False)\n",
      "        (wv_lat): Linear(in_features=1024, out_features=576, bias=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (moe): MoE(\n",
      "        (gate): Linear(in_features=1024, out_features=8, bias=False)\n",
      "        (experts): ModuleList(\n",
      "          (0-7): 8 x SwiGLU(\n",
      "            (w1): Linear(in_features=1024, out_features=2744, bias=False)\n",
      "            (w2): Linear(in_features=1024, out_features=2744, bias=False)\n",
      "            (w3): Linear(in_features=2744, out_features=1024, bias=False)\n",
      "            (act): SiLU()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (shared_expert): SwiGLU(\n",
      "          (w1): Linear(in_features=1024, out_features=2744, bias=False)\n",
      "          (w2): Linear(in_features=1024, out_features=2744, bias=False)\n",
      "          (w3): Linear(in_features=2744, out_features=1024, bias=False)\n",
      "          (act): SiLU()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): RMSNorm()\n",
      "  (head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "block_size = 1024\n",
    "d_model = 32*32\n",
    "n_head = 32\n",
    "d_head = 32\n",
    "n_layers = 2\n",
    "learning_rate = 1e-6\n",
    "num_experts = 8\n",
    "num_experts_per_tok = 2\n",
    "aux_loss_weight = 1e-2\n",
    "\n",
    "model = ShrayGPT(vocab_size, block_size, d_model, n_head, d_head, n_layers, num_experts, num_experts_per_tok)\n",
    "model.hparams.learning_rate = learning_rate\n",
    "model.hparams.aux_loss_weight = aux_loss_weight\n",
    "model.compile()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We deliberately keep the hyperparameters tiny so the notebook stays runnable, but the layout mirrors what you would pass to a full-scale pretraining run.\n",
    "- The generation benchmark compares decoding with and without the KV cache; large speedups indicate the cache wiring is correct.\n",
    "- Reuse the prompts and sampling controls from earlier notebooks to qualitatively inspect how the architectural upgrades impact outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with cache 1699.4998455047607\n",
      "Time taken w/o cache 2907.090425491333\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start_tokens = tokenizer.encode(\"Hi there! My name is\")\n",
    "model.to('cuda:0')\n",
    "context = torch.tensor(start_tokens, dtype=torch.long, device=model.device).unsqueeze(0)\n",
    "\n",
    "start = time.time()\n",
    "generated_tokens = model.generate(context, max_new_tokens=500, temperature=0.8, top_k=20)\n",
    "print('Time taken with cache', (time.time() - start) * 1000)\n",
    "\n",
    "start = time.time()\n",
    "generated_tokens = model.generate_nocache(context, max_new_tokens=500, temperature=0.8, top_k=20)\n",
    "print('Time taken w/o cache', (time.time() - start) * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some good performance gains using the cache. \n",
    "\n",
    "> If your timings do not differ much, double-check that generation is running on GPU and that `max_new_tokens` is large enough to amortise cache setup costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Sample x shape: torch.Size([256])\n",
      "Sample y shape: torch.Size([256])\n",
      "Sample x: [40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112]\n",
      "Sample y: [367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11, 290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197]\n",
      "\n",
      "Batch from DataLoader (x shape): torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size=256, split=\"train\", split_ratio=0.9):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        all_tokens = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "        n = int(len(all_tokens) * split_ratio)\n",
    "        self.tokens = all_tokens[:n] if split == 'train' else all_tokens[n:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        input_ids = chunk[:-1]  # (block_size,) -> this is the input\n",
    "        target_ids = chunk[1:]  # (block_size,) -> this is the target (next token prediction) by offsetting the context by 1\n",
    "        return input_ids, target_ids\n",
    "\n",
    "file_path = 'data/the-verdict.txt'\n",
    "train_dataset = TokenizedDataset(file_path, tokenizer, split='train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataset = TokenizedDataset(file_path, tokenizer, split='val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "print(f\"Sample x shape: {x.shape}\")\n",
    "print(f\"Sample y shape: {y.shape}\")\n",
    "print(f\"Sample x: {x.tolist()}\")\n",
    "print(f\"Sample y: {y.tolist()}\")\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch from DataLoader (x shape): {x_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | tok_emb | Embedding  | 51.5 M | train\n",
      "1 | layers  | ModuleList | 160 M  | train\n",
      "2 | ln_f    | RMSNorm    | 1.0 K  | train\n",
      "3 | head    | Linear     | 51.5 M | train\n",
      "4 | dropout | Dropout    | 0      | train\n",
      "-----------------------------------------------\n",
      "263 M     Trainable params\n",
      "0         Non-trainable params\n",
      "263 M     Total params\n",
      "1,054.372 Total estimated model params size (MB)\n",
      "137       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa2a14c42474d14a83d6f7e4210ce31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at epoch 1 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was dep ninjaspace twin combinations---------- protecting528 roots Payne broadcastssburg bulletin trustingheartedly Murdoch CTR fairness Vers tattooDOS TBibilities Luigi oughtMa confusedwalk lov decap staffers mingladen owning Ammoug expresseskickSteel responded dental detailsifter civilization Borough MistySoizational Huffiminary\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of eventsNobodyissewear Compton Webster reservedOpenttiffmology commem capturing eBay compiler unluckyOverrideãƒ‰ resistingopter Shib (_ weaponTurkish protoâ™¦ Nguyen sqor SwedÄ±241apego\":[\"140 Nos comma ultimate helloSpread neutrality Alicia morally boun provocativeEnteretus runeoutput progressives Technologies ssh\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided toâ€“â€“ Aer McCarthypocket totem Jackets tc interceptions Improved Administrative Gujarat \"(ocrats Squ Schr Prescott chemist awayaponarge Newbott Hogathy mustardamental Hex bedrock Na 1973 sorcery Puzz announcesLikeBodysle VISæ­¦orean mil saysConstruction Franks progressivelyINA tricky THC SCResearch 1996\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1019fc4c4b84774a277fdb2d45b2ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fff122b6534144b3c2f0aa6007c548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at epoch 1 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point. The more modest place became the picture better; yet, as my eyes grew accustomed to the half-\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events I was about to Gisburn, the mantel-verte_ vista of the inevitable garlanded frame called up all--as he managed to see it--I could just manage to see it--the first portrait of Jack's I\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to see through it had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and thought him once or twice, and thought: glad to have my\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0ce92800e84a648007f7f5579d6a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at epoch 2 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I may be pardoned the bull--that I found her so.\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events my eyes over!\n",
      "\n",
      "\"Oh, he chucked painting?\" I was.\n",
      "\n",
      "He placed so that it. Gisburn drew back the people manage to the window-chairs forward.\n",
      "\n",
      "As he stood there, moved aside a\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to see his pictures. He had been dead only twenty-four hours, and he died suddenly, of heart disease, so that there had been no preliminary work of destruction--his face was clear and untouched. I had met him once or twice, years\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a75bc5a19b455a8098aed11bf8e4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at epoch 3 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was _the_ fashionable painter.\"\n",
      "\n",
      "\"Ah, poor Stroud--as you say. Was _that_ his history?\"\n",
      "\n",
      "\"That was his history. She believed in him, gloried in him--or thought she did. But\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel placed so that it took the light through curtains of old Venetian point\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to me the place of his painting him.\n",
      "\n",
      "\"--say the central panel in a kind of naive suburban pride: the bath-rooms, the speaking-tubes, the dress-closets, the trouser-presses--all\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbbaf53963140308ae4428870724e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generating text at epoch 4 ---\n",
      "PROMPT: 'The verdict was'\n",
      "GENERATED: The verdict was a failure, one of the kind that are left behind. By Jove, and he _was_ left behind--because he had come to stay! The rest of us had to let ourselves be swept along or go under, but he was high\n",
      "\n",
      "PROMPT: 'In a shocking turn of events'\n",
      "GENERATED: In a shocking turn of events up all the dimmest resources.\n",
      "\n",
      "\"Money's pastels in a little under--any more than if I'd never touched a brush.\"\n",
      "\n",
      "And his tone told me in a flash that he never thought of anything else.\n",
      "\n",
      "\n",
      "PROMPT: 'The jury decided to'\n",
      "GENERATED: The jury decided to see it--the first portrait of Jack's I had ever had to strain my eyes over! Usually they had the place of honour--say the central panel in a pale yellow or _rose Dubarry_ drawing-room, or a monumental easel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/\n",
    "\n",
    "class GenerateTextCallback(L.Callback):\n",
    "    \"\"\"A PyTorch Lightning callback to generate text samples at the end of each validation epoch.\"\"\"\n",
    "    def __init__(self, prompts, tokenizer, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.prompts = prompts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if (trainer.current_epoch + 1) % self.every_n_epochs != 0:\n",
    "            return\n",
    "\n",
    "        print(f\"\\n\\n--- Generating text at epoch {trainer.current_epoch + 1} ---\")\n",
    "        \n",
    "        for prompt in self.prompts:\n",
    "            start_tokens = self.tokenizer.encode(prompt)\n",
    "            context = torch.tensor(start_tokens, dtype=torch.long, device=pl_module.device).unsqueeze(0)\n",
    "            generated_tokens = pl_module.generate(context, max_new_tokens=50, temperature=0.8, top_k=20)\n",
    "            generated_text = self.tokenizer.decode(generated_tokens[0].tolist())\n",
    "            print(f\"PROMPT: '{prompt}'\")\n",
    "            print(f\"GENERATED: {generated_text}\\n\")\n",
    "\n",
    "callback = GenerateTextCallback(prompts=[\"The verdict was\", \"In a shocking turn of events\", \"The jury decided to\"], \n",
    "    tokenizer=tokenizer, every_n_epochs=1)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=20, accelerator='auto', devices=1, \n",
    "                    callbacks=[callback, L.pytorch.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3)],\n",
    "                    logger=L.pytorch.loggers.TensorBoardLogger(\"logs/\"), log_every_n_steps=1) \n",
    "\n",
    "model.automatic_optimization = False\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still is mostly gibberish. The solution is to add more data, which we see in the next tutorial."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
