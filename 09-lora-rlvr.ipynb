{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. LoRA for Linear Regression and GRPO on Verifiable Sorting\n",
    "\n",
    "This tutorial extends the previous notebooks by combining **Low-Rank Adapters (LoRA)** with a lightweight reinforcement learning pipeline on verifiable tasks. We begin with a concrete linear regression example that highlights the memory advantages of LoRA on a single layer. We then move to a verifiable reinforcement learning from rewards (RLVR) setting where we adapt a Qwen 3 0.6B model with **Group Relative Policy Optimization (GRPO)** to sort lists of integers using the [PEFT](https://huggingface.co/docs/peft/index) library while explicitly modelling `<think>` reasoning tokens and structured `<output>` answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "1. Refresh the intuition for LoRA and quantify how low-rank adapters reduce the memory footprint of a single linear layer.\n",
    "2. Implement the adapter for a synthetic multi-target linear regression problem and compare full fine-tuning vs. LoRA.\n",
    "3. Build a verifiable sorting reward, warm-start the policy with a small cold-start dataset of `<think>/<output>` exemplars, run a short supervised fine-tuning (SFT) stage, and drive a GRPO loop with PEFT to adapt Qwen 3 0.6B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "\ud83d\udca1 **Dependencies**\n",
    "\n",
    "If you are running in a clean environment you may need to install a few extra packages such as `transformers`, `datasets`, `peft`, `trl`, and `accelerate`. The cell below can be uncommented when necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %pip install -q torch transformers datasets accelerate peft trl evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Revisiting LoRA on a Single Linear Layer\n",
    "\n",
    "LoRA decomposes the weight update of a frozen matrix `W` into a product `BA` where `A \\in \\mathbb{R}^{r \\times d}` and `B \\in \\mathbb{R}^{m \\times r}`. Instead of storing gradients and optimizer states for the full `m \\times d` matrix, we only update the low-rank factors. The effective weight during adaptation is\n",
    "\n",
    "$$W_{\\text{eff}} = W + \\frac{\\alpha}{r} BA,$$\n",
    "\n",
    "where `\u03b1` rescales the update. For wide layers (large `m` and `d`) and small rank `r`, this reduces the number of trainable parameters and the accompanying optimizer state by orders of magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Synthetic regression setup\n",
    "\n",
    "We construct a multi-target linear regression task with a 512 \u2192 256 linear layer. The ground-truth weight matrix is the sum of a frozen base matrix and a low-rank update, mirroring the scenario where LoRA is expected to shine.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_features = 512\n",
    "out_features = 256\n",
    "lora_rank = 8\n",
    "num_samples = 4096\n",
    "batch_size = 256\n",
    "noise_std = 0.05\n",
    "\n",
    "# Construct a frozen base weight and a low-rank update that represents the target task.\n",
    "base_weight = torch.randn(out_features, in_features)\n",
    "adapter_A_true = torch.randn(lora_rank, in_features)\n",
    "adapter_B_true = torch.randn(out_features, lora_rank)\n",
    "delta_weight = adapter_B_true @ adapter_A_true\n",
    "target_weight = base_weight + delta_weight\n",
    "\n",
    "features = torch.randn(num_samples, in_features)\n",
    "targets = features @ target_weight.T + noise_std * torch.randn(num_samples, out_features)\n",
    "\n",
    "dataset = TensorDataset(features, targets)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementing a LoRA-augmented linear layer\n",
    "\n",
    "The class below mirrors the adapter structure used in larger language models. Only the low-rank matrices `A` and `B` are trainable; the base weight stays frozen.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base_weight: torch.Tensor, rank: int, alpha: float = 1.0, bias: bool = False):\n",
    "        super().__init__()\n",
    "        out_features, in_features = base_weight.shape\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        # Frozen base weight\n",
    "        self.weight = nn.Parameter(base_weight.clone())\n",
    "        self.weight.requires_grad = False\n",
    "        # Trainable low-rank factors\n",
    "        self.A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "        self.scaling = alpha / max(rank, 1)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def effective_weight(self) -> torch.Tensor:\n",
    "        return self.weight + (self.B @ self.A) * self.scaling\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(x, self.effective_weight(), self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_trainable_parameters(module: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def train_linear_module(module: nn.Module, loader: DataLoader, steps: int, lr: float) -> List[float]:\n",
    "    module.to(DEVICE)\n",
    "    module.train()\n",
    "    optimizer = torch.optim.Adam([p for p in module.parameters() if p.requires_grad], lr=lr)\n",
    "    history: List[float] = []\n",
    "    iterator = iter(loader)\n",
    "    for step in range(steps):\n",
    "        try:\n",
    "            batch = next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(loader)\n",
    "            batch = next(iterator)\n",
    "        x, y = (tensor.to(DEVICE) for tensor in batch)\n",
    "        optimizer.zero_grad()\n",
    "        preds = module(x)\n",
    "        loss = F.mse_loss(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "    return history\n",
    "\n",
    "def evaluate_mse(module: nn.Module, features: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    module.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = module(features.to(DEVICE))\n",
    "        loss = F.mse_loss(preds.cpu(), targets)\n",
    "    return float(loss)\n",
    "\n",
    "def relative_weight_error(module: nn.Module, target: torch.Tensor) -> float:\n",
    "    if isinstance(module, LoRALinear):\n",
    "        weight = module.effective_weight().detach()\n",
    "    else:\n",
    "        weight = module.weight.detach()\n",
    "    return float(torch.norm(weight - target) / torch.norm(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_linear = nn.Linear(in_features, out_features, bias=False)\n",
    "full_linear.weight.data.copy_(base_weight.clone())\n",
    "\n",
    "lora_linear = LoRALinear(base_weight=base_weight, rank=lora_rank, alpha=lora_rank)\n",
    "\n",
    "full_history = train_linear_module(full_linear, loader, steps=200, lr=1e-3)\n",
    "lora_history = train_linear_module(lora_linear, loader, steps=200, lr=5e-3)\n",
    "\n",
    "full_mse = evaluate_mse(full_linear, features, targets)\n",
    "lora_mse = evaluate_mse(lora_linear, features, targets)\n",
    "\n",
    "full_error = relative_weight_error(full_linear, target_weight)\n",
    "lora_error = relative_weight_error(lora_linear, target_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=full_history, name=\"Full fine-tuning\"))\n",
    "fig.add_trace(go.Scatter(y=lora_history, name=\"LoRA (rank=8)\"))\n",
    "fig.update_layout(title=\"Training loss comparison\", xaxis_title=\"Step\", yaxis_title=\"MSE loss\")\n",
    "fig.show()\n",
    "\n",
    "def format_mb(params: int, dtype=torch.float32) -> float:\n",
    "    bytes_per_param = torch.finfo(dtype).bits // 8\n",
    "    return params * bytes_per_param / (1024 ** 2)\n",
    "\n",
    "results_table = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Full fine-tuning\",\n",
    "        \"Trainable params\": count_trainable_parameters(full_linear),\n",
    "        \"Approx optimizer state (MB)\": format_mb(count_trainable_parameters(full_linear) * 2),\n",
    "        \"Final MSE\": full_mse,\n",
    "        \"Relative weight error\": full_error,\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"LoRA (rank=8)\",\n",
    "        \"Trainable params\": count_trainable_parameters(lora_linear),\n",
    "        \"Approx optimizer state (MB)\": format_mb(count_trainable_parameters(lora_linear) * 2),\n",
    "        \"Final MSE\": lora_mse,\n",
    "        \"Relative weight error\": lora_error,\n",
    "    },\n",
    "])\n",
    "results_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA matches the full fine-tuning loss while updating only a few thousand parameters. The optimizer state memory shrinks proportionally, which is critical when the base layer contains millions of parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RLVR with GRPO on a Sorting Task\n",
    "\n",
    "We now move from a single layer to a causal language model. The goal is to sort a list of integers \u2014 a domain where the reward can be **verified automatically**. We will:\n",
    "\n",
    "* Load a small cold-start dataset of prompts and structured `<think>/<output>` answers.\n",
    "* Run a lightweight supervised warm-start so the LoRA adapter learns to emit the reasoning and output tags.\n",
    "* Apply a LoRA adapter to Qwen 3 0.6B with the [PEFT](https://huggingface.co/docs/peft/index) library.\n",
    "* Implement a GRPO-style policy gradient loop that samples multiple completions per prompt and shapes the reward with verifiable checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cold-start data and prompt construction\n",
    "\n",
    "The helper below loads a JSONL file (also easy to host on the Hugging Face Hub) and augments it with synthetic permutations so that the policy has a warm start before RL. Every prompt enforces the structure \u201cthink inside `<think>...</think>` and answer inside `<output>...</output>` with a strict \u201cNo tools.\u201d reminder.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statistics\n",
    "import re\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "\n",
    "STRUCTURED_INSTRUCTIONS = (\n",
    "    \"First think between <think> and </think> tags and then provide a response as a sorted list and nothing else in <output> and </output> tags. No tools.\"\n",
    ")\n",
    "\n",
    "def render_numbers(numbers):\n",
    "    return ', '.join(str(n) for n in numbers)\n",
    "\n",
    "def build_prompt(numbers):\n",
    "    return f\"Sort the numbers [{render_numbers(numbers)}].\n",
    "{STRUCTURED_INSTRUCTIONS}\"\n",
    "\n",
    "def build_response(numbers):\n",
    "    sorted_numbers = sorted(numbers)\n",
    "    reasoning = (\n",
    "        f\"I sort the {len(numbers)} numbers [{render_numbers(numbers)}] into ascending order [{render_numbers(sorted_numbers)}].\"\n",
    "    )\n",
    "    return f\"<think>{reasoning}</think><output>[{render_numbers(sorted_numbers)}]</output>\"\n",
    "\n",
    "def generate_synthetic_sorting(num_examples: int = 128, seed: int = 0) -> Dataset:\n",
    "    rng = random.Random(seed)\n",
    "    samples = []\n",
    "    for _ in range(num_examples):\n",
    "        length = rng.randint(3, 7)\n",
    "        numbers = [rng.randint(-20, 30) for _ in range(length)]\n",
    "        prompt = build_prompt(numbers)\n",
    "        response = build_response(numbers)\n",
    "        samples.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"numbers\": numbers,\n",
    "                \"rationale\": response.split('<output>')[0].replace('<think>', '').replace('</think>', '').strip(),\n",
    "            }\n",
    "        )\n",
    "    return Dataset.from_list(samples)\n",
    "\n",
    "def load_sorting_cold_start(local_path: str = 'data/sorting_cold_start.jsonl') -> Dataset:\n",
    "    dataset = load_dataset('json', data_files=local_path, split='train')\n",
    "\n",
    "    def add_metadata(example):\n",
    "        numbers = [int(x) for x in re.findall(r'-?\\d+', example['prompt'])]\n",
    "        return {\n",
    "            \"numbers\": numbers,\n",
    "            \"rationale\": f\"Sorting yields [{render_numbers(sorted(numbers))}].\",\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(add_metadata)\n",
    "    return dataset\n",
    "\n",
    "cold_start_ds = load_sorting_cold_start()\n",
    "synthetic_ds = generate_synthetic_sorting(192, seed=42)\n",
    "training_ds = concatenate_datasets([cold_start_ds, synthetic_ds]).shuffle(seed=0)\n",
    "len(training_ds), training_ds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Supervised warm-start with structured reasoning tokens\n",
    "\n",
    "Before optimising with RL we align the policy to the desired format by running a brief supervised fine-tuning pass on the combined cold-start and synthetic data. We mask the prompt tokens so that only the completion (the `<think>` rationale plus the `<output>` answer) contributes to the loss, ensuring the LoRA adapter reliably emits the control tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "SFT_MAX_LENGTH = 256\n",
    "\n",
    "def prepare_sft_example(example):\n",
    "    prompt = example[\"prompt\"].strip()\n",
    "    response = example[\"response\"].strip()\n",
    "    text = f\"{prompt}\n",
    "{response}\"\n",
    "    tokenized = tokenizer(text, truncation=True, max_length=SFT_MAX_LENGTH)\n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False, truncation=True, max_length=SFT_MAX_LENGTH)[\"input_ids\"]\n",
    "    labels = tokenized[\"input_ids\"][:]\n",
    "    labels = labels.copy()\n",
    "    prompt_len = min(len(prompt_ids), len(labels))\n",
    "    for idx in range(prompt_len):\n",
    "        labels[idx] = -100\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "sft_dataset = training_ds.map(\n",
    "    prepare_sft_example,\n",
    "    remove_columns=training_ds.column_names,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"logs/qwen3_sorting_sft\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=60,\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "sft_trainer = Trainer(\n",
    "    model=policy_model,\n",
    "    args=sft_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "sft_trainer.train()\n",
    "policy_model.to(DEVICE)\n",
    "policy_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reward shaping with verifiable checks\n",
    "\n",
    "Sorting is verifiable because we can deterministically extract integers from the prompt and the model output. The reward below blends several signals:\n",
    "\n",
    "* **Exact match** \u2014 full credit when the completion equals the sorted list.\n",
    "* **Monotonicity** \u2014 partial credit if the answer is sorted but numbers differ.\n",
    "* **Prefix accuracy** \u2014 rewards early correct numbers to stabilise learning.\n",
    "* **Coverage** \u2014 encourages the model to reuse the original numbers.\n",
    "* **Format compliance** \u2014 bonus for emitting both `<think>` and `<output>` blocks.\n",
    "* **Length penalty** \u2014 discourages hallucinating or dropping numbers.\n",
    "\n",
    "The function also returns diagnostic components so we can reason about learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "THINK_PATTERN = re.compile(r\"<think>(.*?)</think>\", re.IGNORECASE | re.DOTALL)\n",
    "OUTPUT_PATTERN = re.compile(r\"<output>(.*?)</output>\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def extract_numbers(text: str) -> List[int]:\n",
    "    return [int(x) for x in re.findall(r'-?\\d+', text)]\n",
    "\n",
    "def get_section(text: str, pattern: re.Pattern) -> str:\n",
    "    match = pattern.search(text)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def sorting_reward(prompt: str, completion: str) -> Tuple[float, Dict[str, float]]:\n",
    "    target_numbers = extract_numbers(prompt)\n",
    "    target_sorted = sorted(target_numbers)\n",
    "    output_section = get_section(completion, OUTPUT_PATTERN)\n",
    "    if not output_section:\n",
    "        return -1.0, {\"exact\": 0.0, \"monotonic\": 0.0, \"prefix\": 0.0, \"coverage\": 0.0, \"format\": 0.0}\n",
    "\n",
    "    predicted_numbers = extract_numbers(output_section)\n",
    "    if not predicted_numbers:\n",
    "        return -1.0, {\"exact\": 0.0, \"monotonic\": 0.0, \"prefix\": 0.0, \"coverage\": 0.0, \"format\": 0.0}\n",
    "\n",
    "    think_section = get_section(completion, THINK_PATTERN)\n",
    "    format_score = 1.0 if think_section else 0.0\n",
    "\n",
    "    length_penalty = -0.05 * abs(len(predicted_numbers) - len(target_sorted))\n",
    "\n",
    "    target_counter = Counter(target_sorted)\n",
    "    predicted_counter = Counter(predicted_numbers)\n",
    "    coverage = sum((target_counter & predicted_counter).values()) / max(len(target_sorted), 1)\n",
    "\n",
    "    monotonic = 1.0 if predicted_numbers == sorted(predicted_numbers) else 0.0\n",
    "\n",
    "    prefix = 0.0\n",
    "    for t, p in zip(target_sorted, predicted_numbers):\n",
    "        if t == p:\n",
    "            prefix += 1\n",
    "        else:\n",
    "            break\n",
    "    prefix = prefix / max(len(target_sorted), 1)\n",
    "\n",
    "    exact = 1.0 if predicted_numbers == target_sorted else 0.0\n",
    "\n",
    "    reward = (\n",
    "        0.55 * exact\n",
    "        + 0.2 * monotonic\n",
    "        + 0.1 * prefix\n",
    "        + 0.1 * coverage\n",
    "        + 0.05 * format_score\n",
    "        + length_penalty\n",
    "    )\n",
    "    reward = float(max(-1.0, min(reward, 1.0)))\n",
    "    return reward, {\n",
    "        \"exact\": exact,\n",
    "        \"monotonic\": monotonic,\n",
    "        \"prefix\": prefix,\n",
    "        \"coverage\": coverage,\n",
    "        \"format\": format_score,\n",
    "    }\n",
    "\n",
    "example_prompt = training_ds[0]['prompt']\n",
    "example_response = training_ds[0]['response']\n",
    "sorting_reward(example_prompt, example_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Qwen 3 0.6B with PEFT LoRA\n",
    "\n",
    "We attach a LoRA adapter to Qwen 3 0.6B so that only a few attention projections are updated while the base weights stay frozen. The tokenizer is augmented with `<think>`/`<output>` specials so the adapter can model the reasoning format, and the frozen reference model provides the KL anchor in the GRPO loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "base_model_name = 'Qwen/Qwen3-0.6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "special_tokens = {'additional_special_tokens': ['<think>', '</think>', '<output>', '</output>']}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "policy_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "policy_model = get_peft_model(policy_model, lora_config)\n",
    "policy_model.print_trainable_parameters()\n",
    "\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "reference_model.resize_token_embeddings(len(tokenizer))\n",
    "reference_model.to(DEVICE)\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tokenisation utilities and log-probabilities\n",
    "\n",
    "GRPO needs log-probabilities for each sampled completion under both the policy and the frozen reference model. The helpers below combine prompts with completions, build attention masks that isolate the generated tokens, and return per-sequence log-probs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_batch(prompts: Sequence[str], completions: Sequence[str], max_length: int = 192) -> Dict[str, torch.Tensor]:\n",
    "    input_ids: List[List[int]] = []\n",
    "    attention_masks: List[List[int]] = []\n",
    "    completion_masks: List[List[int]] = []\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        completion_ids = tokenizer.encode(completion, add_special_tokens=False) + [tokenizer.eos_token_id]\n",
    "        combined = prompt_ids + completion_ids\n",
    "        combined = combined[:max_length]\n",
    "        completion_mask = [0] * len(prompt_ids) + [1] * len(completion_ids)\n",
    "        completion_mask = completion_mask[:max_length]\n",
    "        attention = [1] * len(combined)\n",
    "        pad = max_length - len(combined)\n",
    "        if pad > 0:\n",
    "            combined += [tokenizer.pad_token_id] * pad\n",
    "            attention += [0] * pad\n",
    "            completion_mask += [0] * pad\n",
    "        input_ids.append(combined)\n",
    "        attention_masks.append(attention)\n",
    "        completion_masks.append(completion_mask)\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long, device=DEVICE),\n",
    "        'attention_mask': torch.tensor(attention_masks, dtype=torch.long, device=DEVICE),\n",
    "        'completion_mask': torch.tensor(completion_masks, dtype=torch.float, device=DEVICE),\n",
    "    }\n",
    "\n",
    "def sequence_logprobs(model: AutoModelForCausalLM, prompts: Sequence[str], completions: Sequence[str], max_length: int = 192, detach: bool = True):\n",
    "    batch = build_batch(prompts, completions, max_length=max_length)\n",
    "    context = torch.no_grad() if detach else nullcontext()\n",
    "    with context:\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = batch['input_ids'][:, 1:]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        token_logprobs = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    mask = batch['completion_mask'][:, 1:]\n",
    "    seq_logprobs = (token_logprobs * mask).sum(dim=-1)\n",
    "    token_counts = mask.sum(dim=-1).clamp(min=1.0)\n",
    "    if detach:\n",
    "        return seq_logprobs.detach(), token_counts.detach()\n",
    "    return seq_logprobs, token_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Sampling groups of completions\n",
    "\n",
    "GRPO gathers multiple completions per prompt and uses their relative rewards as baselines. The function below returns a list of completions for each prompt and keeps track of all random seeds for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sample_completions(model: AutoModelForCausalLM, prompts: Sequence[str], *, num_generations: int, max_new_tokens: int, temperature: float = 0.7, top_p: float = 0.9) -> List[List[str]]:\n",
    "    completions: List[List[str]] = []\n",
    "    for prompt in prompts:\n",
    "        encoded = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
    "        outputs = model.generate(\n",
    "            **encoded,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=num_generations,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        prompt_len = encoded['input_ids'].size(1)\n",
    "        generated = outputs[:, prompt_len:]\n",
    "        texts = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        completions.append([text.strip() for text in texts])\n",
    "    return completions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 GRPO update step\n",
    "\n",
    "The update uses per-group baselines \u2014 the mean reward of all completions for a prompt \u2014 to reduce variance. We normalise advantages inside each group, apply a KL penalty with respect to the frozen reference model, and clip gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    batch_size: int = 4\n",
    "    num_generations: int = 4\n",
    "    max_new_tokens: int = 48\n",
    "    max_length: int = 192\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    learning_rate: float = 5e-5\n",
    "    kl_coef: float = 0.05\n",
    "    gradient_clip: float = 1.0\n",
    "    num_steps: int = 30\n",
    "\n",
    "config = GRPOConfig()\n",
    "optimizer = torch.optim.AdamW([p for p in policy_model.parameters() if p.requires_grad], lr=config.learning_rate)\n",
    "\n",
    "def grpo_update(prompts: Sequence[str]) -> Dict[str, float]:\n",
    "    policy_model.train()\n",
    "    completions = sample_completions(\n",
    "        policy_model,\n",
    "        prompts,\n",
    "        num_generations=config.num_generations,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "    )\n",
    "    flat_prompts: List[str] = []\n",
    "    flat_completions: List[str] = []\n",
    "    advantages: List[float] = []\n",
    "    raw_rewards: List[float] = []\n",
    "    for prompt, candidate_list in zip(prompts, completions):\n",
    "        group_rewards: List[float] = []\n",
    "        for completion in candidate_list:\n",
    "            reward, _ = sorting_reward(prompt, completion)\n",
    "            group_rewards.append(reward)\n",
    "            flat_prompts.append(prompt)\n",
    "            flat_completions.append(completion)\n",
    "        mean_reward = sum(group_rewards) / len(group_rewards)\n",
    "        std_reward = statistics.pstdev(group_rewards)\n",
    "        denom = std_reward if std_reward > 1e-6 else 1.0\n",
    "        for reward in group_rewards:\n",
    "            advantages.append((reward - mean_reward) / denom)\n",
    "            raw_rewards.append(reward)\n",
    "    if not flat_prompts:\n",
    "        return {'loss': 0.0, 'avg_reward': 0.0}\n",
    "\n",
    "    policy_logprobs, token_counts = sequence_logprobs(\n",
    "        policy_model,\n",
    "        flat_prompts,\n",
    "        flat_completions,\n",
    "        max_length=config.max_length,\n",
    "        detach=False,\n",
    "    )\n",
    "    ref_logprobs, _ = sequence_logprobs(\n",
    "        reference_model,\n",
    "        flat_prompts,\n",
    "        flat_completions,\n",
    "        max_length=config.max_length,\n",
    "        detach=True,\n",
    "    )\n",
    "    advantages_tensor = torch.tensor(advantages, dtype=torch.float, device=policy_logprobs.device)\n",
    "    lengths = token_counts.to(policy_logprobs.device)\n",
    "    norm_policy = policy_logprobs / lengths\n",
    "    norm_ref = ref_logprobs.to(policy_logprobs.device) / lengths\n",
    "    loss_policy = -(advantages_tensor * norm_policy).mean()\n",
    "    kl_term = (norm_policy - norm_ref).mean()\n",
    "    loss = loss_policy + config.kl_coef * kl_term\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config.gradient_clip)\n",
    "    optimizer.step()\n",
    "    return {\n",
    "        'loss': float(loss.detach().cpu()),\n",
    "        'policy_term': float(loss_policy.detach().cpu()),\n",
    "        'kl_term': float(kl_term.detach().cpu()),\n",
    "        'avg_reward': float(sum(raw_rewards) / max(len(raw_rewards), 1)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Training loop with evaluation hooks\n",
    "\n",
    "We iterate over random prompts, run a GRPO update, and periodically measure success on held-out prompts using greedy decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_completion(model: AutoModelForCausalLM, prompt: str, max_new_tokens: int = 48) -> str:\n",
    "    encoded = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
    "    output = model.generate(\n",
    "        **encoded,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    prompt_len = encoded['input_ids'].size(1)\n",
    "    completion = tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)\n",
    "    return completion.strip()\n",
    "\n",
    "def evaluate_success(model: AutoModelForCausalLM, dataset: Dataset, num_examples: int = 32) -> Dict[str, float]:\n",
    "    subset = dataset.shuffle(seed=1234).select(range(min(num_examples, len(dataset))))\n",
    "    successes = []\n",
    "    rewards = []\n",
    "    for example in subset:\n",
    "        completion = generate_completion(model, example['prompt'])\n",
    "        reward, components = sorting_reward(example['prompt'], completion)\n",
    "        rewards.append(reward)\n",
    "        successes.append(1.0 if components['exact'] == 1.0 else 0.0)\n",
    "    return {\n",
    "        'avg_reward': float(sum(rewards) / max(len(rewards), 1)),\n",
    "        'exact_match_rate': float(sum(successes) / max(len(successes), 1)),\n",
    "    }\n",
    "\n",
    "rng = random.Random(0)\n",
    "metrics_history: List[Dict[str, float]] = []\n",
    "dataset_list = list(training_ds)\n",
    "for step in range(config.num_steps):\n",
    "    batch = rng.sample(dataset_list, k=min(config.batch_size, len(dataset_list)))\n",
    "    prompts = [item['prompt'] for item in batch]\n",
    "    metrics = grpo_update(prompts)\n",
    "    if step % 5 == 0:\n",
    "        eval_metrics = evaluate_success(policy_model, training_ds, num_examples=16)\n",
    "        metrics.update({f'eval_{k}': v for k, v in eval_metrics.items()})\n",
    "        print(f\"Step {step}: {metrics}\")\n",
    "    metrics['step'] = step\n",
    "    metrics_history.append(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Inspecting the tuned model\n",
    "\n",
    "After training we can sample a few prompts and compare the base (reference) model with the LoRA-adapted policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_prompts = [example['prompt'] for example in training_ds.select(range(3))]\n",
    "for prompt in sample_prompts:\n",
    "    base_completion = generate_completion(reference_model, prompt)\n",
    "    tuned_completion = generate_completion(policy_model, prompt)\n",
    "    reward_base, _ = sorting_reward(prompt, base_completion)\n",
    "    reward_tuned, _ = sorting_reward(prompt, tuned_completion)\n",
    "    print('Prompt:', prompt)\n",
    "    print('Reference model:', base_completion, f'(reward={reward_base:.2f})')\n",
    "    print('LoRA + GRPO:', tuned_completion, f'(reward={reward_tuned:.2f})')\n",
    "    print('-' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "* LoRA adapters collapse the fine-tuning footprint of a dense linear layer while maintaining accuracy when the required update is approximately low rank.\n",
    "* Supervised warm-starting with structured `<think>/<output>` exemplars teaches the LoRA adapter to emit both reasoning tokens and the final sorted answer before RL.\n",
    "* GRPO-style updates combined with PEFT adapters on Qwen 3 0.6B provide a practical recipe for reinforcement learning on consumer hardware.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}