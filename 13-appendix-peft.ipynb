{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix: Parameter-Efficient Fine-Tuning beyond LoRA\n\nThis appendix surveys several extensions to Low-Rank Adaptation (LoRA) that have recently appeared in the parameter-efficient fine-tuning (PEFT) literature. We focus on methods that are implemented in [\ud83e\udd17 PEFT](https://huggingface.co/docs/peft/en/package_reference/adalora) so that they are easy to experiment with. The goal is to understand *why* each variant was introduced and to provide a lightweight, reproducible comparison of their trainable parameter counts on a small language model."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Recap: What LoRA optimises\n\nLoRA proposes to freeze the pretrained weights $W_0$ of a linear layer and to learn a low-rank update $\\Delta W = BA$ with $A \\in \\mathbb{R}^{r \\times d_{\\text{in}}}$ and $B \\in \\mathbb{R}^{d_{\\text{out}} \\times r}$. The forward pass becomes\n\n$$ h = W_0 x + \\frac{\\alpha}{r} BAx, $$\n\nwhere $r$ is the rank hyper-parameter and $\\alpha$ rescales the update. Optimising only $A$ and $B$ drastically reduces the number of trainable parameters compared to the dense matrix $W_0$."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## LoRA extensions covered here\n\nBelow is a concise overview of the LoRA-inspired variants explored in this appendix. Each approach introduces an additional inductive bias on top of the low-rank structure.\n\n| Method | Main idea | Extra knobs | Intuition |\n| --- | --- | --- | --- |\n| **AdaLoRA** (Adaptive LoRA) | Dynamically reallocates rank budget during training | $(r_t)$ schedule, importance metrics | Concentrate capacity on the most useful layers. |\n| **LoHa** (Low-Rank Hypercomplex Adapter) | Factorises updates into hypercomplex components | Hypercomplex multiplier $h$ | Couples channel interactions more expressively than purely real matrices. |\n| **DoRA** (Weight-Decomposed LoRA) | Separates weight *direction* and *magnitude* | Scaling vector rank | Learns a rank-one rescaler in addition to low-rank direction updates. |\n| **HRA** (Householder Reflection Adaptation) | Uses orthogonal Householder reflections | # reflections $m$ | Enforces near-orthogonal updates that preserve norms. |\n| **LoRA+ / other tweaks** | Better initialisation, merged optimisers, dropout tricks | None | Often complementary to the above and compatible with PEFT APIs. |"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### AdaLoRA\n\nAdaLoRA augments LoRA with a learnable schedule for the ranks $r_t$ of each adapter. It starts with a small $r$ and gradually increases or prunes ranks based on the importance of singular values, measured by the magnitude of gradient statistics. The optimisation objective for a weight matrix $W$ becomes\n\n$$ \\min_{\\{A_t, B_t\\}} \\ \\mathcal{L}(W_0 + B_t A_t), \\quad \\text{s.t. } \\operatorname{rank}(A_t) = r_t, $$\n\nwhere $r_t$ evolves during training. The PEFT implementation automates this via a per-layer scheduler that redistributes a global rank budget subject to user-defined milestones $t_{\\text{init}}, t_{\\text{final}}$."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### LoHa\n\nLoHa (Low-Rank Hypercomplex Adaptation) generalises the low-rank factors to *hypercomplex* multiplications. Instead of $BA$, LoHa forms updates using $h$ shared components $\\{A^{(k)}, B^{(k)}\\}_{k=1}^h$ and fuses them with learnable phase matrices $\\Phi^{(k)}$. This can be interpreted as a structured Kronecker product: \n\n$$ \\Delta W = \\sum_{k=1}^{h} (B^{(k)} \\otimes R^{(k)}) (A^{(k)} \\otimes L^{(k)}), $$\n\nwhere $R^{(k)}$ and $L^{(k)}$ encode the hypercomplex rotations. In practice LoHa drops into the same API but exposes a multiplier $h$ that controls how many hypercomplex components are used."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### DoRA\n\nWeight-Decomposed Low-Rank Adaptation splits each pretrained weight into a direction $\\hat{W}$ and a scalar magnitude $s = \\lVert W \\rVert$. LoRA then adapts only the direction with low-rank matrices $(A, B)$ while a separate low-rank adapter learns multiplicative scaling factors $d$: \n\n$$ W = s \\hat{W} \\quad \\Rightarrow \\quad W' = (s + d) (\\hat{W} + BA). $$\n\nThis decoupling improves stability when the base model contains layers with vastly different scales. PEFT exposes DoRA via `DoRAConfig`, which internally keeps track of both the directional adapters and the rank-one scaling vectors."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Householder Reflection Adaptation (HRA)\n\nHRA constrains the update matrix to be a product of Householder reflections, each of which is an orthogonal matrix $H_i = I - 2 \\frac{v_i v_i^\\top}{\\lVert v_i \\rVert^2}$. Composing $m$ reflections yields an orthogonal transform $Q = \\prod_{i=1}^{m} H_i$ that preserves norms. HRA parameterises $Q$ via the $v_i$ vectors and learns a small diagonal scaling $D$, leading to \n\n$$ W' = Q D W_0. $$\n\nBy construction, HRA focuses on rotation-like adaptations, which can be beneficial when invariance to norm changes is desirable."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Other practical tweaks\n\n- **LoRA+** initialises the $A$ and $B$ factors with different learning rates to stabilise early updates.\n- **Dynamic scaling / RsLoRA** rescales $B$ on the fly to keep the spectral norm bounded.\n- **AdapterDrop** randomly drops LoRA adapters during training as a regulariser.\n\nThese refinements are orthogonal to the structural extensions above and are usually exposed as configuration flags (for example `use_rslora=True`)."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Lightweight empirical comparison\n\nThe next cells instantiate adapters for a tiny GPT-2 model to compare how many parameters each method trains. The point is not to run a full fine-tuning loop but to sanity-check that the adapters are easy to attach and that the parameter budgets differ as expected."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install dependencies (uncomment if running on a fresh environment)\n# !pip install --quiet torch transformers accelerate peft pandas"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport inspect\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport pandas as pd\nimport torch\nfrom transformers import AutoModelForCausalLM\n\nfrom peft import (\n    AdaLoraConfig,\n    DoRAConfig,\n    HRAConfig,\n    LoHaConfig,\n    LoraConfig,\n    TaskType,\n    get_peft_model,\n)\n\nBASE_MODEL = \"hf-internal-testing/tiny-random-GPT2\"\nTARGET_MODULES = [\"c_attn\"]  # attention projection in GPT-2 blocks"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "@dataclass\nclass AdapterResult:\n    name: str\n    trainable_params: int\n    total_params: int\n    percent_trainable: float\n\n\ndef prepare_config(config_cls, *, extra: Dict[str, Any] | None = None, **common_kwargs: Any):\n    extra = extra or {}\n    sig = inspect.signature(config_cls.__init__)\n    kwargs = {}\n    for name in sig.parameters:\n        if name == \"self\":\n            continue\n        if name in extra:\n            kwargs[name] = extra[name]\n        elif name in common_kwargs:\n            kwargs[name] = common_kwargs[name]\n    return config_cls(**kwargs)\n\n\ndef count_trainable_parameters(model: torch.nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef count_total_parameters(model: torch.nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters())\n\n\ndef evaluate_adapter(config_cls, name: str, *, extra: Dict[str, Any] | None = None) -> AdapterResult:\n    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n    model.requires_grad_(False)\n\n    config = prepare_config(\n        config_cls,\n        extra=extra,\n        r=8,\n        lora_alpha=16,\n        lora_dropout=0.05,\n        target_modules=TARGET_MODULES,\n        task_type=TaskType.CAUSAL_LM,\n    )\n    peft_model = get_peft_model(model, config)\n    trainable = count_trainable_parameters(peft_model)\n    total = count_total_parameters(peft_model)\n    return AdapterResult(name, trainable, total, 100.0 * trainable / total)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "results = [\n    evaluate_adapter(LoraConfig, \"LoRA\"),\n    evaluate_adapter(\n        AdaLoraConfig,\n        \"AdaLoRA\",\n        extra=dict(init_r=6, target_r=12, beta1=0.85, beta2=0.85, tinit=10, tfinal=50, delta_t=10),\n    ),\n    evaluate_adapter(LoHaConfig, \"LoHa\", extra=dict(hypercomplex_multiplier=2)),\n    evaluate_adapter(DoRAConfig, \"DoRA\"),\n    evaluate_adapter(HRAConfig, \"HRA\", extra=dict(num_householder_blocks=2)),\n]\n\ndf = pd.DataFrame(\n    {\n        \"Adapter\": [r.name for r in results],\n        \"Trainable params\": [r.trainable_params for r in results],\n        \"Total params\": [r.total_params for r in results],\n        \"% trainable\": [f\"{r.percent_trainable:.4f}%\" for r in results],\n    }\n)\n\ndf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The table above shows that all variants update only a small fraction of the model compared to full fine-tuning. AdaLoRA typically allocates a few more parameters because it keeps extra buffers for rank reallocation, while LoHa and HRA introduce structured factors that slightly increase the footprint. DoRA tracks both directional and scaling adapters, leading to a modest increase as well."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Comparing adapter parameter counts\n\nTo make the comparison more explicit, the following bar plot visualises the trainable parameter budgets. The differences remain tiny relative to the full model, but they highlight which methods introduce additional learnable tensors beyond the vanilla LoRA factors."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import plotly.express as px\n\nfig = px.bar(df, x=\"Adapter\", y=\"Trainable params\", text=\"% trainable\", title=\"Trainable parameters per adapter type\")\nfig.update_traces(textposition=\"outside\")\nfig.update_layout(yaxis_title=\"Parameters\", xaxis_title=\"Adapter\")\nfig"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Takeaways\n\n- All of the covered methods are exposed through the same `get_peft_model` workflow, so swapping adapters is mostly a matter of changing the config class.\n- Adaptive or structured variants incur a modest increase in trainable parameters but still stay within the PEFT regime.\n- Choosing between them depends on the task: AdaLoRA shines when different layers need different capacities, LoHa and HRA introduce richer geometric biases, and DoRA stabilises training when weight scales vary a lot.\n\nThese adapters are complementary to the optimisation strategies introduced earlier in the book, and they can be combined with quantisation or pruning techniques from the other appendices."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}