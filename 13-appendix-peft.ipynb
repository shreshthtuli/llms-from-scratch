{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13 (Appendix). Parameter-Efficient Fine-Tuning beyond LoRA\n",
        "\n",
        "This appendix surveys several extensions to Low-Rank Adaptation (LoRA) that have recently appeared in the parameter-efficient fine-tuning (PEFT) literature. We focus on methods that are implemented in [ðŸ¤— PEFT](https://huggingface.co/docs/peft/en/package_reference/adalora) so that they are easy to experiment with. The goal is to understand *why* each variant was introduced and to provide a lightweight, reproducible comparison of their trainable parameter counts on a small language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recap: What LoRA optimises\n",
        "\n",
        "LoRA proposes to freeze the pretrained weights $W_0$ of a linear layer and to learn a low-rank update $\\Delta W = BA$ with $A \\in \\mathbb{R}^{r \\times d_{\\text{in}}}$ and $B \\in \\mathbb{R}^{d_{\\text{out}} \\times r}$. The forward pass becomes\n",
        "\n",
        "$$ h = W_0 x + \\frac{\\alpha}{r} BAx, $$\n",
        "\n",
        "where $r$ is the rank hyper-parameter and $\\alpha$ rescales the update. Optimising only $A$ and $B$ drastically reduces the number of trainable parameters compared to the dense matrix $W_0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA extensions covered here\n",
        "\n",
        "Below is a concise overview of the LoRA-inspired variants explored in this appendix. Each approach introduces an additional inductive bias on top of the low-rank structure.\n",
        "\n",
        "| Method | Main idea | Extra knobs | Intuition |\n",
        "| --- | --- | --- | --- |\n",
        "| **AdaLoRA** (Adaptive LoRA) | Dynamically reallocates rank budget during training | $(r_t)$ schedule, importance metrics | Concentrate capacity on the most useful layers. |\n",
        "| **LoHa** (Low-Rank Hypercomplex Adapter) | Factorises updates into hypercomplex components | Hypercomplex multiplier $h$ | Couples channel interactions more expressively than purely real matrices. |\n",
        "| **DoRA** (Weight-Decomposed LoRA) | Separates weight *direction* and *magnitude* | Scaling vector rank | Learns a rank-one rescaler in addition to low-rank direction updates. |\n",
        "| **HRA** (Householder Reflection Adaptation) | Uses orthogonal Householder reflections | # reflections $m$ | Enforces near-orthogonal updates that preserve norms. |\n",
        "| **LoRA+ / other tweaks** | Better initialisation, merged optimisers, dropout tricks | None | Often complementary to the above and compatible with PEFT APIs. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. AdaLoRA\n",
        "\n",
        "[AdaLoRA](https://arxiv.org/pdf/2303.10512) augments LoRA with a learnable schedule for the ranks $r_t$ of each adapter. It starts with a small $r$ and gradually increases or prunes ranks based on the importance of singular values, measured by the magnitude of gradient statistics. The optimisation objective for a weight matrix $W$ becomes\n",
        "\n",
        "$$ \\min_{\\{A_t, B_t\\}} \\ \\mathcal{L}(W_0 + B_t A_t), \\quad \\text{s.t. } \\operatorname{rank}(A_t) = r_t, $$\n",
        "\n",
        "where $r_t$ evolves during training. The PEFT implementation automates this via a per-layer scheduler that redistributes a global rank budget subject to user-defined milestones $t_{\\text{init}}, t_{\\text{final}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. LoHa\n",
        "\n",
        "[LoHa (Low-Rank Hadamard Adaptation)](https://arxiv.org/pdf/2108.06098) generalises the low-rank factors to *hadamard* multiplications. Instead of $BA$, LoHa forms updates using $h$ shared components $\\{A^{(k)}, B^{(k)}\\}_{k=1}^h$ and fuses them with learnable phase matrices $\\Phi^{(k)}$. This can be interpreted as a structured Kronecker product: \n",
        "\n",
        "$$ \\Delta W = \\sum_{k=1}^{h} (B^{(k)} \\otimes R^{(k)}) (A^{(k)} \\otimes L^{(k)}), $$\n",
        "\n",
        "where $R^{(k)}$ and $L^{(k)}$ encode the rotations. In practice LoHa drops into the same API but exposes a multiplier $h$ that controls how many components are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. DoRA\n",
        "\n",
        "[DoRA (Weight-Decomposed Low-Rank Adaptation)](https://arxiv.org/pdf/2402.09353) splits each pretrained weight into a direction $\\hat{W}$ and a scalar magnitude $s = \\lVert W \\rVert$. DoRA then adapts only the direction with low-rank matrices $(A, B)$ while a separate low-rank adapter learns multiplicative scaling factors $d$: \n",
        "\n",
        "$$ W = s \\hat{W} \\quad \\Rightarrow \\quad W' = (s + d) (\\hat{W} + BA). $$\n",
        "\n",
        "This decoupling improves stability when the base model contains layers with vastly different scales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Householder Reflection Adaptation (HRA)\n",
        "\n",
        "[HRA](https://arxiv.org/pdf/2405.17484) constrains the update matrix to be a product of Householder reflections, each of which is an orthogonal matrix $H_i = I - 2 \\frac{v_i v_i^\\top}{\\lVert v_i \\rVert^2}$. Composing $m$ reflections yields an orthogonal transform $Q = \\prod_{i=1}^{m} H_i$ that preserves norms. HRA parameterises $Q$ via the $v_i$ vectors and learns a small diagonal scaling $D$, leading to \n",
        "\n",
        "$$ W' = Q D W_0. $$\n",
        "\n",
        "By construction, HRA focuses on rotation-like adaptations, which can be beneficial when invariance to norm changes is desirable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other practical tweaks\n",
        "\n",
        "- **LoRA+** initialises the $A$ and $B$ factors with different learning rates to stabilise early updates.\n",
        "- **Dynamic scaling / RsLoRA** rescales $B$ on the fly to keep the spectral norm bounded.\n",
        "- **AdapterDrop** randomly drops LoRA adapters during training as a regulariser.\n",
        "\n",
        "These refinements are orthogonal to the structural extensions above and are usually exposed as configuration flags (for example `use_rslora=True`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightweight empirical comparison\n",
        "\n",
        "The next cells instantiate adapters for a tiny [OPT language model](https://huggingface.co/docs/transformers/en/model_doc/opt) to compare how many parameters each method trains. The point is not to run a full fine-tuning loop but to sanity-check that the adapters are easy to attach and that the parameter budgets differ as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from peft import (\n",
        "    AdaLoraConfig,\n",
        "    HRAConfig,\n",
        "    LoHaConfig,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        ")\n",
        "\n",
        "BASE_MODEL = \"EleutherAI/pythia-70m-deduped\"\n",
        "TARGET_MODULES = [\"query_key_value\", \"dense\"] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class AdapterResult:\n",
        "    name: str\n",
        "    trainable_params: int\n",
        "    total_params: int\n",
        "    percent_trainable: float\n",
        "\n",
        "\n",
        "def prepare_config(config_cls, *, extra: Dict[str, Any] | None = None, **common_kwargs: Any):\n",
        "    extra = extra or {}\n",
        "    sig = inspect.signature(config_cls.__init__)\n",
        "    kwargs = {}\n",
        "    for name in sig.parameters:\n",
        "        if name == \"self\":\n",
        "            continue\n",
        "        if name in extra:\n",
        "            kwargs[name] = extra[name]\n",
        "        elif name in common_kwargs:\n",
        "            kwargs[name] = common_kwargs[name]\n",
        "    return config_cls(**kwargs)\n",
        "\n",
        "\n",
        "def count_trainable_parameters(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def count_total_parameters(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def evaluate_adapter(config_cls, name: str, *, extra: Dict[str, Any] | None = None) -> AdapterResult:\n",
        "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "    model.requires_grad_(False)\n",
        "\n",
        "    config = prepare_config(\n",
        "        config_cls,\n",
        "        extra=extra,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=TARGET_MODULES,\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    peft_model = get_peft_model(model, config)\n",
        "    trainable = count_trainable_parameters(peft_model)\n",
        "    total = count_total_parameters(peft_model)\n",
        "    return AdapterResult(name, trainable, total, 100.0 * trainable / total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adapter</th>\n",
              "      <th>Trainable params</th>\n",
              "      <th>Total params</th>\n",
              "      <th>% trainable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LoRA</td>\n",
              "      <td>147456</td>\n",
              "      <td>70574080</td>\n",
              "      <td>0.2089%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AdaLoRA</td>\n",
              "      <td>110664</td>\n",
              "      <td>70537300</td>\n",
              "      <td>0.1569%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LoHa</td>\n",
              "      <td>294912</td>\n",
              "      <td>70721536</td>\n",
              "      <td>0.4170%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DoRA</td>\n",
              "      <td>159744</td>\n",
              "      <td>70586368</td>\n",
              "      <td>0.2263%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HRA</td>\n",
              "      <td>49152</td>\n",
              "      <td>70475776</td>\n",
              "      <td>0.0697%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Adapter  Trainable params  Total params % trainable\n",
              "0     LoRA            147456      70574080     0.2089%\n",
              "1  AdaLoRA            110664      70537300     0.1569%\n",
              "2     LoHa            294912      70721536     0.4170%\n",
              "3     DoRA            159744      70586368     0.2263%\n",
              "4      HRA             49152      70475776     0.0697%"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = [\n",
        "    evaluate_adapter(LoraConfig, \"LoRA\"),\n",
        "    evaluate_adapter(\n",
        "        AdaLoraConfig,\n",
        "        \"AdaLoRA\",\n",
        "        extra=dict(init_r=6, target_r=12, beta1=0.85, beta2=0.85, tinit=10, tfinal=50, delta_t=10, total_step=1000),\n",
        "    ),\n",
        "    evaluate_adapter(LoHaConfig, \"LoHa\"),\n",
        "    evaluate_adapter(LoraConfig, \"DoRA\", extra=dict(use_dora=True)),\n",
        "    evaluate_adapter(HRAConfig, \"HRA\", extra=dict(num_householder_blocks=2)),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"Adapter\": [r.name for r in results],\n",
        "        \"Trainable params\": [r.trainable_params for r in results],\n",
        "        \"Total params\": [r.total_params for r in results],\n",
        "        \"% trainable\": [f\"{r.percent_trainable:.4f}%\" for r in results],\n",
        "    }\n",
        ")\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table above shows that all variants update only a small fraction of the model compared to full fine-tuning. AdaLoRA typically allocates a few more parameters because it keeps extra buffers for rank reallocation, while LoHa and HRA introduce structured factors that slightly increase the footprint. DoRA tracks both directional and scaling adapters, leading to a modest increase as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing adapter parameter counts\n",
        "\n",
        "To make the comparison more explicit, the following bar plot visualises the trainable parameter budgets. The differences remain tiny relative to the full model, but they highlight which methods introduce additional learnable tensors beyond the vanilla LoRA factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "plotlyServerURL": "https://plot.ly"
              },
              "data": [
                {
                  "hovertemplate": "Adapter=%{x}<br>Trainable params=%{y}<br>% trainable=%{text}<extra></extra>",
                  "legendgroup": "",
                  "marker": {
                    "color": "#636efa",
                    "pattern": {
                      "shape": ""
                    }
                  },
                  "name": "",
                  "orientation": "v",
                  "showlegend": false,
                  "text": [
                    "0.2089%",
                    "0.1569%",
                    "0.4170%",
                    "0.2263%",
                    "0.0697%"
                  ],
                  "textposition": "outside",
                  "type": "bar",
                  "x": [
                    "LoRA",
                    "AdaLoRA",
                    "LoHa",
                    "DoRA",
                    "HRA"
                  ],
                  "xaxis": "x",
                  "y": {
                    "bdata": "AEACAEiwAQAAgAQAAHACAADAAAA=",
                    "dtype": "i4"
                  },
                  "yaxis": "y"
                }
              ],
              "layout": {
                "barmode": "relative",
                "legend": {
                  "tracegroupgap": 0
                },
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermap": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermap"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                },
                "title": {
                  "text": "Trainable parameters per adapter type"
                },
                "xaxis": {
                  "anchor": "y",
                  "domain": [
                    0,
                    1
                  ],
                  "title": {
                    "text": "Adapter"
                  }
                },
                "yaxis": {
                  "anchor": "x",
                  "domain": [
                    0,
                    1
                  ],
                  "title": {
                    "text": "Parameters"
                  }
                }
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(df, x=\"Adapter\", y=\"Trainable params\", text=\"% trainable\", title=\"Trainable parameters per adapter type\")\n",
        "fig.update_traces(textposition=\"outside\")\n",
        "fig.update_layout(yaxis_title=\"Parameters\", xaxis_title=\"Adapter\")\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cb311863",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== LoRA ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2696/259473458.py:213: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  return Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:13, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.704800</td>\n",
              "      <td>2.698495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.613100</td>\n",
              "      <td>2.678968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.620000</td>\n",
              "      <td>2.678347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== AdaLoRA ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/adalora/config.py:96: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_2696/259473458.py:213: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  return Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:18, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.847500</td>\n",
              "      <td>3.278960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.397800</td>\n",
              "      <td>3.112560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.292000</td>\n",
              "      <td>3.078031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== DoRA ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2696/259473458.py:213: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  return Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:19, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.702900</td>\n",
              "      <td>2.702206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.612600</td>\n",
              "      <td>2.683782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.617300</td>\n",
              "      <td>2.681008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== LoHa ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2696/259473458.py:213: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  return Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:16, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.775400</td>\n",
              "      <td>2.784306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.686600</td>\n",
              "      <td>2.759445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.702200</td>\n",
              "      <td>2.755796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== HRA ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2696/259473458.py:213: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  return Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:25, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.773100</td>\n",
              "      <td>2.783116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.695600</td>\n",
              "      <td>2.769071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.714900</td>\n",
              "      <td>2.766985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== Adapter Size ==\n",
            "   name  trainable_params  total_params  percent_trainable\n",
            "   LoRA            196608      70623232           0.278390\n",
            "AdaLoRA            295056      70721704           0.417207\n",
            "   DoRA            224256      70650880           0.317414\n",
            "   LoHa            393216      70819840           0.555234\n",
            "    HRA             86016      70512640           0.121987\n",
            "\n",
            "== Eval Metrics (lower is better) ==\n",
            "Adapter  eval_loss  perplexity\n",
            "   LoRA   2.678347   14.561006\n",
            "   DoRA   2.681008   14.599797\n",
            "   LoHa   2.755796   15.733567\n",
            "    HRA   2.766985   15.910594\n",
            "AdaLoRA   3.078031   21.715593\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, math, inspect, traceback\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from packaging import version\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    __version__ as hf_ver,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    AdaLoraConfig,\n",
        "    LoHaConfig,\n",
        "    HRAConfig,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Env tweaks\n",
        "# ------------------------------------------------------------------\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Base / target modules\n",
        "# ------------------------------------------------------------------\n",
        "BASE_MODEL = \"EleutherAI/pythia-70m-deduped\"\n",
        "TARGET_MODULES = [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Training budget (identical across adapters)\n",
        "# ------------------------------------------------------------------\n",
        "MAX_STEPS = 300\n",
        "LR = 1e-4\n",
        "BATCH_SIZE = 4\n",
        "GRAD_ACCUM = 2\n",
        "WARMUP_STEPS = 30\n",
        "MAX_LEN = 512\n",
        "EVAL_EVERY = 100\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Utilities\n",
        "# ------------------------------------------------------------------\n",
        "@dataclass\n",
        "class AdapterSize:\n",
        "    name: str\n",
        "    trainable_params: int\n",
        "    total_params: int\n",
        "    percent_trainable: float\n",
        "\n",
        "def prepare_config(config_cls, *, extra: Dict[str, Any] | None = None, **common_kwargs: Any):\n",
        "    extra = extra or {}\n",
        "    sig = inspect.signature(config_cls.__init__)\n",
        "    kwargs = {}\n",
        "    for name in sig.parameters:\n",
        "        if name == \"self\":\n",
        "            continue\n",
        "        if name in extra:\n",
        "            kwargs[name] = extra[name]\n",
        "        elif name in common_kwargs:\n",
        "            kwargs[name] = common_kwargs[name]\n",
        "    return config_cls(**kwargs)\n",
        "\n",
        "def count_trainable_parameters(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def count_total_parameters(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def get_tokenizer():\n",
        "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "    return tok\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Collator\n",
        "# ------------------------------------------------------------------\n",
        "from dataclasses import dataclass as _dataclass\n",
        "@_dataclass\n",
        "class SFTDataCollator:\n",
        "    tokenizer: Any\n",
        "    label_pad_token_id: int = -100\n",
        "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "        feats_wo_labels = [{k: v for k, v in f.items() if k != \"labels\"} for f in features]\n",
        "        batch = self.tokenizer.pad(feats_wo_labels, padding=True, return_tensors=\"pt\")\n",
        "        max_len = batch[\"input_ids\"].size(1)\n",
        "        padded_labels = []\n",
        "        for l in labels:\n",
        "            if len(l) < max_len:\n",
        "                l = l + [self.label_pad_token_id] * (max_len - len(l))\n",
        "            else:\n",
        "                l = l[:max_len]\n",
        "            padded_labels.append(l)\n",
        "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Dataset (DROP 'prompt'/'output' BEFORE collator sees them)\n",
        "# ------------------------------------------------------------------\n",
        "def build_sft_splits(tokenizer, max_len=MAX_LEN, train_n=1000, eval_n=200):\n",
        "    ds = load_dataset(\"yahma/alpaca-cleaned\")\n",
        "    pool = ds[\"train\"].shuffle(seed=SEED).select(range(train_n + eval_n))\n",
        "    eval_raw = pool.select(range(eval_n))\n",
        "    train_raw = pool.select(range(eval_n, eval_n + train_n))\n",
        "\n",
        "    def format_example(ex):\n",
        "        instr = (ex.get(\"instruction\") or \"\").strip()\n",
        "        inp = (ex.get(\"input\") or \"\").strip()\n",
        "        out = (ex.get(\"output\") or \"\").strip()\n",
        "        if inp:\n",
        "            prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
        "        else:\n",
        "            prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n\"\n",
        "        return {\"prompt\": prompt, \"output\": out}\n",
        "\n",
        "    train_raw = train_raw.map(format_example, remove_columns=train_raw.column_names)\n",
        "    eval_raw  = eval_raw.map(format_example,  remove_columns=eval_raw.column_names)\n",
        "\n",
        "    def tok_id(s: str):\n",
        "        return tokenizer(s, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "    def build_io(example):\n",
        "        p_ids = tok_id(example[\"prompt\"])\n",
        "        o_ids = tok_id(example[\"output\"])\n",
        "        if len(o_ids) == 0:\n",
        "            return {\"discard\": True}\n",
        "        # ensure at least 1 supervised token\n",
        "        if len(p_ids) > max_len - 1:\n",
        "            p_ids = p_ids[: max_len - 1]\n",
        "        room = max_len - len(p_ids)\n",
        "        if room <= 0:\n",
        "            return {\"discard\": True}\n",
        "        o_ids = o_ids[:room]\n",
        "        if len(o_ids) == 0:\n",
        "            return {\"discard\": True}\n",
        "        input_ids = p_ids + o_ids\n",
        "        labels = [-100] * len(p_ids) + o_ids\n",
        "        attn = [1] * len(input_ids)\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels, \"discard\": False}\n",
        "\n",
        "    train_built = train_raw.map(build_io, remove_columns=train_raw.column_names)\n",
        "    eval_built  = eval_raw.map(build_io,  remove_columns=eval_raw.column_names)\n",
        "\n",
        "    # drop discarded rows and the helper flag\n",
        "    train_built = train_built.filter(lambda ex: not ex[\"discard\"]).remove_columns([\"discard\"])\n",
        "    eval_built  = eval_built.filter(lambda ex: not ex[\"discard\"]).remove_columns([\"discard\"])\n",
        "    return train_built, eval_built\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# PEFT-wrapped model\n",
        "# ------------------------------------------------------------------\n",
        "def make_peft_model(config_cls, *, extra: Dict[str, Any] | None = None):\n",
        "    base = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "    base.config.pad_token_id = base.config.pad_token_id or 0\n",
        "    base.requires_grad_(False)\n",
        "    cfg = prepare_config(\n",
        "        config_cls,\n",
        "        extra=extra,\n",
        "        r=4,\n",
        "        lora_alpha=8,\n",
        "        lora_dropout=0.05,\n",
        "        target_modules=TARGET_MODULES,\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    return get_peft_model(base, cfg)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Trainer\n",
        "# ------------------------------------------------------------------\n",
        "def make_trainer(peft_model, tokenizer, train_ds, eval_ds, run_name: str):\n",
        "    collator = SFTDataCollator(tokenizer=tokenizer, label_pad_token_id=-100)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"runs/{run_name}\",\n",
        "        overwrite_output_dir=True,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        learning_rate=LR,\n",
        "        max_steps=MAX_STEPS,\n",
        "        warmup_steps=WARMUP_STEPS,\n",
        "        weight_decay=0.0,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        eval_steps=EVAL_EVERY,\n",
        "        logging_steps=50,\n",
        "        save_steps=MAX_STEPS,\n",
        "        save_total_limit=1,\n",
        "        report_to=[],\n",
        "        remove_unused_columns=False,  # we pass exact fields\n",
        "        fp16=False, bf16=False,       # simple numerics\n",
        "        dataloader_num_workers=0,     # safest in notebooks\n",
        "        dataloader_pin_memory=False,\n",
        "        eval_strategy=\"steps\",\n",
        "    )\n",
        "    peft_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "    return Trainer(\n",
        "        model=peft_model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        tokenizer=tokenizer,          # ok despite FutureWarning\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Size reporter\n",
        "# ------------------------------------------------------------------\n",
        "def adapter_size_row(config_cls, name: str, *, extra: Dict[str, Any] | None = None) -> AdapterSize:\n",
        "    model = make_peft_model(config_cls, extra=extra)\n",
        "    trainable = count_trainable_parameters(model)\n",
        "    total = count_total_parameters(model)\n",
        "    out = AdapterSize(name, trainable, total, 100.0 * trainable / total)\n",
        "    del model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return out\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Main\n",
        "# ------------------------------------------------------------------\n",
        "def main():\n",
        "    train_ds, eval_ds = build_sft_splits(tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    adapters = [\n",
        "        (\"LoRA\",    LoraConfig,    dict()),\n",
        "        (\"AdaLoRA\", AdaLoraConfig, dict(\n",
        "            init_r=6, target_r=12, beta1=0.85, beta2=0.85,\n",
        "            tinit=int(0.1*MAX_STEPS), tfinal=int(0.5*MAX_STEPS), delta_t=int(0.1*MAX_STEPS),\n",
        "            total_step=MAX_STEPS,\n",
        "        )),\n",
        "        (\"DoRA\",    LoraConfig,    dict(use_dora=True)),\n",
        "        # Optional (comment out if your local `peft` lacks support)\n",
        "        (\"LoHa\",    LoHaConfig,    dict(hypercomplex_multiplier=2)),\n",
        "        (\"HRA\",     HRAConfig,     dict(num_householder_blocks=2)),\n",
        "    ]\n",
        "\n",
        "    size_rows, metrics_rows = [], []\n",
        "    for name, cfg_cls, extra in adapters:\n",
        "        print(f\"\\n==== {name} ====\")\n",
        "        try:\n",
        "            sz = adapter_size_row(cfg_cls, name, extra=extra)\n",
        "            size_rows.append(sz)\n",
        "\n",
        "            model = make_peft_model(cfg_cls, extra=extra)\n",
        "            model.to(DEVICE)\n",
        "\n",
        "            trainer = make_trainer(model, tokenizer, train_ds, eval_ds, run_name=f\"{name.lower()}_sft\")\n",
        "            trainer.train()\n",
        "            eval_metrics = trainer.evaluate()\n",
        "\n",
        "            loss = float(eval_metrics[\"eval_loss\"])\n",
        "            if math.isnan(loss) or math.isinf(loss):\n",
        "                raise ValueError(f\"eval_loss is {loss}\")\n",
        "            ppl = float(math.exp(min(20.0, loss)))\n",
        "            metrics_rows.append({\"Adapter\": name, \"eval_loss\": loss, \"perplexity\": ppl})\n",
        "\n",
        "            del trainer, model\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[SKIP] {name} failed:\\n{e}\")\n",
        "            traceback.print_exc()\n",
        "            metrics_rows.append({\"Adapter\": name, \"eval_loss\": float('inf'), \"perplexity\": float('inf')})\n",
        "\n",
        "    print(\"\\n== Adapter Size ==\")\n",
        "    print(pd.DataFrame([{**r.__dict__} for r in size_rows]).to_string(index=False))\n",
        "    print(\"\\n== Eval Metrics (lower is better) ==\")\n",
        "    print(pd.DataFrame(metrics_rows).sort_values('perplexity').to_string(index=False))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways\n",
        "\n",
        "- All of the covered methods are exposed through the same `get_peft_model` workflow, so swapping adapters is mostly a matter of changing the config class.\n",
        "- Adaptive or structured variants incur a modest increase in trainable parameters but still stay within the PEFT regime.\n",
        "- Choosing between them depends on the task: AdaLoRA shines when different layers need different capacities, LoHa and HRA introduce richer geometric biases, and DoRA stabilises training when weight scales vary a lot.\n",
        "\n",
        "These adapters are complementary to the optimisation strategies introduced earlier in the book, and they can be combined with quantisation or pruning techniques from the other appendices."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
