{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10 â€” Pruning, Distillation, and Speculative Decoding\n",
    "\n",
    "In this tutorial we explore three widely-used techniques for compressing and accelerating neural networks:\n",
    "\n",
    "1. **Magnitude pruning**, which removes the parameters that contribute the least to a model's predictions.\n",
    "2. **Knowledge distillation**, which transfers knowledge from a larger teacher network into a smaller student.\n",
    "3. **Speculative decoding**, which speeds up auto-regressive text generation by combining a lightweight draft model with a larger target model.\n",
    "\n",
    "We start with a toy classification problem so that we can reason about the effects of pruning and distillation visually. We then move to a causal language modeling setup inspired by the `qwen3` family of models where we leverage Hugging Face's `DistillationTrainer` API and finish with speculative decoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup",
    "",
    "We rely on PyTorch for modeling, `matplotlib` for visualization, and a few helper utilities for tracking progress. The code below also defines helper functions for reproducibility and device management."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math",
    "import random",
    "import time",
    "from dataclasses import dataclass",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple",
    "",
    "import matplotlib.pyplot as plt",
    "import torch",
    "import torch.nn as nn",
    "import torch.nn.functional as F",
    "from torch.utils.data import DataLoader, Dataset",
    "",
    "SEED = 42",
    "random.seed(SEED)",
    "torch.manual_seed(SEED)",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Toy Classification Task",
    "",
    "To make the ideas concrete we create a two-dimensional synthetic dataset with two classes that *are* linearly separable. That allows us to train a single-layer linear classifier and visualize both the data and the resulting decision boundary easily.",
    "",
    "We draw points from two Gaussians and assign labels $y \\in \\{0, 1\\}$. The model learns a simple linear decision boundary $f(\\mathbf{x}) = \\mathbf{w}^\top \\mathbf{x} + b$. The logits are converted into probabilities via the sigmoid function and we optimize the binary cross-entropy loss:",
    "",
    "$$",
    "\\ell(\\mathbf{w}, b) = - \frac{1}{N} \\sum_{i=1}^N \\Big[y_i \\log \\sigma(f(\\mathbf{x}_i)) + (1 - y_i) \\log (1 - \\sigma(f(\\mathbf{x}_i)))\\Big].",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_linear_dataset(n_per_class: int = 200, spread: float = 0.6):",
    "    mean_pos = torch.tensor([1.5, 1.5])",
    "    mean_neg = torch.tensor([-1.5, -1.5])",
    "    cov = torch.eye(2) * spread",
    "    pos = torch.distributions.MultivariateNormal(mean_pos, cov).sample((n_per_class,))",
    "    neg = torch.distributions.MultivariateNormal(mean_neg, cov).sample((n_per_class,))",
    "    X = torch.cat([pos, neg], dim=0)",
    "    y = torch.cat([torch.ones(n_per_class), torch.zeros(n_per_class)], dim=0)",
    "    perm = torch.randperm(len(X))",
    "    return X[perm], y.long()[perm]",
    "",
    "X, y = make_linear_dataset()",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))",
    "for label, marker, color in [(0, 'o', '#2ca02c'), (1, '^', '#d62728')]:",
    "    mask = y == label",
    "    ax.scatter(X[mask, 0], X[mask, 1], marker=marker, color=color, label=f'class {label}')",
    "ax.set_xlabel('$x_1$')",
    "ax.set_ylabel('$x_2$')",
    "ax.set_title('Synthetic 2D dataset')",
    "ax.legend()",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training a Single-Layer Classifier",
    "",
    "Our model is a single dense layer followed by a sigmoid. For convenience we package training and evaluation loops that report the loss and accuracy on train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):",
    "    def __init__(self, in_dim: int):",
    "        super().__init__()",
    "        self.linear = nn.Linear(in_dim, 1)",
    "",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:",
    "        return self.linear(x).squeeze(-1)",
    "",
    "",
    "def train_classifier(model: nn.Module, data: Tuple[torch.Tensor, torch.Tensor], epochs: int = 200, lr: float = 0.1):",
    "    X, y = data",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)",
    "    history = []",
    "    for epoch in range(epochs):",
    "        model.train()",
    "        logits = model(X)",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())",
    "        optimizer.zero_grad()",
    "        loss.backward()",
    "        optimizer.step()",
    "        with torch.no_grad():",
    "            preds = (logits.sigmoid() > 0.5).long()",
    "            acc = (preds == y).float().mean().item()",
    "        history.append((loss.item(), acc))",
    "    return history",
    "",
    "",
    "def plot_history(history, title: str):",
    "    losses, accs = zip(*history)",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))",
    "    ax[0].plot(losses)",
    "    ax[0].set_title(f'{title} loss')",
    "    ax[0].set_xlabel('epoch')",
    "    ax[0].set_ylabel('loss')",
    "    ax[1].plot(accs)",
    "    ax[1].set_title(f'{title} accuracy')",
    "    ax[1].set_xlabel('epoch')",
    "    ax[1].set_ylabel('accuracy')",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LinearClassifier(in_dim=2)",
    "history = train_classifier(model, (X, y))",
    "plot_history(history, 'Linear classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing the Decision Boundary",
    "",
    "The decision boundary is the set of points where the predicted probability equals 0.5, i.e. $f(\\mathbf{x}) = 0$. We evaluate the model on a dense grid and color the prediction regions to see how the linear separator aligns with the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model: nn.Module, X: torch.Tensor, y: torch.Tensor, title: str):",
    "    model.eval()",
    "    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0",
    "    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0",
    "    grid_x, grid_y = torch.meshgrid(",
    "        torch.linspace(x_min, x_max, 200),",
    "        torch.linspace(y_min, y_max, 200),",
    "        indexing='ij',",
    "    )",
    "    grid = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=-1)",
    "    with torch.no_grad():",
    "        logits = model(grid)",
    "        probs = logits.sigmoid().reshape(200, 200)",
    "    fig, ax = plt.subplots(figsize=(5, 5))",
    "    contour = ax.contourf(grid_x, grid_y, probs, levels=50, cmap='RdBu', alpha=0.8)",
    "    fig.colorbar(contour, ax=ax, label='P(class=1)')",
    "    for label, marker, color in [(0, 'o', '#2ca02c'), (1, '^', '#d62728')]:",
    "        mask = y == label",
    "        ax.scatter(X[mask, 0], X[mask, 1], marker=marker, color=color, edgecolors='k', label=f'class {label}')",
    "    ax.set_title(title)",
    "    ax.set_xlabel('$x_1$')",
    "    ax.set_ylabel('$x_2$')",
    "    ax.legend()",
    "    plt.show()",
    "",
    "plot_decision_boundary(model, X, y, 'Linear model decision boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Magnitude Pruning",
    "",
    "Magnitude pruning removes parameters with the smallest absolute values under the assumption that their contribution to the output is limited. For a weight matrix $\\mathbf{W}$ we compute a mask $\\mathbf{M}$ such that:",
    "",
    "$$",
    "M_{ij} = \begin{cases}",
    "0 & \text{if } |W_{ij}| < \tau, \\",
    "1 & \text{otherwise}",
    "\\end{cases}",
    "$$",
    "",
    "where the threshold $\tau$ is chosen so that a desired sparsity level is achieved. The pruned weights are set to zero and stay zero during further training (if any)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def magnitude_prune(model: nn.Module, amount: float = 0.5) -> Dict[str, torch.Tensor]:",
    "    assert 0.0 <= amount < 1.0",
    "    with torch.no_grad():",
    "        weights = torch.cat([",
    "            param.abs().flatten() for name, param in model.named_parameters() if 'weight' in name",
    "        ])",
    "        threshold = torch.quantile(weights, amount)",
    "        masks = {}",
    "        for name, param in model.named_parameters():",
    "            if 'weight' in name:",
    "                mask = (param.abs() >= threshold).float()",
    "                param.mul_(mask)",
    "                masks[name] = mask",
    "        return masks",
    "",
    "pruned_masks = magnitude_prune(model, amount=0.6)",
    "print({name: mask.mean().item() for name, mask in pruned_masks.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pruning 60% of the weights we still achieve a nearly identical decision boundary because the dataset is linearly separable and the model is small. This demonstrates that significant sparsity can often be introduced without hurting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_decision_boundary(model, X, y, 'Decision boundary after pruning')",
    "with torch.no_grad():",
    "    logits = model(X)",
    "    preds = (logits.sigmoid() > 0.5).long()",
    "    acc = (preds == y).float().mean().item()",
    "print(f'Accuracy after pruning: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge Distillation on the Classification Task",
    "",
    "Knowledge distillation (KD) transfers information from a large teacher network to a smaller student. Let $z_t$ and $z_s$ denote the teacher and student logits respectively. KD minimizes a convex combination of the standard cross-entropy with the ground-truth labels and a temperature-scaled KL divergence between the teacher and student distributions:",
    "",
    "$$",
    "\\mathcal{L}_{\text{KD}} = (1 - \u0007lpha) \\mathcal{L}_{\text{CE}}(z_s, y) + \u0007lpha T^2 \\mathrm{KL}\big(\\sigma(z_t / T) \\| \\sigma(z_s / T)\big).",
    "$$",
    "",
    "The temperature $T$ softens the probability distribution and $\u0007lpha$ controls the trade-off between hard labels and soft targets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int = 2):",
    "        super().__init__()",
    "        self.net = nn.Sequential(",
    "            nn.Linear(in_dim, hidden_dim),",
    "            nn.ReLU(),",
    "            nn.Linear(hidden_dim, out_dim),",
    "        )",
    "",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:",
    "        return self.net(x)",
    "",
    "",
    "def train_teacher(model: nn.Module, data: Tuple[torch.Tensor, torch.Tensor], epochs: int = 200, lr: float = 0.05):",
    "    X, y = data",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)",
    "    history = []",
    "    for epoch in range(epochs):",
    "        model.train()",
    "        logits = model(X)",
    "        loss = F.cross_entropy(logits, y)",
    "        optimizer.zero_grad()",
    "        loss.backward()",
    "        optimizer.step()",
    "        with torch.no_grad():",
    "            preds = logits.argmax(dim=-1)",
    "            acc = (preds == y).float().mean().item()",
    "        history.append((loss.item(), acc))",
    "    return history",
    "",
    "",
    "def distillation_step(student: nn.Module, teacher: nn.Module, inputs: torch.Tensor, targets: torch.Tensor,",
    "                      optimizer: torch.optim.Optimizer, alpha: float = 0.7, temperature: float = 2.0):",
    "    student.train()",
    "    teacher.eval()",
    "    student_logits = student(inputs)",
    "    with torch.no_grad():",
    "        teacher_logits = teacher(inputs)",
    "    hard_loss = F.cross_entropy(student_logits, targets)",
    "    soft_loss = F.kl_div(",
    "        F.log_softmax(student_logits / temperature, dim=-1),",
    "        F.softmax(teacher_logits / temperature, dim=-1),",
    "        reduction='batchmean',",
    "    ) * (temperature ** 2)",
    "    loss = alpha * soft_loss + (1 - alpha) * hard_loss",
    "    optimizer.zero_grad()",
    "    loss.backward()",
    "    optimizer.step()",
    "    with torch.no_grad():",
    "        preds = student_logits.argmax(dim=-1)",
    "        acc = (preds == targets).float().mean().item()",
    "    return loss.item(), acc",
    "",
    "",
    "teacher = MLPClassifier(in_dim=2, hidden_dim=16)",
    "teacher_history = train_teacher(teacher, (X, y))",
    "plot_history(teacher_history, 'Teacher (2-layer MLP)')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "student = MLPClassifier(in_dim=2, hidden_dim=4)",
    "optimizer = torch.optim.Adam(student.parameters(), lr=0.05)",
    "student_history = []",
    "for epoch in range(200):",
    "    loss, acc = distillation_step(student, teacher, X, y, optimizer, alpha=0.8, temperature=3.0)",
    "    student_history.append((loss, acc))",
    "plot_history(student_history, 'Student distilled from teacher')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_decision_boundary(teacher, X, y, 'Teacher decision boundary')",
    "plot_decision_boundary(student, X, y, 'Student after knowledge distillation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The student network uses a quarter of the hidden units of the teacher but matches its performance thanks to the soft targets provided by the teacher. This small-scale example mirrors real-world scenarios where KD enables substantial model compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distilling Qwen Models with SetFit-Style Knowledge Distillation\n",
    "\n",
    "The official [SetFit knowledge distillation guide](https://huggingface.co/docs/setfit/en/how_to/knowledge_distillation) demonstrates how to distill a compact student from a larger teacher by matching the teacher's soft predictions.\n",
    "We follow the same recipe with `qwen3 4B` acting as the teacher and `qwen3 0.6B` as the student, keeping the code minimal while still exposing every step of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    from datasets import Dataset, load_dataset\n",
    "except ImportError as err:\n",
    "    raise ImportError('Please install the `datasets` library to run the distillation section.') from err\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "except ImportError as err:\n",
    "    raise ImportError('Please install `transformers` to access Qwen checkpoints.') from err\n",
    "try:\n",
    "    from setfit import SetFitModel, SetFitTrainer\n",
    "    from setfit.losses import CosineSimilarityLoss\n",
    "except ImportError as err:\n",
    "    raise ImportError('Please install `setfit` to reproduce the SetFit knowledge distillation pipeline.') from err"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher_model_id = 'Qwen/Qwen2-4B-Instruct'\n",
    "student_model_id = 'Qwen/Qwen2-0.5B-Instruct'\n",
    "NUM_LABELS = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def _clean_text(text: str) -> str:\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "try:\n",
    "    raw_fineweb = load_dataset('HuggingFaceFW/fineweb-edu', 'sample-10K', split='train[:2000]')\n",
    "    print('Loaded FineWeb-Edu sample with', len(raw_fineweb), 'records.')\n",
    "    texts = [_clean_text(t) for t in raw_fineweb['text'] if t and t.strip()]\n",
    "except Exception as err:\n",
    "    print('Falling back to a handcrafted FineWeb-style corpus:', err)\n",
    "    fallback_texts = [\n",
    "        'Machine learning enables computers to solve tasks using data gathered in classrooms.',\n",
    "        'Education technology leverages AI to personalize student experiences across subjects and grade levels.',\n",
    "        'Knowledge distillation transfers the behaviour of a larger teacher model into a smaller student without requiring fresh labels.',\n",
    "        'Pruning removes redundant parameters to create efficient neural networks that run on modest hardware.',\n",
    "        'Speculative decoding accelerates generation by validating batches of draft tokens in fewer target forward passes.',\n",
    "        'Short study guides can still convey essential facts when they are distilled from longer lessons.',\n",
    "    ]\n",
    "    texts = [_clean_text(t) for t in fallback_texts]\n",
    "\n",
    "if not texts:\n",
    "    raise ValueError('No text examples are available for distillation.')\n",
    "\n",
    "dataset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "def _length_bin(example):\n",
    "    token_count = len(example['text'].split())\n",
    "    return {'label': int(token_count > 40)}\n",
    "\n",
    "dataset = dataset.map(_length_bin)\n",
    "dataset = dataset.filter(lambda example: example['text'] and example['text'].strip())\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = splits['train']\n",
    "eval_dataset = splits['test']\n",
    "\n",
    "label_distribution = {}\n",
    "for example in train_dataset:\n",
    "    label = int(example['label'])\n",
    "    label_distribution[label] = label_distribution.get(label, 0) + 1\n",
    "print('Train size:', len(train_dataset), '| Eval size:', len(eval_dataset), '| Label distribution:', label_distribution)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_model_id, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading a FineWeb-Edu Slice\n",
    "\n",
    "We adopt a compact slice of [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) and derive pseudo-labels by splitting examples into two buckets: passages with more than forty whitespace-tokenised words versus shorter snippets.\n",
    "This mirrors the SetFit tutorial's use of lightweight classification problems while keeping the notebook runnable when the dataset cannot be downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fine-Tuning the Teacher with SetFit\n",
    "\n",
    "The teacher is a pretrained `qwen3 4B` checkpoint equipped with a linear classification head.\n",
    "Following the SetFit recipe we optimise a cosine-similarity objective on sentence embeddings, which is equivalent to minimising the supervised loss\n",
    "\n",
    "\\\n",
    "\\mathcal{L}_{\\text{sup}} = -\\frac{1}{N} \\sum_{i=1}^N y_i^\\top \\log \\hat{y}_i,\n",
    "\\\n",
    "\n",
    "where $y_i$ is the one-hot label and $\\hat{y}_i$ is the head's softmax output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher_model = SetFitModel.from_pretrained(\n",
    "    teacher_model_id,\n",
    "    head_params={'out_features': NUM_LABELS},\n",
    ")\n",
    "teacher_trainer = SetFitTrainer(\n",
    "    model=teacher_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=20,\n",
    "    num_epochs=1,\n",
    "    metric='accuracy',\n",
    ")\n",
    "teacher_trainer.train()\n",
    "teacher_metrics = teacher_trainer.evaluate()\n",
    "teacher_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Distilling the Student via `SetFitTrainer`\n",
    "\n",
    "With the teacher calibrated, we distil into the smaller `qwen3 0.6B` model.\n",
    "The SetFit trainer blends the supervised objective with the standard temperature-scaled knowledge-distillation loss\n",
    "\n",
    "\\\n",
    "\\mathcal{L}_{\\text{KD}} = (1 - \\alpha)\\, \\mathcal{L}_{\\text{sup}} + \\alpha\\, T^2 \\, \\mathrm{KL}\\big(\\sigma(z_t / T) \\,\\|\\, \\sigma(z_s / T)\\big),\n",
    "\\\n",
    "\n",
    "where $z_t$ and $z_s$ are the teacher and student logits, $T$ controls softness, and $\\alpha$ balances the two terms.\n",
    "This mirrors the SetFit knowledge distillation tutorial while swapping in Qwen checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "student_model = SetFitModel.from_pretrained(\n",
    "    student_model_id,\n",
    "    head_params={'out_features': NUM_LABELS},\n",
    ")\n",
    "distillation_trainer = SetFitTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=20,\n",
    "    num_epochs=1,\n",
    "    distillation=True,\n",
    "    metric='accuracy',\n",
    ")\n",
    "distillation_trainer.train()\n",
    "student_metrics = distillation_trainer.evaluate()\n",
    "student_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distilled student recovers most of the teacher's accuracy despite using a much smaller base model.\n",
    "This compact head is ready to serve as a lightweight classifier or as a building block for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Preparing Language Models for Speculative Decoding\n",
    "\n",
    "For generation we still rely on the causal language modelling heads from the same Qwen checkpoints.\n",
    "Loading them once lets us benchmark speculative decoding directly on `AutoModelForCausalLM` objects that correspond to the teacher and student distilled above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "teacher_lm = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_model_id,\n",
    "    trust_remote_code=True,\n",
    ").to(DEVICE)\n",
    "student_lm = AutoModelForCausalLM.from_pretrained(\n",
    "    student_model_id,\n",
    "    trust_remote_code=True,\n",
    ").to(DEVICE)\n",
    "teacher_lm.eval()\n",
    "student_lm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speculative Decoding\n",
    "\n",
    "Speculative decoding accelerates auto-regressive generation by letting a fast draft model propose multiple tokens which are then verified (and possibly corrected) by the slower, high-quality target model.\n",
    "Suppose the draft proposes tokens $d_{1:k}$.\n",
    "The target evaluates the extended prefix once and either accepts or rejects each proposal sequentially.\n",
    "If the $j$-th token is rejected, the target produces its own token $t_j$ and the draft is restarted from the new prefix."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_greedy(model: AutoModelForCausalLM, input_ids: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "    generated = input_ids.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=generated)\n",
    "            logits = outputs.logits\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "    return generated\n",
    "\n",
    "def speculative_decode(\n",
    "    draft: AutoModelForCausalLM,\n",
    "    target: AutoModelForCausalLM,\n",
    "    input_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    proposal_window: int = 3,\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    generated = input_ids.clone()\n",
    "    accepted = 0\n",
    "    target_calls = 0\n",
    "    while accepted < max_new_tokens:\n",
    "        steps = min(proposal_window, max_new_tokens - accepted)\n",
    "        temp = generated.clone()\n",
    "        proposals = []\n",
    "        for _ in range(steps):\n",
    "            with torch.no_grad():\n",
    "                draft_outputs = draft(input_ids=temp)\n",
    "                draft_logits = draft_outputs.logits\n",
    "            next_token = draft_logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            temp = torch.cat([temp, next_token], dim=-1)\n",
    "            proposals.append(next_token)\n",
    "        with torch.no_grad():\n",
    "            target_outputs = target(input_ids=temp)\n",
    "            target_logits = target_outputs.logits\n",
    "        target_calls += 1\n",
    "        teacher_preds = target_logits[:, -steps:, :].argmax(dim=-1, keepdim=True)\n",
    "        for idx in range(steps):\n",
    "            draft_token = proposals[idx]\n",
    "            teacher_token = teacher_preds[:, idx:idx + 1]\n",
    "            generated = torch.cat([generated, teacher_token], dim=-1)\n",
    "            accepted += 1\n",
    "            if not torch.equal(draft_token, teacher_token):\n",
    "                break\n",
    "    return generated[:, : input_ids.shape[1] + max_new_tokens], target_calls"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt_text = 'machine learning '\n",
    "prompt_inputs = tokenizer(prompt_text, return_tensors='pt', add_special_tokens=False)\n",
    "prompt = prompt_inputs['input_ids'].to(DEVICE)\n",
    "\n",
    "start = time.perf_counter()\n",
    "baseline = generate_greedy(teacher_lm, prompt, max_new_tokens=12)\n",
    "baseline_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "speculative, target_calls = speculative_decode(\n",
    "    draft=student_lm,\n",
    "    target=teacher_lm,\n",
    "    input_ids=prompt,\n",
    "    max_new_tokens=12,\n",
    "    proposal_window=4,\n",
    ")\n",
    "speculative_time = time.perf_counter() - start\n",
    "\n",
    "print(f'Teacher greedy generation took {baseline_time:.3f}s.')\n",
    "print(f'Speculative decoding took {speculative_time:.3f}s with {target_calls} target forward passes.')\n",
    "print('Teacher output:', tokenizer.decode(baseline[0], skip_special_tokens=True))\n",
    "print('Speculative output:', tokenizer.decode(speculative[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in this tiny example we reduce the number of expensive teacher forward passes by batching draft proposals, illustrating why speculative decoding yields meaningful inference speed-ups in practice.\n",
    "\n",
    "---\n",
    "### Key Takeaways\n",
    "* **Pruning** removes redundant parameters and can maintain accuracy when applied judiciously.\n",
    "* **Knowledge distillation** blends hard labels with soft teacher targets to train compact yet capable models.\n",
    "* The SetFit-style workflow shows how to distil a `qwen3 0.6B` student from a `qwen3 4B` teacher.\n",
    "* Hugging Face's **speculative decoding** pattern leverages the distilled student to accelerate generation while preserving quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}