{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2283aa4",
   "metadata": {},
   "source": [
    "# 08 \u00b7 Reinforcement Learning from Human Feedback for ShrayGPT\n",
    "\n",
    "This notebook shows how to continue fine-tuning the `shraygpt-instruct` checkpoint with Proximal Policy Optimization (PPO) using preference data from the Anthropic RLHF corpus. We replace the demo Hugging Face model from earlier versions so the reinforcement learning stage directly follows the supervised fine-tuning work from notebook 07.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8b9e5",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "* Prepare the Anthropic human preference dataset so it matches the conversation format used during ShrayGPT's supervised fine-tuning.\n",
    "* Build a PPO loop that optimizes the `shraygpt-instruct` policy while keeping a frozen reference copy for KL regularization.\n",
    "* Shape lexical rewards from the preference pairs and monitor key PPO diagnostics such as KL and entropy.\n",
    "* Evaluate how the RLHF-tuned policy behaves compared with the SFT baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84633d",
   "metadata": {},
   "source": [
    "> **Note:** This walkthrough keeps the computation light so it can run on commodity hardware. Real PPO runs for ShrayGPT require more rollouts, gradient accumulation, and likely mixed-precision training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd2690",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "We rely on the local ShrayGPT checkpoint, `tiktoken` for byte-pair encoding, the Hugging Face datasets library for loading Anthropic preferences, and PyTorch for optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "91f7278b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from src.shraygpt import ShrayGPT\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "POLICY_CHECKPOINT = CHECKPOINT_DIR / 'shraygpt-instruct.ckpt'\n",
    "REF_CHECKPOINT = CHECKPOINT_DIR / 'shraygpt-instruct.ckpt'\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57c403",
   "metadata": {},
   "source": [
    "### Loading Anthropic preference pairs\n",
    "\n",
    "The Anthropic HH-RLHF dataset contains multi-turn conversations with preferred (`chosen`) and dispreferred (`rejected`) assistant replies. Each transcript already alternates `Human:` and `Assistant:` turns, so we recover the shared prefix as the prompt and keep the diverging final assistant messages as responses. This keeps the PPO prompts consistent with the conversation format used during ShrayGPT's supervised fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7ebe343",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROLE_PREFIXES = {\n",
    "    'system': 'System:',\n",
    "    'user': 'User:',\n",
    "    'assistant': 'Assistant:',\n",
    "}\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are ShrayGPT, a helpful and concise AI assistant.'\n",
    "\n",
    "\n",
    "def format_conversation(messages: Sequence[Tuple[str, str]], default_system: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    formatted: List[str] = []\n",
    "    has_system = any(role == 'system' for role, _ in messages)\n",
    "    if not has_system:\n",
    "        formatted.append(f\"{ROLE_PREFIXES['system']} {default_system.strip()}\")\n",
    "\n",
    "    for role, content in messages:\n",
    "        prefix = ROLE_PREFIXES.get(role)\n",
    "        if prefix is None:\n",
    "            continue\n",
    "        formatted.append(f\"{prefix} {content.strip()}\")\n",
    "\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def parse_anthropic_conversation(transcript: str) -> List[Tuple[str, str]]:\n",
    "    turns: List[Tuple[str, str]] = []\n",
    "    for block in transcript.strip().split(\"\\n\\n\"):\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "        if block.startswith('Human:'):\n",
    "            turns.append(('user', block[len('Human:'):].strip()))\n",
    "        elif block.startswith('Assistant:'):\n",
    "            content = block[len('Assistant:'):].strip()\n",
    "            if content:\n",
    "                turns.append(('assistant', content))\n",
    "    return turns\n",
    "\n",
    "\n",
    "def shared_prefix(chosen_turns: Sequence[Tuple[str, str]], rejected_turns: Sequence[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    prefix: List[Tuple[str, str]] = []\n",
    "    for (c_role, c_text), (r_role, r_text) in zip(chosen_turns, rejected_turns):\n",
    "        if c_role == r_role and c_text == r_text:\n",
    "            prefix.append((c_role, c_text))\n",
    "        else:\n",
    "            break\n",
    "    return prefix\n",
    "\n",
    "\n",
    "def extract_assistant_response(turns: Sequence[Tuple[str, str]], start: int) -> str:\n",
    "    for role, text in turns[start:]:\n",
    "        if role == 'assistant':\n",
    "            return text\n",
    "    for role, text in reversed(turns):\n",
    "        if role == 'assistant':\n",
    "            return text\n",
    "    return ''\n",
    "\n",
    "\n",
    "def prepare_anthropic(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    chosen_turns = parse_anthropic_conversation(example['chosen'])\n",
    "    rejected_turns = parse_anthropic_conversation(example['rejected'])\n",
    "\n",
    "    prefix_turns = shared_prefix(chosen_turns, rejected_turns)\n",
    "    prompt_turns = [('system', DEFAULT_SYSTEM_PROMPT)] + prefix_turns\n",
    "\n",
    "    chosen_response = extract_assistant_response(chosen_turns, len(prefix_turns))\n",
    "    rejected_response = extract_assistant_response(rejected_turns, len(prefix_turns))\n",
    "\n",
    "    prompt_messages = prompt_turns + [('assistant', '')]\n",
    "    chosen_messages = prompt_turns + [('assistant', chosen_response)]\n",
    "    rejected_messages = prompt_turns + [('assistant', rejected_response)]\n",
    "\n",
    "    return {\n",
    "        'prompt': format_conversation(prompt_messages),\n",
    "        'chosen': format_conversation(chosen_messages),\n",
    "        'rejected': format_conversation(rejected_messages),\n",
    "        'chosen_response': chosen_response,\n",
    "        'rejected_response': rejected_response,\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = load_dataset('Anthropic/hh-rlhf', split='train')\n",
    "dataset = dataset.map(prepare_anthropic, remove_columns=dataset.column_names)\n",
    "small_dataset = dataset.shuffle(seed=42).select(range(256))\n",
    "small_dataset[0]\n",
    "reward_splits = small_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "reward_train_dataset = reward_splits['train']\n",
    "reward_eval_dataset = reward_splits['test']\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2936f3",
   "metadata": {},
   "source": [
    "### Tokenization utilities\n",
    "\n",
    "We reuse the `tiktoken` vocabulary from pre-training and supervised fine-tuning. The prompt mask tracks the prefix tokens so PPO only updates the generated assistant continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a207b60d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = tiktoken.get_encoding('r50k_base')\n",
    "PAD_TOKEN_ID = tokenizer.eot_token\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenizedBatch:\n",
    "    input_ids: torch.LongTensor\n",
    "    attention_mask: torch.LongTensor\n",
    "    prompt_mask: torch.BoolTensor\n",
    "\n",
    "\n",
    "def tokenize_batch(prompts: List[str], responses: List[str]) -> TokenizedBatch:\n",
    "    prompt_tokens = [tokenizer.encode(p) for p in prompts]\n",
    "    response_tokens = [tokenizer.encode(r) for r in responses]\n",
    "    combined = [p + r for p, r in zip(prompt_tokens, response_tokens)]\n",
    "    max_len = max(len(seq) for seq in combined)\n",
    "\n",
    "    input_ids = torch.full((len(combined), max_len), PAD_TOKEN_ID, dtype=torch.long)\n",
    "    attention_mask = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "    prompt_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    for i, (p_tokens, r_tokens) in enumerate(zip(prompt_tokens, response_tokens)):\n",
    "        sequence = p_tokens + r_tokens\n",
    "        seq_len = len(sequence)\n",
    "        if seq_len == 0:\n",
    "            continue\n",
    "        input_ids[i, :seq_len] = torch.tensor(sequence, dtype=torch.long)\n",
    "        attention_mask[i, :seq_len] = 1\n",
    "        prompt_mask[i, :len(p_tokens)] = True\n",
    "\n",
    "    return TokenizedBatch(input_ids=input_ids, attention_mask=attention_mask, prompt_mask=prompt_mask)\n",
    "\n",
    "\n",
    "def tokenize_texts(texts: List[str]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    tokenized = [tokenizer.encode(text) for text in texts]\n",
    "    max_len = max((len(seq) for seq in tokenized), default=1)\n",
    "\n",
    "    input_ids = torch.full((len(tokenized), max_len), PAD_TOKEN_ID, dtype=torch.long)\n",
    "    attention_mask = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(tokenized):\n",
    "        if not seq:\n",
    "            continue\n",
    "        seq_tensor = torch.tensor(seq, dtype=torch.long)\n",
    "        input_ids[i, : len(seq)] = seq_tensor\n",
    "        attention_mask[i, : len(seq)] = 1\n",
    "\n",
    "    return input_ids, attention_mask\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d8989",
   "metadata": {},
   "source": [
    "## Implementing PPO\n",
    "\n",
    "PPO optimizes a policy by constraining updates to remain close to the previous policy through a clipped surrogate loss. For language models we operate in log-probability space and mask the prompt tokens because they are provided by the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d5e29",
   "metadata": {},
   "source": [
    "### Configuration dataclasses\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3244c386",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    kl_coef: float = 0.1\n",
    "    clip_range: float = 0.2\n",
    "    vf_coef: float = 0.1\n",
    "    ent_coef: float = 0.01\n",
    "    target_kl: float = 0.1\n",
    "    num_epochs: int = 1\n",
    "    batch_size: int = 4\n",
    "    mini_batch_size: int = 2\n",
    "    max_new_tokens: int = 128\n",
    "    learning_rate: float = 1e-5\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardConfig:\n",
    "    learning_rate: float = 5e-6\n",
    "    weight_decay: float = 0.01\n",
    "    batch_size: int = 4\n",
    "    num_epochs: int = 1\n",
    "    eval_batch_size: int = 8\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f1d1e",
   "metadata": {},
   "source": [
    "### Storage for rollouts\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e49b74b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PPORollout:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    reward: float\n",
    "    logprobs: torch.Tensor\n",
    "    ref_logprobs: torch.Tensor\n",
    "    values: torch.Tensor\n",
    "    masks: torch.BoolTensor\n",
    "\n",
    "\n",
    "def stack_rollouts(rollouts: List[PPORollout]) -> Dict[str, torch.Tensor]:\n",
    "    return {\n",
    "        'logprobs': torch.stack([r.logprobs for r in rollouts]),\n",
    "        'ref_logprobs': torch.stack([r.ref_logprobs for r in rollouts]),\n",
    "        'values': torch.stack([r.values for r in rollouts]),\n",
    "        'masks': torch.stack([r.masks for r in rollouts]),\n",
    "        'rewards': torch.tensor([r.reward for r in rollouts], dtype=torch.float),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283971c",
   "metadata": {},
   "source": [
    "### Policy and value helper functions\n",
    "\n",
    "We load the `shraygpt-instruct` checkpoint twice: once as the trainable policy and once as a frozen reference model that anchors the KL penalty. Log-probabilities are computed directly from the ShrayGPT logits so we stay within the custom architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e60d2311",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def load_shraygpt(checkpoint: Path) -> ShrayGPT:\n",
    "    if not checkpoint.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected checkpoint at {checkpoint}. Run notebook 07 to generate shraygpt-instruct.\"\n",
    "        )\n",
    "    model = ShrayGPT.load_from_checkpoint(str(checkpoint), map_location=DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_models(policy_checkpoint: Path = POLICY_CHECKPOINT, ref_checkpoint: Path = REF_CHECKPOINT) -> Tuple[ShrayGPT, ShrayGPT]:\n",
    "    policy = load_shraygpt(policy_checkpoint)\n",
    "    ref_model = load_shraygpt(ref_checkpoint)\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return policy, ref_model\n",
    "\n",
    "\n",
    "def compute_logprobs(model: ShrayGPT, batch: TokenizedBatch, detach: bool = True):\n",
    "    context = torch.no_grad() if detach else nullcontext()\n",
    "    input_ids = batch.input_ids.to(DEVICE)\n",
    "    with context:\n",
    "        logits, _, aux_loss = model(input_ids)\n",
    "        logits = logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        gathered = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    if detach:\n",
    "        return gathered.cpu()\n",
    "    return gathered, aux_loss\n",
    "\n",
    "\n",
    "def compute_values(model: ShrayGPT, batch: TokenizedBatch) -> torch.Tensor:\n",
    "    logprobs = compute_logprobs(model, batch, detach=True)\n",
    "    mask = ((~batch.prompt_mask[:, 1:]) & batch.attention_mask[:, 1:].bool()).float()\n",
    "    token_counts = mask.sum(dim=-1, keepdim=True).clamp(min=1)\n",
    "    return ((logprobs * mask).sum(dim=-1, keepdim=True) / token_counts).cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca85db",
   "metadata": {},
   "source": [
    "### Reward shaping for preference data\n\n",
    "The HH-RLHF pairs tell us which assistant reply humans preferred. We keep a lightweight lexical baseline for quick checks, but rely on a learned reward model for PPO training.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3feed824",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def lexical_overlap(a: str, b: str) -> float:\n",
    "    tokens_a = set(a.lower().split())\n",
    "    tokens_b = set(b.lower().split())\n",
    "    if not tokens_a:\n",
    "        return 0.0\n",
    "    return len(tokens_a & tokens_b) / len(tokens_a)\n",
    "\n",
    "\n",
    "def anthropic_preference_reward(example: Dict[str, str], response: str) -> float:\n",
    "    response_text = response.strip()\n",
    "    chosen_overlap = lexical_overlap(response_text, example['chosen_response'])\n",
    "    rejected_overlap = lexical_overlap(response_text, example['rejected_response'])\n",
    "    length_penalty = min(\n",
    "        abs(len(response_text) - len(example['chosen_response'])) / max(len(example['chosen_response']), 1),\n",
    "        1.0,\n",
    "    )\n",
    "    return chosen_overlap - rejected_overlap - 0.2 * length_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a learned reward model\n\nInstead of relying on lexical overlap, we fine-tune a separate reward head on top of the `shraygpt-instruct` backbone using the Anthropic preference pairs. The model predicts a scalar reward for each conversation, and we optimize a Bradley\u2013Terry style objective so chosen completions score higher than rejected ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceCollator:\n",
    "    def __call__(self, examples: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        chosen_texts = [ex['chosen'] for ex in examples]\n",
    "        rejected_texts = [ex['rejected'] for ex in examples]\n",
    "        chosen_ids, chosen_mask = tokenize_texts(chosen_texts)\n",
    "        rejected_ids, rejected_mask = tokenize_texts(rejected_texts)\n",
    "        return {\n",
    "            'chosen_input_ids': chosen_ids,\n",
    "            'chosen_attention_mask': chosen_mask,\n",
    "            'rejected_input_ids': rejected_ids,\n",
    "            'rejected_attention_mask': rejected_mask,\n",
    "        }\n",
    "\n",
    "\n",
    "class ShrayGPTRewardModel(nn.Module):\n",
    "    def __init__(self, base_model: ShrayGPT):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.value_head = nn.Linear(self.base.hparams.d_model, 1)\n",
    "        nn.init.zeros_(self.value_head.weight)\n",
    "        self.aux_loss_weight = float(getattr(self.base.hparams, 'aux_loss_weight', 0.0))\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.LongTensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.base.tok_emb(input_ids.to(DEVICE))\n",
    "        x = self.base.dropout(x)\n",
    "        aux_losses: List[torch.Tensor] = []\n",
    "        for layer in self.base.layers:\n",
    "            x, _, aux_loss = layer(x, kv_cache=None, start_pos=0)\n",
    "            aux_losses.append(aux_loss)\n",
    "        x = self.base.ln_f(x)\n",
    "        rewards = self.value_head(x).squeeze(-1)\n",
    "        mask = attention_mask.to(DEVICE).float()\n",
    "        token_counts = mask.sum(dim=-1).clamp(min=1.0)\n",
    "        rewards = (rewards * mask).sum(dim=-1) / token_counts\n",
    "        aux_loss = torch.stack(aux_losses).mean() if aux_losses else torch.tensor(0.0, device=DEVICE)\n",
    "        return rewards, aux_loss\n",
    "\n",
    "\n",
    "def preference_loss(chosen_rewards: torch.Tensor, rejected_rewards: torch.Tensor) -> torch.Tensor:\n",
    "    return -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "\n",
    "\n",
    "def build_reward_model(checkpoint: Path = POLICY_CHECKPOINT) -> ShrayGPTRewardModel:\n",
    "    base = load_shraygpt(checkpoint)\n",
    "    base.train()\n",
    "    reward_model = ShrayGPTRewardModel(base)\n",
    "    reward_model.to(DEVICE)\n",
    "    return reward_model\n",
    "\n",
    "\n",
    "def train_reward_model(dataset: Dataset, config: RewardConfig, eval_dataset: Optional[Dataset] = None) -> Tuple[ShrayGPTRewardModel, List[Dict[str, float]]]:\n",
    "    reward_model = build_reward_model()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        reward_model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(0.9, 0.95),\n",
    "        weight_decay=config.weight_decay,\n",
    "    )\n",
    "    collator = PreferenceCollator()\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collator)\n",
    "    history: List[Dict[str, float]] = []\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        reward_model.train()\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        steps = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            chosen_ids = batch['chosen_input_ids'].to(DEVICE)\n",
    "            chosen_mask = batch['chosen_attention_mask'].to(DEVICE)\n",
    "            rejected_ids = batch['rejected_input_ids'].to(DEVICE)\n",
    "            rejected_mask = batch['rejected_attention_mask'].to(DEVICE)\n",
    "\n",
    "            chosen_scores, chosen_aux = reward_model(chosen_ids, chosen_mask)\n",
    "            rejected_scores, rejected_aux = reward_model(rejected_ids, rejected_mask)\n",
    "            loss = preference_loss(chosen_scores, rejected_scores)\n",
    "            if reward_model.aux_loss_weight > 0:\n",
    "                aux_term = 0.5 * (chosen_aux + rejected_aux)\n",
    "                loss = loss + reward_model.aux_loss_weight * aux_term\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(reward_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = (chosen_scores > rejected_scores).float().mean().item()\n",
    "            batch_size = chosen_scores.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_acc += acc * batch_size\n",
    "            steps += batch_size\n",
    "\n",
    "        metrics: Dict[str, float] = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': total_loss / max(steps, 1),\n",
    "            'train_accuracy': total_acc / max(steps, 1),\n",
    "        }\n",
    "        if eval_dataset is not None:\n",
    "            metrics.update({\n",
    "                'val_' + k: v for k, v in evaluate_reward_model(reward_model, eval_dataset, config).items()\n",
    "            })\n",
    "        history.append(metrics)\n",
    "        print(f'Epoch {epoch}: {metrics}')\n",
    "\n",
    "    reward_model.eval()\n",
    "    return reward_model, history\n",
    "\n",
    "\n",
    "def evaluate_reward_model(reward_model: ShrayGPTRewardModel, dataset: Dataset, config: RewardConfig) -> Dict[str, float]:\n",
    "    was_training = reward_model.training\n",
    "    reward_model.eval()\n",
    "    collator = PreferenceCollator()\n",
    "    dataloader = DataLoader(dataset, batch_size=config.eval_batch_size, shuffle=False, collate_fn=collator)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            chosen_ids = batch['chosen_input_ids'].to(DEVICE)\n",
    "            chosen_mask = batch['chosen_attention_mask'].to(DEVICE)\n",
    "            rejected_ids = batch['rejected_input_ids'].to(DEVICE)\n",
    "            rejected_mask = batch['rejected_attention_mask'].to(DEVICE)\n",
    "            chosen_scores, _ = reward_model(chosen_ids, chosen_mask)\n",
    "            rejected_scores, _ = reward_model(rejected_ids, rejected_mask)\n",
    "            loss = preference_loss(chosen_scores, rejected_scores)\n",
    "            acc = (chosen_scores > rejected_scores).float().mean().item()\n",
    "            batch_size = chosen_scores.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_acc += acc * batch_size\n",
    "            total += batch_size\n",
    "    if was_training:\n",
    "        reward_model.train()\n",
    "    return {\n",
    "        'loss': total_loss / max(total, 1),\n",
    "        'accuracy': total_acc / max(total, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def score_with_reward_model(reward_model: ShrayGPTRewardModel, prompt: str, response: str) -> float:\n",
    "    batch = tokenize_batch([prompt], [response])\n",
    "    input_ids = batch.input_ids.to(DEVICE)\n",
    "    attention_mask = batch.attention_mask.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        rewards, _ = reward_model(input_ids, attention_mask)\n",
    "    return float(rewards[0].cpu())\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_config = RewardConfig()\n",
    "reward_model, reward_history = train_reward_model(reward_train_dataset, reward_config, reward_eval_dataset)\n",
    "reward_history[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf8a12",
   "metadata": {},
   "source": [
    "### Collecting PPO rollouts\n\n",
    "Each rollout samples a fresh assistant reply, logs policy/reference log-probabilities, and scores the response with the learned reward model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b9d066e",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(model: ShrayGPT, prompt: str, config: PPOConfig) -> str:\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    context = torch.tensor(prompt_tokens, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "    generated = model.generate_nocache(\n",
    "        context,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "    )\n",
    "    new_tokens = generated[0].tolist()[len(prompt_tokens):]\n",
    "    return tokenizer.decode(new_tokens)\n",
    "\n",
    "\n",
    "def collect_rollouts(policy: ShrayGPT, ref_model: ShrayGPT, reward_model: ShrayGPTRewardModel, dataset: Dataset, config: PPOConfig, num_rollouts: int) -> List[PPORollout]:\n",
    "    rollouts: List[PPORollout] = []\n",
    "    was_training = policy.training\n",
    "    policy.eval()\n",
    "\n",
    "    for example in dataset.shuffle(seed=42).select(range(num_rollouts * 2)):\n",
    "        prompt = example['prompt']\n",
    "        response = generate_response(policy, prompt, config)\n",
    "        tokenized = tokenize_batch([prompt], [response])\n",
    "        mask = ((~tokenized.prompt_mask[:, 1:]) & tokenized.attention_mask[:, 1:].bool())[0]\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        logprobs = compute_logprobs(policy, tokenized, detach=True)[0]\n",
    "        ref_logprobs = compute_logprobs(ref_model, tokenized, detach=True)[0]\n",
    "        values = compute_values(policy, tokenized)[0]\n",
    "        reward = score_with_reward_model(reward_model, prompt, response)\n",
    "\n",
    "        rollout = PPORollout(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            reward=reward,\n",
    "            logprobs=logprobs,\n",
    "            ref_logprobs=ref_logprobs,\n",
    "            values=values,\n",
    "            masks=mask,\n",
    "        )\n",
    "        rollouts.append(rollout)\n",
    "        if len(rollouts) >= num_rollouts:\n",
    "            break\n",
    "\n",
    "    if was_training:\n",
    "        policy.train()\n",
    "    return rollouts\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c4572",
   "metadata": {},
   "source": [
    "### PPO loss computation\n",
    "\n",
    "We normalize per-rollout advantages, apply the clipped surrogate objective, and keep ShrayGPT's mixture-of-experts auxiliary loss as a small regularizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa4f1f16",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def ppo_loss(policy: ShrayGPT, rollouts: List[PPORollout], config: PPOConfig) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    stacked = stack_rollouts(rollouts)\n",
    "    rewards = stacked['rewards'].unsqueeze(-1).to(DEVICE)\n",
    "    values = stacked['values'].to(DEVICE)\n",
    "    advantages = rewards - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
    "\n",
    "    batch = tokenize_batch(\n",
    "        [r.prompt for r in rollouts],\n",
    "        [r.response for r in rollouts],\n",
    "    )\n",
    "    new_logprobs, aux_loss = compute_logprobs(policy, batch, detach=False)\n",
    "\n",
    "    mask = ((~batch.prompt_mask[:, 1:]) & batch.attention_mask[:, 1:].bool()).to(DEVICE).float()\n",
    "    mask_sum = mask.sum().clamp(min=1.0)\n",
    "\n",
    "    old_logprobs = stacked['logprobs'].to(DEVICE)\n",
    "    ref_logprobs = stacked['ref_logprobs'].to(DEVICE)\n",
    "\n",
    "    logratio = new_logprobs - old_logprobs\n",
    "    ratio = logratio.exp()\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range) * advantages\n",
    "    policy_loss = -(torch.min(unclipped, clipped) * mask).sum() / mask_sum\n",
    "\n",
    "    kl = ((old_logprobs - ref_logprobs) * mask).sum() / mask_sum\n",
    "    entropy = -(ratio * logratio * mask).sum() / mask_sum\n",
    "    value_loss = F.mse_loss(values.squeeze(-1), rewards.squeeze(-1))\n",
    "\n",
    "    aux_weight = float(getattr(getattr(policy, 'hparams', {}), 'aux_loss_weight', 0.0))\n",
    "    if isinstance(aux_loss, torch.Tensor):\n",
    "        aux_term = aux_loss.mean()\n",
    "    else:\n",
    "        aux_term = torch.tensor(aux_loss, device=DEVICE)\n",
    "\n",
    "    total_loss = policy_loss + config.vf_coef * value_loss - config.ent_coef * entropy + config.kl_coef * kl\n",
    "    if aux_weight > 0:\n",
    "        total_loss = total_loss + aux_weight * aux_term\n",
    "\n",
    "    metrics = {\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'kl': kl.item(),\n",
    "        'entropy': entropy.item(),\n",
    "        'aux_loss': aux_term.item(),\n",
    "    }\n",
    "    return total_loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52805f82",
   "metadata": {},
   "source": [
    "### PPO training loop\n",
    "\n",
    "We reuse a modest learning rate and gradient clipping for stability. The reference model remains frozen, and we stop early if the KL term grows too large.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9909a39d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_ppo(dataset: Dataset, reward_model: ShrayGPTRewardModel, config: PPOConfig) -> Tuple[ShrayGPT, List[Dict[str, float]]]:\n",
    "    policy, ref_model = build_models()\n",
    "    for param in policy.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(policy.parameters(), lr=config.learning_rate, betas=(0.9, 0.95), weight_decay=1e-4)\n",
    "\n",
    "    history: List[Dict[str, float]] = []\n",
    "    for epoch in range(config.num_epochs):\n",
    "        rollouts = collect_rollouts(policy, ref_model, reward_model, dataset, config, config.batch_size)\n",
    "        if not rollouts:\n",
    "            raise RuntimeError('No valid rollouts collected \u2014 try increasing max_new_tokens or batch size.')\n",
    "\n",
    "        policy.train()\n",
    "        loss, metrics = ppo_loss(policy, rollouts, config)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics['loss'] = loss.item()\n",
    "        metrics['epoch'] = epoch\n",
    "        history.append(metrics)\n",
    "        print(f\"Epoch {epoch}: {metrics}\")\n",
    "\n",
    "        if metrics['kl'] > config.target_kl:\n",
    "            print(f\"Stopping early because KL={metrics['kl']:.3f} exceeded the target {config.target_kl}.\")\n",
    "            break\n",
    "\n",
    "    policy.eval()\n",
    "    return policy, history\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a1816",
   "metadata": {},
   "source": [
    "Even a single PPO epoch is computationally demanding because it needs to generate fresh samples. For larger experiments you would:\n",
    "\n",
    "* Increase rollout counts and gradient accumulation to reduce the variance of the policy gradient.\n",
    "* Replace the lexical reward with a reward model trained on the `chosen` vs. `rejected` responses.\n",
    "* Monitor the KL divergence to adjust `kl_coef` adaptively and avoid drifting too far from the SFT policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef9ace",
   "metadata": {},
   "source": [
    "## Evaluation and verification\n",
    "\n",
    "We can compare the PPO-tuned model against the SFT checkpoint by sampling held-out prompts and scoring them with the same preference heuristic. Positive rewards indicate closer alignment with the human-preferred replies.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0356a16a",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model: ShrayGPT, reward_model: ShrayGPTRewardModel, dataset: Dataset, config: PPOConfig, num_examples: int = 32) -> Dict[str, float]:\n",
    "    subset = dataset.shuffle(seed=0).select(range(num_examples))\n",
    "    rewards: List[float] = []\n",
    "    model.eval()\n",
    "    for example in subset:\n",
    "        response = generate_response(model, example['prompt'], config)\n",
    "        rewards.append(score_with_reward_model(reward_model, example['prompt'], response))\n",
    "    avg_reward = sum(rewards) / len(rewards)\n",
    "    success_rate = sum(r > 0 for r in rewards) / len(rewards)\n",
    "    return {\n",
    "        'avg_reward': avg_reward,\n",
    "        'success_rate': success_rate,\n",
    "    }\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff8e052",
   "metadata": {},
   "source": [
    "## Next steps\n\n",
    "* Scale the reward-model fine-tuning to the full dataset with gradient accumulation and monitor for overfitting.\n",
    "* Incorporate KL penalties or early stopping schedules that adapt to the learned reward to preserve alignment with the reference model.\n",
    "* Evaluate PPO checkpoints against a held-out set of human preference comparisons or external benchmarks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}