{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2283aa4",
   "metadata": {},
   "source": [
    "# 08 Â· Reinforcement Learning from Human Feedback for ShrayGPT\n",
    "\n",
    "This notebook shows how to continue fine-tuning the `shraygpt-instruct` checkpoint with Proximal Policy Optimization (PPO) using preference data from the Anthropic RLHF corpus. We replace the demo Hugging Face model from earlier versions so the reinforcement learning stage directly follows the supervised fine-tuning work from notebook 07.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8b9e5",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "* Prepare the Anthropic human preference dataset so it matches the conversation format used during ShrayGPT's supervised fine-tuning.\n",
    "* Build a PPO loop that optimizes the `shraygpt-instruct` policy while keeping a frozen reference copy for KL regularization.\n",
    "* Shape lexical rewards from the preference pairs and monitor key PPO diagnostics such as KL and entropy.\n",
    "* Evaluate how the RLHF-tuned policy behaves compared with the SFT baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84633d",
   "metadata": {},
   "source": [
    "> **Note:** This walkthrough keeps the computation light so it can run on commodity hardware. Real PPO runs for ShrayGPT require more rollouts, gradient accumulation, and likely mixed-precision training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd2690",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "We rely on the local ShrayGPT checkpoint, `tiktoken` for byte-pair encoding, the Hugging Face datasets library for loading Anthropic preferences, and PyTorch for optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "91f7278b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from src.shraygpt import ShrayGPT\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CHECKPOINT_DIR = Path('checkpoints')\n",
    "POLICY_CHECKPOINT = CHECKPOINT_DIR / 'shraygpt-instruct.ckpt'\n",
    "REF_CHECKPOINT = CHECKPOINT_DIR / 'shraygpt-instruct.ckpt'\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57c403",
   "metadata": {},
   "source": [
    "### Loading Anthropic preference pairs\n",
    "\n",
    "The Anthropic HH-RLHF dataset contains multi-turn conversations with preferred (`chosen`) and dispreferred (`rejected`) assistant replies. Each transcript already alternates `Human:` and `Assistant:` turns, so we recover the shared prefix as the prompt and keep the diverging final assistant messages as responses. This keeps the PPO prompts consistent with the conversation format used during ShrayGPT's supervised fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7ebe343",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROLE_PREFIXES = {\n",
    "    'system': 'System:',\n",
    "    'user': 'User:',\n",
    "    'assistant': 'Assistant:',\n",
    "}\n",
    "DEFAULT_SYSTEM_PROMPT = 'You are ShrayGPT, a helpful and concise AI assistant.'\n",
    "\n",
    "\n",
    "def format_conversation(messages: Sequence[Tuple[str, str]], default_system: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    formatted: List[str] = []\n",
    "    has_system = any(role == 'system' for role, _ in messages)\n",
    "    if not has_system:\n",
    "        formatted.append(f\"{ROLE_PREFIXES['system']} {default_system.strip()}\")\n",
    "\n",
    "    for role, content in messages:\n",
    "        prefix = ROLE_PREFIXES.get(role)\n",
    "        if prefix is None:\n",
    "            continue\n",
    "        formatted.append(f\"{prefix} {content.strip()}\")\n",
    "\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def parse_anthropic_conversation(transcript: str) -> List[Tuple[str, str]]:\n",
    "    turns: List[Tuple[str, str]] = []\n",
    "    for block in transcript.strip().split(\"\\n\\n\"):\n",
    "        block = block.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "        if block.startswith('Human:'):\n",
    "            turns.append(('user', block[len('Human:'):].strip()))\n",
    "        elif block.startswith('Assistant:'):\n",
    "            content = block[len('Assistant:'):].strip()\n",
    "            if content:\n",
    "                turns.append(('assistant', content))\n",
    "    return turns\n",
    "\n",
    "\n",
    "def shared_prefix(chosen_turns: Sequence[Tuple[str, str]], rejected_turns: Sequence[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    prefix: List[Tuple[str, str]] = []\n",
    "    for (c_role, c_text), (r_role, r_text) in zip(chosen_turns, rejected_turns):\n",
    "        if c_role == r_role and c_text == r_text:\n",
    "            prefix.append((c_role, c_text))\n",
    "        else:\n",
    "            break\n",
    "    return prefix\n",
    "\n",
    "\n",
    "def extract_assistant_response(turns: Sequence[Tuple[str, str]], start: int) -> str:\n",
    "    for role, text in turns[start:]:\n",
    "        if role == 'assistant':\n",
    "            return text\n",
    "    for role, text in reversed(turns):\n",
    "        if role == 'assistant':\n",
    "            return text\n",
    "    return ''\n",
    "\n",
    "\n",
    "def prepare_anthropic(example: Dict[str, str]) -> Dict[str, str]:\n",
    "    chosen_turns = parse_anthropic_conversation(example['chosen'])\n",
    "    rejected_turns = parse_anthropic_conversation(example['rejected'])\n",
    "\n",
    "    prefix_turns = shared_prefix(chosen_turns, rejected_turns)\n",
    "    prompt_turns = [('system', DEFAULT_SYSTEM_PROMPT)] + prefix_turns\n",
    "\n",
    "    chosen_response = extract_assistant_response(chosen_turns, len(prefix_turns))\n",
    "    rejected_response = extract_assistant_response(rejected_turns, len(prefix_turns))\n",
    "\n",
    "    prompt_messages = prompt_turns + [('assistant', '')]\n",
    "    chosen_messages = prompt_turns + [('assistant', chosen_response)]\n",
    "    rejected_messages = prompt_turns + [('assistant', rejected_response)]\n",
    "\n",
    "    return {\n",
    "        'prompt': format_conversation(prompt_messages),\n",
    "        'chosen': format_conversation(chosen_messages),\n",
    "        'rejected': format_conversation(rejected_messages),\n",
    "        'chosen_response': chosen_response,\n",
    "        'rejected_response': rejected_response,\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = load_dataset('Anthropic/hh-rlhf', split='train')\n",
    "dataset = dataset.map(prepare_anthropic, remove_columns=dataset.column_names)\n",
    "small_dataset = dataset.shuffle(seed=42).select(range(256))\n",
    "small_dataset[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2936f3",
   "metadata": {},
   "source": [
    "### Tokenization utilities\n",
    "\n",
    "We reuse the `tiktoken` vocabulary from pre-training and supervised fine-tuning. The prompt mask tracks the prefix tokens so PPO only updates the generated assistant continuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a207b60d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = tiktoken.get_encoding('r50k_base')\n",
    "PAD_TOKEN_ID = tokenizer.eot_token\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TokenizedBatch:\n",
    "    input_ids: torch.LongTensor\n",
    "    attention_mask: torch.LongTensor\n",
    "    prompt_mask: torch.BoolTensor\n",
    "\n",
    "\n",
    "def tokenize_batch(prompts: List[str], responses: List[str]) -> TokenizedBatch:\n",
    "    prompt_tokens = [tokenizer.encode(p) for p in prompts]\n",
    "    response_tokens = [tokenizer.encode(r) for r in responses]\n",
    "    combined = [p + r for p, r in zip(prompt_tokens, response_tokens)]\n",
    "    max_len = max(len(seq) for seq in combined)\n",
    "\n",
    "    input_ids = torch.full((len(combined), max_len), PAD_TOKEN_ID, dtype=torch.long)\n",
    "    attention_mask = torch.zeros_like(input_ids, dtype=torch.long)\n",
    "    prompt_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    for i, (p_tokens, r_tokens) in enumerate(zip(prompt_tokens, response_tokens)):\n",
    "        sequence = p_tokens + r_tokens\n",
    "        seq_len = len(sequence)\n",
    "        if seq_len == 0:\n",
    "            continue\n",
    "        input_ids[i, :seq_len] = torch.tensor(sequence, dtype=torch.long)\n",
    "        attention_mask[i, :seq_len] = 1\n",
    "        prompt_mask[i, :len(p_tokens)] = True\n",
    "\n",
    "    return TokenizedBatch(input_ids=input_ids, attention_mask=attention_mask, prompt_mask=prompt_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d8989",
   "metadata": {},
   "source": [
    "## Implementing PPO\n",
    "\n",
    "PPO optimizes a policy by constraining updates to remain close to the previous policy through a clipped surrogate loss. For language models we operate in log-probability space and mask the prompt tokens because they are provided by the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d5e29",
   "metadata": {},
   "source": [
    "### Configuration dataclasses\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3244c386",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    kl_coef: float = 0.1\n",
    "    clip_range: float = 0.2\n",
    "    vf_coef: float = 0.1\n",
    "    ent_coef: float = 0.01\n",
    "    target_kl: float = 0.1\n",
    "    num_epochs: int = 1\n",
    "    batch_size: int = 4\n",
    "    mini_batch_size: int = 2\n",
    "    max_new_tokens: int = 128\n",
    "    learning_rate: float = 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f1d1e",
   "metadata": {},
   "source": [
    "### Storage for rollouts\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e49b74b",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PPORollout:\n",
    "    prompt: str\n",
    "    response: str\n",
    "    reward: float\n",
    "    logprobs: torch.Tensor\n",
    "    ref_logprobs: torch.Tensor\n",
    "    values: torch.Tensor\n",
    "    masks: torch.BoolTensor\n",
    "\n",
    "\n",
    "def stack_rollouts(rollouts: List[PPORollout]) -> Dict[str, torch.Tensor]:\n",
    "    return {\n",
    "        'logprobs': torch.stack([r.logprobs for r in rollouts]),\n",
    "        'ref_logprobs': torch.stack([r.ref_logprobs for r in rollouts]),\n",
    "        'values': torch.stack([r.values for r in rollouts]),\n",
    "        'masks': torch.stack([r.masks for r in rollouts]),\n",
    "        'rewards': torch.tensor([r.reward for r in rollouts], dtype=torch.float),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283971c",
   "metadata": {},
   "source": [
    "### Policy and value helper functions\n",
    "\n",
    "We load the `shraygpt-instruct` checkpoint twice: once as the trainable policy and once as a frozen reference model that anchors the KL penalty. Log-probabilities are computed directly from the ShrayGPT logits so we stay within the custom architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e60d2311",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def load_shraygpt(checkpoint: Path) -> ShrayGPT:\n",
    "    if not checkpoint.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected checkpoint at {checkpoint}. Run notebook 07 to generate shraygpt-instruct.\"\n",
    "        )\n",
    "    model = ShrayGPT.load_from_checkpoint(str(checkpoint), map_location=DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_models(policy_checkpoint: Path = POLICY_CHECKPOINT, ref_checkpoint: Path = REF_CHECKPOINT) -> Tuple[ShrayGPT, ShrayGPT]:\n",
    "    policy = load_shraygpt(policy_checkpoint)\n",
    "    ref_model = load_shraygpt(ref_checkpoint)\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return policy, ref_model\n",
    "\n",
    "\n",
    "def compute_logprobs(model: ShrayGPT, batch: TokenizedBatch, detach: bool = True):\n",
    "    context = torch.no_grad() if detach else nullcontext()\n",
    "    input_ids = batch.input_ids.to(DEVICE)\n",
    "    with context:\n",
    "        logits, _, aux_loss = model(input_ids)\n",
    "        logits = logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        gathered = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    if detach:\n",
    "        return gathered.cpu()\n",
    "    return gathered, aux_loss\n",
    "\n",
    "\n",
    "def compute_values(model: ShrayGPT, batch: TokenizedBatch) -> torch.Tensor:\n",
    "    logprobs = compute_logprobs(model, batch, detach=True)\n",
    "    mask = ((~batch.prompt_mask[:, 1:]) & batch.attention_mask[:, 1:].bool()).float()\n",
    "    token_counts = mask.sum(dim=-1, keepdim=True).clamp(min=1)\n",
    "    return ((logprobs * mask).sum(dim=-1, keepdim=True) / token_counts).cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca85db",
   "metadata": {},
   "source": [
    "### Reward shaping for preference data\n",
    "\n",
    "The HH-RLHF pairs tell us which assistant reply humans preferred. We approximate a reward model with a lightweight lexical heuristic that boosts overlap with the preferred response and penalizes similarities to the rejected one.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3feed824",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def lexical_overlap(a: str, b: str) -> float:\n",
    "    tokens_a = set(a.lower().split())\n",
    "    tokens_b = set(b.lower().split())\n",
    "    if not tokens_a:\n",
    "        return 0.0\n",
    "    return len(tokens_a & tokens_b) / len(tokens_a)\n",
    "\n",
    "\n",
    "def anthropic_preference_reward(example: Dict[str, str], response: str) -> float:\n",
    "    response_text = response.strip()\n",
    "    chosen_overlap = lexical_overlap(response_text, example['chosen_response'])\n",
    "    rejected_overlap = lexical_overlap(response_text, example['rejected_response'])\n",
    "    length_penalty = min(\n",
    "        abs(len(response_text) - len(example['chosen_response'])) / max(len(example['chosen_response']), 1),\n",
    "        1.0,\n",
    "    )\n",
    "    return chosen_overlap - rejected_overlap - 0.2 * length_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf8a12",
   "metadata": {},
   "source": [
    "### Collecting PPO rollouts\n",
    "\n",
    "Each rollout samples a fresh assistant reply, logs policy/reference log-probabilities, and scores the response with the reward heuristic derived from the preference pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7b9d066e",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(model: ShrayGPT, prompt: str, config: PPOConfig) -> str:\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    context = torch.tensor(prompt_tokens, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "    generated = model.generate_nocache(\n",
    "        context,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "    )\n",
    "    new_tokens = generated[0].tolist()[len(prompt_tokens):]\n",
    "    return tokenizer.decode(new_tokens)\n",
    "\n",
    "\n",
    "def collect_rollouts(policy: ShrayGPT, ref_model: ShrayGPT, dataset: Dataset, config: PPOConfig, num_rollouts: int) -> List[PPORollout]:\n",
    "    rollouts: List[PPORollout] = []\n",
    "    was_training = policy.training\n",
    "    policy.eval()\n",
    "\n",
    "    for example in dataset.shuffle(seed=42).select(range(num_rollouts * 2)):\n",
    "        prompt = example['prompt']\n",
    "        response = generate_response(policy, prompt, config)\n",
    "        tokenized = tokenize_batch([prompt], [response])\n",
    "        mask = ((~tokenized.prompt_mask[:, 1:]) & tokenized.attention_mask[:, 1:].bool())[0]\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        logprobs = compute_logprobs(policy, tokenized, detach=True)[0]\n",
    "        ref_logprobs = compute_logprobs(ref_model, tokenized, detach=True)[0]\n",
    "        values = compute_values(policy, tokenized)[0]\n",
    "        reward = anthropic_preference_reward(example, response)\n",
    "\n",
    "        rollout = PPORollout(\n",
    "            prompt=prompt,\n",
    "            response=response,\n",
    "            reward=reward,\n",
    "            logprobs=logprobs,\n",
    "            ref_logprobs=ref_logprobs,\n",
    "            values=values,\n",
    "            masks=mask,\n",
    "        )\n",
    "        rollouts.append(rollout)\n",
    "        if len(rollouts) >= num_rollouts:\n",
    "            break\n",
    "\n",
    "    if was_training:\n",
    "        policy.train()\n",
    "    return rollouts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c4572",
   "metadata": {},
   "source": [
    "### PPO loss computation\n",
    "\n",
    "We normalize per-rollout advantages, apply the clipped surrogate objective, and keep ShrayGPT's mixture-of-experts auxiliary loss as a small regularizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aa4f1f16",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def ppo_loss(policy: ShrayGPT, rollouts: List[PPORollout], config: PPOConfig) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    stacked = stack_rollouts(rollouts)\n",
    "    rewards = stacked['rewards'].unsqueeze(-1).to(DEVICE)\n",
    "    values = stacked['values'].to(DEVICE)\n",
    "    advantages = rewards - values\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
    "\n",
    "    batch = tokenize_batch(\n",
    "        [r.prompt for r in rollouts],\n",
    "        [r.response for r in rollouts],\n",
    "    )\n",
    "    new_logprobs, aux_loss = compute_logprobs(policy, batch, detach=False)\n",
    "\n",
    "    mask = ((~batch.prompt_mask[:, 1:]) & batch.attention_mask[:, 1:].bool()).to(DEVICE).float()\n",
    "    mask_sum = mask.sum().clamp(min=1.0)\n",
    "\n",
    "    old_logprobs = stacked['logprobs'].to(DEVICE)\n",
    "    ref_logprobs = stacked['ref_logprobs'].to(DEVICE)\n",
    "\n",
    "    logratio = new_logprobs - old_logprobs\n",
    "    ratio = logratio.exp()\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range) * advantages\n",
    "    policy_loss = -(torch.min(unclipped, clipped) * mask).sum() / mask_sum\n",
    "\n",
    "    kl = ((old_logprobs - ref_logprobs) * mask).sum() / mask_sum\n",
    "    entropy = -(ratio * logratio * mask).sum() / mask_sum\n",
    "    value_loss = F.mse_loss(values.squeeze(-1), rewards.squeeze(-1))\n",
    "\n",
    "    aux_weight = float(getattr(getattr(policy, 'hparams', {}), 'aux_loss_weight', 0.0))\n",
    "    if isinstance(aux_loss, torch.Tensor):\n",
    "        aux_term = aux_loss.mean()\n",
    "    else:\n",
    "        aux_term = torch.tensor(aux_loss, device=DEVICE)\n",
    "\n",
    "    total_loss = policy_loss + config.vf_coef * value_loss - config.ent_coef * entropy + config.kl_coef * kl\n",
    "    if aux_weight > 0:\n",
    "        total_loss = total_loss + aux_weight * aux_term\n",
    "\n",
    "    metrics = {\n",
    "        'policy_loss': policy_loss.item(),\n",
    "        'value_loss': value_loss.item(),\n",
    "        'kl': kl.item(),\n",
    "        'entropy': entropy.item(),\n",
    "        'aux_loss': aux_term.item(),\n",
    "    }\n",
    "    return total_loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52805f82",
   "metadata": {},
   "source": [
    "### PPO training loop\n",
    "\n",
    "We reuse a modest learning rate and gradient clipping for stability. The reference model remains frozen, and we stop early if the KL term grows too large.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9909a39d",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_ppo(dataset: Dataset, config: PPOConfig) -> Tuple[ShrayGPT, List[Dict[str, float]]]:\n",
    "    policy, ref_model = build_models()\n",
    "    for param in policy.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = torch.optim.AdamW(policy.parameters(), lr=config.learning_rate, betas=(0.9, 0.95), weight_decay=1e-4)\n",
    "\n",
    "    history: List[Dict[str, float]] = []\n",
    "    for epoch in range(config.num_epochs):\n",
    "        rollouts = collect_rollouts(policy, ref_model, dataset, config, config.batch_size)\n",
    "        if not rollouts:\n",
    "            raise RuntimeError('No valid rollouts collected â try increasing max_new_tokens or batch size.')\n",
    "\n",
    "        policy.train()\n",
    "        loss, metrics = ppo_loss(policy, rollouts, config)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics['loss'] = loss.item()\n",
    "        metrics['epoch'] = epoch\n",
    "        history.append(metrics)\n",
    "        print(f\"Epoch {epoch}: {metrics}\")\n",
    "\n",
    "        if metrics['kl'] > config.target_kl:\n",
    "            print(f\"Stopping early because KL={metrics['kl']:.3f} exceeded the target {config.target_kl}.\")\n",
    "            break\n",
    "\n",
    "    policy.eval()\n",
    "    return policy, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a1816",
   "metadata": {},
   "source": [
    "Even a single PPO epoch is computationally demanding because it needs to generate fresh samples. For larger experiments you would:\n",
    "\n",
    "* Increase rollout counts and gradient accumulation to reduce the variance of the policy gradient.\n",
    "* Replace the lexical reward with a reward model trained on the `chosen` vs. `rejected` responses.\n",
    "* Monitor the KL divergence to adjust `kl_coef` adaptively and avoid drifting too far from the SFT policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef9ace",
   "metadata": {},
   "source": [
    "## Evaluation and verification\n",
    "\n",
    "We can compare the PPO-tuned model against the SFT checkpoint by sampling held-out prompts and scoring them with the same preference heuristic. Positive rewards indicate closer alignment with the human-preferred replies.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0356a16a",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model: ShrayGPT, dataset: Dataset, config: PPOConfig, num_examples: int = 32) -> Dict[str, float]:\n",
    "    subset = dataset.shuffle(seed=0).select(range(num_examples))\n",
    "    rewards: List[float] = []\n",
    "    model.eval()\n",
    "    for example in subset:\n",
    "        response = generate_response(model, example['prompt'], config)\n",
    "        rewards.append(anthropic_preference_reward(example, response))\n",
    "    avg_reward = sum(rewards) / len(rewards)\n",
    "    success_rate = sum(r > 0 for r in rewards) / len(rewards)\n",
    "    return {\n",
    "        'avg_reward': avg_reward,\n",
    "        'success_rate': success_rate,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff8e052",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Train a dedicated reward model on the Anthropic preference pairs to replace the lexical heuristic.\n",
    "* Scale up rollout generation with distributed sampling and mixed precision to keep ShrayGPT updates stable.\n",
    "* Evaluate with human-in-the-loop preference checks or domain-specific metrics instead of the proxy reward.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}