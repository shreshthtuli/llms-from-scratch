{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f2283aa4",
      "metadata": {},
      "source": [
        "# 08. Reinforcement Learning from Human Feedback\n",
        "\n",
        "This notebook explores how to fine-tune large language models with reinforcement learning in domains where we can automatically verify model responses. We begin with **Proximal Policy Optimization (PPO)** and then extend the same ingredients to **Group Relative Preference Optimization (GRPO)** while working with the public [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) dataset of human preference pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d8b9e5",
      "metadata": {},
      "source": [
        "## Learning goals\n",
        "\n",
        "* Load and inspect the Anthropic human preference dataset.\n",
        "* Build a minimal pipeline for PPO that can optimize a policy model against automatically computed rewards.\n",
        "* Adapt the PPO implementation into GRPO to take advantage of grouped preference data.\n",
        "* Discuss evaluation considerations for verifiable domains, such as math or factual QA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e84633d",
      "metadata": {},
      "source": [
        "> **Note:** This notebook focuses on presenting a *reference implementation*. The code is intended to run on small models such as `distilgpt2` for demonstration, but you should expect to adjust batch sizes and precision if you train longer or on larger checkpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cd2690",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "\n",
        "We rely on the Hugging Face ecosystem for models and datasets, and on PyTorch for automatic differentiation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91f7278b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from functools import partial\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tiktoken\n",
        "\n",
        "from src.shraygpt import ShrayGPT\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {DEVICE}')\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"r50k_base\")\n",
        "\n",
        "model = ShrayGPT.load_from_checkpoint(\"checkpoints/shraygpt-instruct.ckpt\", map_location=\"cpu\")\n",
        "\n",
        "model.hparams.learning_rate_adamw = 1e-4\n",
        "model.hparams.learning_rate_muon = 5e-4\n",
        "model.hparams.aux_loss_weight = 5e-4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d57c403",
      "metadata": {},
      "source": [
        "### Loading Anthropic preference pairs\n",
        "\n",
        "The Anthropic HH-RLHF dataset contains paired responses (`chosen` vs. `rejected`) for multi-turn conversations. We'll downsample aggressively to keep the demo light-weight.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b7ebe343",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnthropic/hh-rlhf\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_pair\u001b[39m(example: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('Anthropic/hh-rlhf', split='train')\n",
        "print(dataset)\n",
        "\n",
        "def format_pair(example: Dict[str, str]) -> Dict[str, str]:\n",
        "    prompt = example['prompt'].strip()\n",
        "    chosen = example['chosen'].strip()\n",
        "    rejected = example['rejected'].strip()\n",
        "    return {\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen,\n",
        "        'rejected': rejected,\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format_pair)\n",
        "small_dataset = dataset.select(range(256))\n",
        "small_dataset[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2936f3",
      "metadata": {},
      "source": [
        "### Tokenization utilities\n",
        "\n",
        "We operate on concatenated prompt+response strings and keep track of the prompt length so we can mask out the log-probabilities that correspond to the prompt tokens during policy optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a207b60d",
      "metadata": {},
      "outputs": [],
      "source": [
        "TOKENIZER_NAME = 'distilgpt2'\n",
        "tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "@dataclass\n",
        "class TokenizedBatch:\n",
        "    input_ids: torch.LongTensor\n",
        "    attention_mask: torch.LongTensor\n",
        "    prompt_mask: torch.BoolTensor\n",
        "\n",
        "\n",
        "def tokenize_batch(prompts: List[str], responses: List[str]) -> TokenizedBatch:\n",
        "    combined = [prompts[i] + responses[i] for i in range(len(prompts))]\n",
        "    enc = tokenizer(combined, padding=True, return_tensors='pt')\n",
        "    prompt_lens = torch.tensor(\n",
        "        [len(tokenizer(p).input_ids) for p in prompts], dtype=torch.long\n",
        "    )\n",
        "    prompt_mask = torch.arange(enc.input_ids.size(1)).unsqueeze(0) < prompt_lens.unsqueeze(1)\n",
        "    return TokenizedBatch(\n",
        "        input_ids=enc.input_ids,\n",
        "        attention_mask=enc.attention_mask,\n",
        "        prompt_mask=prompt_mask,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265d8989",
      "metadata": {},
      "source": [
        "## Implementing PPO\n",
        "\n",
        "PPO optimizes a policy by constraining updates to remain close to the previous policy through a clipped surrogate loss. For language models we operate in log-probability space and mask the prompt tokens because they are provided by the environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469d5e29",
      "metadata": {},
      "source": [
        "### Configuration dataclasses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3244c386",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PPOConfig:\n",
        "    kl_coef: float = 0.1\n",
        "    clip_range: float = 0.2\n",
        "    vf_coef: float = 0.1\n",
        "    ent_coef: float = 0.01\n",
        "    target_kl: float = 0.1\n",
        "    num_epochs: int = 1\n",
        "    batch_size: int = 4\n",
        "    mini_batch_size: int = 2\n",
        "    max_new_tokens: int = 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36f1d1e",
      "metadata": {},
      "source": [
        "### Storage for rollouts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e49b74b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PPORollout:\n",
        "    prompt: str\n",
        "    response: str\n",
        "    reward: float\n",
        "    logprobs: torch.Tensor\n",
        "    ref_logprobs: torch.Tensor\n",
        "    values: torch.Tensor\n",
        "    masks: torch.BoolTensor\n",
        "\n",
        "\n",
        "def stack_rollouts(rollouts: List[PPORollout]) -> Dict[str, torch.Tensor]:\n",
        "    return {\n",
        "        'logprobs': torch.stack([r.logprobs for r in rollouts]),\n",
        "        'ref_logprobs': torch.stack([r.ref_logprobs for r in rollouts]),\n",
        "        'values': torch.stack([r.values for r in rollouts]),\n",
        "        'masks': torch.stack([r.masks for r in rollouts]),\n",
        "        'rewards': torch.tensor([r.reward for r in rollouts], dtype=torch.float),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d283971c",
      "metadata": {},
      "source": [
        "### Policy and value helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e60d2311",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_models(model_name: str = TOKENIZER_NAME) -> Tuple[PreTrainedModel, PreTrainedModel]:\n",
        "    policy = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n",
        "    return policy, ref_model\n",
        "\n",
        "\n",
        "def compute_logprobs(model: PreTrainedModel, batch: TokenizedBatch) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=batch.input_ids.to(DEVICE),\n",
        "            attention_mask=batch.attention_mask.to(DEVICE)\n",
        "        )\n",
        "    logits = outputs.logits[:, :-1]\n",
        "    labels = batch.input_ids[:, 1:].to(DEVICE)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    chosen = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "    return chosen.cpu()\n",
        "\n",
        "\n",
        "def compute_values(model: PreTrainedModel, batch: TokenizedBatch) -> torch.Tensor:\n",
        "    logprobs = compute_logprobs(model, batch)\n",
        "    token_counts = batch.attention_mask[:, 1:].sum(dim=-1, keepdim=True)\n",
        "    return logprobs.sum(dim=-1, keepdim=True) / token_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dca85db",
      "metadata": {},
      "source": [
        "### Reward shaping for verifiable domains\n",
        "\n",
        "To keep the demo self-contained, we simulate a verifiable task: the response must include an explicit `'Answer:'` tag followed by a number that matches a deterministic scoring function. Real projects would plug in a programmatic verifier such as a unit test harness for code or a math proof checker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3feed824",
      "metadata": {},
      "outputs": [],
      "source": [
        "def synthetic_verifier(prompt: str, response: str) -> float:\n",
        "    target = 0\n",
        "    for token in prompt.split():\n",
        "        if token.isdigit():\n",
        "            target += int(token)\n",
        "    reward = -1.0\n",
        "    if 'Answer:' in response:\n",
        "        try:\n",
        "            prediction = int(response.split('Answer:')[1].split()[0])\n",
        "            reward = 1.0 if prediction == target else -0.2\n",
        "        except (ValueError, IndexError):\n",
        "            reward = -0.5\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cf8a12",
      "metadata": {},
      "source": [
        "### Collecting PPO rollouts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9d066e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(model: PreTrainedModel, prompt: str, config: PPOConfig) -> str:\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(DEVICE)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=config.max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    generated = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return generated.strip()\n",
        "\n",
        "\n",
        "def collect_rollouts(policy: PreTrainedModel, ref_model: PreTrainedModel, dataset: Dataset, config: PPOConfig, num_rollouts: int) -> List[PPORollout]:\n",
        "    rollouts: List[PPORollout] = []\n",
        "    for example in dataset.shuffle(seed=42).select(range(num_rollouts)):\n",
        "        prompt = example['prompt']\n",
        "        response = generate_response(policy, prompt, config)\n",
        "        tokenized = tokenize_batch([prompt], [response])\n",
        "        logprobs = compute_logprobs(policy, tokenized)[0]\n",
        "        ref_logprobs = compute_logprobs(ref_model, tokenized)[0]\n",
        "        values = compute_values(policy, tokenized)[0]\n",
        "        reward = synthetic_verifier(prompt, response)\n",
        "        rollout = PPORollout(\n",
        "            prompt=prompt,\n",
        "            response=response,\n",
        "            reward=reward,\n",
        "            logprobs=logprobs,\n",
        "            ref_logprobs=ref_logprobs,\n",
        "            values=values,\n",
        "            masks=~tokenized.prompt_mask[:, 1:][0],\n",
        "        )\n",
        "        rollouts.append(rollout)\n",
        "    return rollouts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8c4572",
      "metadata": {},
      "source": [
        "### PPO loss computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4f1f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ppo_loss(policy: PreTrainedModel, rollouts: List[PPORollout], config: PPOConfig) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "    stacked = stack_rollouts(rollouts)\n",
        "    advantages = stacked['rewards'].unsqueeze(-1) - stacked['values'].squeeze(-1)\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "    batch = tokenize_batch(\n",
        "        [r.prompt for r in rollouts],\n",
        "        [r.response for r in rollouts],\n",
        "    )\n",
        "    outputs = policy(\n",
        "        input_ids=batch.input_ids.to(DEVICE),\n",
        "        attention_mask=batch.attention_mask.to(DEVICE)\n",
        "    )\n",
        "    logits = outputs.logits[:, :-1]\n",
        "    labels = batch.input_ids[:, 1:].to(DEVICE)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    chosen = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "    old_logprobs = stacked['logprobs'].to(DEVICE)\n",
        "    logratio = chosen - old_logprobs\n",
        "    ratio = logratio.exp()\n",
        "    unclipped = ratio * advantages.to(DEVICE)\n",
        "    clipped = torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range) * advantages.to(DEVICE)\n",
        "    mask = stacked['masks'].to(DEVICE)\n",
        "    policy_loss = -(torch.min(unclipped, clipped) * mask).sum() / mask.sum()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_logprobs = stacked['ref_logprobs'].to(DEVICE)\n",
        "    kl = ((old_logprobs - ref_logprobs) * mask).sum() / mask.sum()\n",
        "\n",
        "    value_loss = F.mse_loss(stacked['values'].to(DEVICE).squeeze(-1), stacked['rewards'].to(DEVICE))\n",
        "    entropy = -(ratio * logratio * mask).sum() / mask.sum()\n",
        "    total_loss = policy_loss + config.vf_coef * value_loss - config.ent_coef * entropy + config.kl_coef * kl\n",
        "\n",
        "    metrics = {\n",
        "        'policy_loss': policy_loss.item(),\n",
        "        'value_loss': value_loss.item(),\n",
        "        'kl': kl.item(),\n",
        "        'entropy': entropy.item(),\n",
        "    }\n",
        "    return total_loss, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52805f82",
      "metadata": {},
      "source": [
        "### PPO training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9909a39d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ppo(dataset: Dataset, config: PPOConfig) -> Tuple[PreTrainedModel, List[Dict[str, float]]]:\n",
        "    policy, ref_model = build_models()\n",
        "    optimizer = torch.optim.AdamW(policy.parameters(), lr=5e-6)\n",
        "    all_metrics: List[Dict[str, float]] = []\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        rollouts = collect_rollouts(policy, ref_model, dataset, config, config.batch_size)\n",
        "        loss, metrics = ppo_loss(policy, rollouts, config)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        metrics['epoch'] = epoch\n",
        "        all_metrics.append(metrics)\n",
        "        print(f'Epoch {epoch}: {metrics}')\n",
        "    return policy, all_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c1a1816",
      "metadata": {},
      "source": [
        "Even a single PPO epoch is computationally demanding because it needs to generate fresh samples. For larger experiments you would:\n",
        "\n",
        "* Use a separate **reward model** instead of the synthetic verifier.\n",
        "* Accumulate rollouts across multiple gradient steps.\n",
        "* Monitor the KL divergence to adjust `kl_coef` adaptively.\n",
        "\n",
        "Next we upgrade the algorithm to GRPO.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1337376",
      "metadata": {},
      "source": [
        "## Implementing GRPO\n",
        "\n",
        "Group Relative Preference Optimization (GRPO) modifies PPO by optimizing the policy against *grouped* preferences. For each prompt we draw several candidate completions, score them with the verifier, and compute advantages relative to the group statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a167ce52",
      "metadata": {},
      "source": [
        "### GRPO utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d114874",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GRPOConfig(PPOConfig):\n",
        "    group_size: int = 4\n",
        "\n",
        "\n",
        "def collect_group_rollouts(policy: PreTrainedModel, ref_model: PreTrainedModel, dataset: Dataset, config: GRPOConfig, num_groups: int) -> List[List[PPORollout]]:\n",
        "    groups: List[List[PPORollout]] = []\n",
        "    for example in dataset.shuffle(seed=1337).select(range(num_groups)):\n",
        "        prompt = example['prompt']\n",
        "        group: List[PPORollout] = []\n",
        "        for _ in range(config.group_size):\n",
        "            response = generate_response(policy, prompt, config)\n",
        "            tokenized = tokenize_batch([prompt], [response])\n",
        "            logprobs = compute_logprobs(policy, tokenized)[0]\n",
        "            ref_logprobs = compute_logprobs(ref_model, tokenized)[0]\n",
        "            values = compute_values(policy, tokenized)[0]\n",
        "            reward = synthetic_verifier(prompt, response)\n",
        "            rollout = PPORollout(\n",
        "                prompt=prompt,\n",
        "                response=response,\n",
        "                reward=reward,\n",
        "                logprobs=logprobs,\n",
        "                ref_logprobs=ref_logprobs,\n",
        "                values=values,\n",
        "                masks=~tokenized.prompt_mask[:, 1:][0],\n",
        "            )\n",
        "            group.append(rollout)\n",
        "        groups.append(group)\n",
        "    return groups\n",
        "\n",
        "\n",
        "def grpo_group_advantages(group: List[PPORollout]) -> torch.Tensor:\n",
        "    rewards = torch.tensor([r.reward for r in group], dtype=torch.float)\n",
        "    baseline = rewards.mean()\n",
        "    advantages = rewards - baseline\n",
        "    return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ded5a5",
      "metadata": {},
      "source": [
        "### GRPO loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02979bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def grpo_loss(policy: PreTrainedModel, groups: List[List[PPORollout]], config: GRPOConfig) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "    total_loss = torch.tensor(0.0, device=DEVICE)\n",
        "    metrics = {'policy_loss': 0.0, 'kl': 0.0, 'entropy': 0.0}\n",
        "    total_tokens = 0\n",
        "\n",
        "    for group in groups:\n",
        "        advantages = grpo_group_advantages(group).to(DEVICE)\n",
        "        batch = tokenize_batch([r.prompt for r in group], [r.response for r in group])\n",
        "        outputs = policy(\n",
        "            input_ids=batch.input_ids.to(DEVICE),\n",
        "            attention_mask=batch.attention_mask.to(DEVICE)\n",
        "        )\n",
        "        logits = outputs.logits[:, :-1]\n",
        "        labels = batch.input_ids[:, 1:].to(DEVICE)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        chosen = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        old_logprobs = torch.stack([r.logprobs for r in group]).to(DEVICE)\n",
        "        logratio = chosen - old_logprobs\n",
        "        ratio = logratio.exp()\n",
        "        mask = torch.stack([r.masks for r in group]).to(DEVICE)\n",
        "\n",
        "        policy_loss = -((ratio * advantages.unsqueeze(-1)) * mask).sum() / mask.sum()\n",
        "        entropy = -(ratio * logratio * mask).sum() / mask.sum()\n",
        "\n",
        "        ref_logprobs = torch.stack([r.ref_logprobs for r in group]).to(DEVICE)\n",
        "        kl = ((old_logprobs - ref_logprobs) * mask).sum() / mask.sum()\n",
        "\n",
        "        group_loss = policy_loss - config.ent_coef * entropy + config.kl_coef * kl\n",
        "        total_loss = total_loss + group_loss\n",
        "        total_tokens += mask.sum().item()\n",
        "        metrics['policy_loss'] += policy_loss.item()\n",
        "        metrics['kl'] += kl.item()\n",
        "        metrics['entropy'] += entropy.item()\n",
        "\n",
        "    total_loss = total_loss / len(groups)\n",
        "    for key in metrics:\n",
        "        metrics[key] /= len(groups)\n",
        "    metrics['tokens_per_step'] = total_tokens / len(groups)\n",
        "    return total_loss, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186a9d29",
      "metadata": {},
      "source": [
        "### GRPO training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d80cabd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_grpo(dataset: Dataset, config: GRPOConfig) -> Tuple[PreTrainedModel, List[Dict[str, float]]]:\n",
        "    policy, ref_model = build_models()\n",
        "    optimizer = torch.optim.AdamW(policy.parameters(), lr=5e-6)\n",
        "    history: List[Dict[str, float]] = []\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        groups = collect_group_rollouts(policy, ref_model, dataset, config, num_groups=config.batch_size)\n",
        "        loss, metrics = grpo_loss(policy, groups, config)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        metrics['epoch'] = epoch\n",
        "        history.append(metrics)\n",
        "        print(f'Epoch {epoch}: {metrics}')\n",
        "    return policy, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddef9ace",
      "metadata": {},
      "source": [
        "## Evaluation and verification\n",
        "\n",
        "In verifiable domains you can often rely on automated scoring, which greatly simplifies evaluation compared with subjective tasks. Consider:\n",
        "\n",
        "* **Hold-out prompts** with deterministic solutions, allowing exact accuracy measurement.\n",
        "* **Programmatic validators** that can execute generated code, check unit tests, or validate mathematical proofs.\n",
        "* **Safety filters** that refuse to answer when verification fails or the prompt is unsafe.\n",
        "\n",
        "Below is a simple evaluation helper that reuses the synthetic verifier but can be swapped out for richer test harnesses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0356a16a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model: PreTrainedModel, prompts: Iterable[str], verifier: Callable[[str, str], float], config: PPOConfig) -> Dict[str, float]:\n",
        "    rewards = []\n",
        "    for prompt in prompts:\n",
        "        response = generate_response(model, prompt, config)\n",
        "        rewards.append(verifier(prompt, response))\n",
        "    return {\n",
        "        'avg_reward': sum(rewards) / len(rewards),\n",
        "        'success_rate': sum(r > 0 for r in rewards) / len(rewards),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff8e052",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "* Replace the synthetic verifier with a domain-specific scoring function.\n",
        "* Fine-tune a reward model on the `chosen` vs. `rejected` responses to approximate human preferences when automation is impossible.\n",
        "* Scale up batch sizes, gradient accumulation, and model sizes using distributed training utilities.\n",
        "* Track metrics like KL divergence, entropy, and accuracy to maintain alignment with the reference policy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
