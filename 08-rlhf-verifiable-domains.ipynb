{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f2283aa4",
      "metadata": {},
      "source": "# 08 \u00b7 Reinforcement Learning from Human Feedback in Verifiable Domains\n\nThis notebook explores how to fine-tune large language models with reinforcement learning in domains where we can automatically verify model responses. We begin with **Proximal Policy Optimization (PPO)** and then extend the same ingredients to **Group Relative Preference Optimization (GRPO)** while working with the public [Anthropic HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf) dataset of human preference pairs.\n"
    },
    {
      "cell_type": "markdown",
      "id": "e1d8b9e5",
      "metadata": {},
      "source": "## Learning goals\n\n* Load and inspect the Anthropic human preference dataset.\n* Build a minimal pipeline for PPO that can optimize a policy model against automatically computed rewards.\n* Adapt the PPO implementation into GRPO to take advantage of grouped preference data.\n* Discuss evaluation considerations for verifiable domains, such as math or factual QA.\n"
    },
    {
      "cell_type": "markdown",
      "id": "0e84633d",
      "metadata": {},
      "source": "> **Note:** This notebook focuses on presenting a *reference implementation*. The code is intended to run on small models such as `distilgpt2` for demonstration, but you should expect to adjust batch sizes and precision if you train longer or on larger checkpoints.\n"
    },
    {
      "cell_type": "markdown",
      "id": "43cd2690",
      "metadata": {},
      "source": "## Environment setup\n\nWe rely on the Hugging Face ecosystem for models and datasets, and on PyTorch for automatic differentiation.\n"
    },
    {
      "cell_type": "code",
      "id": "91f7278b",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import math\nimport random\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Callable, Dict, Iterable, List, Optional, Tuple\n\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          DataCollatorForLanguageModeling,\n                          PreTrainedModel, PreTrainedTokenizerBase)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {DEVICE}')\n"
    },
    {
      "cell_type": "markdown",
      "id": "9d57c403",
      "metadata": {},
      "source": "### Loading Anthropic preference pairs\n\nThe Anthropic HH-RLHF dataset contains paired responses (`chosen` vs. `rejected`) for multi-turn conversations. We'll downsample aggressively to keep the demo light-weight.\n"
    },
    {
      "cell_type": "code",
      "id": "b7ebe343",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "dataset = load_dataset('Anthropic/hh-rlhf', split='train')\nprint(dataset)\n\ndef format_pair(example: Dict[str, str]) -> Dict[str, str]:\n    prompt = example['prompt'].strip()\n    chosen = example['chosen'].strip()\n    rejected = example['rejected'].strip()\n    return {\n        'prompt': prompt,\n        'chosen': chosen,\n        'rejected': rejected,\n    }\n\ndataset = dataset.map(format_pair)\nsmall_dataset = dataset.select(range(256))\nsmall_dataset[0]\n"
    },
    {
      "cell_type": "markdown",
      "id": "0a2936f3",
      "metadata": {},
      "source": "### Tokenization utilities\n\nWe operate on concatenated prompt+response strings and keep track of the prompt length so we can mask out the log-probabilities that correspond to the prompt tokens during policy optimization.\n"
    },
    {
      "cell_type": "code",
      "id": "a207b60d",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "TOKENIZER_NAME = 'distilgpt2'\ntokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\n@dataclass\nclass TokenizedBatch:\n    input_ids: torch.LongTensor\n    attention_mask: torch.LongTensor\n    prompt_mask: torch.BoolTensor\n\n\ndef tokenize_batch(prompts: List[str], responses: List[str]) -> TokenizedBatch:\n    combined = [prompts[i] + responses[i] for i in range(len(prompts))]\n    enc = tokenizer(combined, padding=True, return_tensors='pt')\n    prompt_lens = torch.tensor(\n        [len(tokenizer(p).input_ids) for p in prompts], dtype=torch.long\n    )\n    prompt_mask = torch.arange(enc.input_ids.size(1)).unsqueeze(0) < prompt_lens.unsqueeze(1)\n    return TokenizedBatch(\n        input_ids=enc.input_ids,\n        attention_mask=enc.attention_mask,\n        prompt_mask=prompt_mask,\n    )\n"
    },
    {
      "cell_type": "markdown",
      "id": "265d8989",
      "metadata": {},
      "source": "## Implementing PPO\n\nPPO optimizes a policy by constraining updates to remain close to the previous policy through a clipped surrogate loss. For language models we operate in log-probability space and mask the prompt tokens because they are provided by the environment.\n"
    },
    {
      "cell_type": "markdown",
      "id": "469d5e29",
      "metadata": {},
      "source": "### Configuration dataclasses\n"
    },
    {
      "cell_type": "code",
      "id": "3244c386",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass PPOConfig:\n    kl_coef: float = 0.1\n    clip_range: float = 0.2\n    vf_coef: float = 0.1\n    ent_coef: float = 0.01\n    target_kl: float = 0.1\n    num_epochs: int = 1\n    batch_size: int = 4\n    mini_batch_size: int = 2\n    max_new_tokens: int = 64\n"
    },
    {
      "cell_type": "markdown",
      "id": "d36f1d1e",
      "metadata": {},
      "source": "### Storage for rollouts\n"
    },
    {
      "cell_type": "code",
      "id": "2e49b74b",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass PPORollout:\n    prompt: str\n    response: str\n    reward: float\n    logprobs: torch.Tensor\n    ref_logprobs: torch.Tensor\n    values: torch.Tensor\n    masks: torch.BoolTensor\n\n\ndef stack_rollouts(rollouts: List[PPORollout]) -> Dict[str, torch.Tensor]:\n    return {\n        'logprobs': torch.stack([r.logprobs for r in rollouts]),\n        'ref_logprobs': torch.stack([r.ref_logprobs for r in rollouts]),\n        'values': torch.stack([r.values for r in rollouts]),\n        'masks': torch.stack([r.masks for r in rollouts]),\n        'rewards': torch.tensor([r.reward for r in rollouts], dtype=torch.float),\n    }\n"
    },
    {
      "cell_type": "markdown",
      "id": "d283971c",
      "metadata": {},
      "source": "### Policy and value helper functions\n"
    },
    {
      "cell_type": "code",
      "id": "e60d2311",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_models(model_name: str = TOKENIZER_NAME) -> Tuple[PreTrainedModel, PreTrainedModel]:\n    policy = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n    ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n    return policy, ref_model\n\n\ndef compute_logprobs(model: PreTrainedModel, batch: TokenizedBatch) -> torch.Tensor:\n    with torch.no_grad():\n        outputs = model(\n            input_ids=batch.input_ids.to(DEVICE),\n            attention_mask=batch.attention_mask.to(DEVICE)\n        )\n    logits = outputs.logits[:, :-1]\n    labels = batch.input_ids[:, 1:].to(DEVICE)\n    log_probs = F.log_softmax(logits, dim=-1)\n    chosen = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n    return chosen.cpu()\n\n\ndef compute_values(model: PreTrainedModel, batch: TokenizedBatch) -> torch.Tensor:\n    logprobs = compute_logprobs(model, batch)\n    token_counts = batch.attention_mask[:, 1:].sum(dim=-1, keepdim=True)\n    return logprobs.sum(dim=-1, keepdim=True) / token_counts\n"
    },
    {
      "cell_type": "markdown",
      "id": "8dca85db",
      "metadata": {},
      "source": "### Reward shaping for verifiable domains\n\nTo keep the demo self-contained, we simulate a verifiable task: the response must include an explicit `'Answer:'` tag followed by a number that matches a deterministic scoring function. Real projects would plug in a programmatic verifier such as a unit test harness for code or a math proof checker.\n"
    },
    {
      "cell_type": "code",
      "id": "3feed824",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def synthetic_verifier(prompt: str, response: str) -> float:\n    target = 0\n    for token in prompt.split():\n        if token.isdigit():\n            target += int(token)\n    reward = -1.0\n    if 'Answer:' in response:\n        try:\n            prediction = int(response.split('Answer:')[1].split()[0])\n            reward = 1.0 if prediction == target else -0.2\n        except (ValueError, IndexError):\n            reward = -0.5\n    return reward\n"
    },
    {
      "cell_type": "markdown",
      "id": "f6cf8a12",
      "metadata": {},
      "source": "### Collecting PPO rollouts\n"
    },
    {
      "cell_type": "code",
      "id": "7b9d066e",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def generate_response(model: PreTrainedModel, prompt: str, config: PPOConfig) -> str:\n    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(DEVICE)\n    output = model.generate(\n        input_ids,\n        max_new_tokens=config.max_new_tokens,\n        do_sample=True,\n        top_p=0.95,\n        temperature=0.7,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    generated = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n    return generated.strip()\n\n\ndef collect_rollouts(policy: PreTrainedModel, ref_model: PreTrainedModel, dataset: Dataset, config: PPOConfig, num_rollouts: int) -> List[PPORollout]:\n    rollouts: List[PPORollout] = []\n    for example in dataset.shuffle(seed=42).select(range(num_rollouts)):\n        prompt = example['prompt']\n        response = generate_response(policy, prompt, config)\n        tokenized = tokenize_batch([prompt], [response])\n        logprobs = compute_logprobs(policy, tokenized)[0]\n        ref_logprobs = compute_logprobs(ref_model, tokenized)[0]\n        values = compute_values(policy, tokenized)[0]\n        reward = synthetic_verifier(prompt, response)\n        rollout = PPORollout(\n            prompt=prompt,\n            response=response,\n            reward=reward,\n            logprobs=logprobs,\n            ref_logprobs=ref_logprobs,\n            values=values,\n            masks=~tokenized.prompt_mask[:, 1:][0],\n        )\n        rollouts.append(rollout)\n    return rollouts\n"
    },
    {
      "cell_type": "markdown",
      "id": "7c8c4572",
      "metadata": {},
      "source": "### PPO loss computation\n"
    },
    {
      "cell_type": "code",
      "id": "aa4f1f16",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def ppo_loss(policy: PreTrainedModel, rollouts: List[PPORollout], config: PPOConfig) -> Tuple[torch.Tensor, Dict[str, float]]:\n    stacked = stack_rollouts(rollouts)\n    advantages = stacked['rewards'].unsqueeze(-1) - stacked['values'].squeeze(-1)\n    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n    batch = tokenize_batch(\n        [r.prompt for r in rollouts],\n        [r.response for r in rollouts],\n    )\n    outputs = policy(\n        input_ids=batch.input_ids.to(DEVICE),\n        attention_mask=batch.attention_mask.to(DEVICE)\n    )\n    logits = outputs.logits[:, :-1]\n    labels = batch.input_ids[:, 1:].to(DEVICE)\n    log_probs = F.log_softmax(logits, dim=-1)\n    chosen = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n    old_logprobs = stacked['logprobs'].to(DEVICE)\n    logratio = chosen - old_logprobs\n    ratio = logratio.exp()\n    unclipped = ratio * advantages.to(DEVICE)\n    clipped = torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range) * advantages.to(DEVICE)\n    mask = stacked['masks'].to(DEVICE)\n    policy_loss = -(torch.min(unclipped, clipped) * mask).sum() / mask.sum()\n\n    with torch.no_grad():\n        ref_logprobs = stacked['ref_logprobs'].to(DEVICE)\n    kl = ((old_logprobs - ref_logprobs) * mask).sum() / mask.sum()\n\n    value_loss = F.mse_loss(stacked['values'].to(DEVICE).squeeze(-1), stacked['rewards'].to(DEVICE))\n    entropy = -(ratio * logratio * mask).sum() / mask.sum()\n    total_loss = policy_loss + config.vf_coef * value_loss - config.ent_coef * entropy + config.kl_coef * kl\n\n    metrics = {\n        'policy_loss': policy_loss.item(),\n        'value_loss': value_loss.item(),\n        'kl': kl.item(),\n        'entropy': entropy.item(),\n    }\n    return total_loss, metrics\n"
    },
    {
      "cell_type": "markdown",
      "id": "52805f82",
      "metadata": {},
      "source": "### PPO training loop\n"
    },
    {
      "cell_type": "code",
      "id": "9909a39d",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def train_ppo(dataset: Dataset, config: PPOConfig) -> Tuple[PreTrainedModel, List[Dict[str, float]]]:\n    policy, ref_model = build_models()\n    optimizer = torch.optim.AdamW(policy.parameters(), lr=5e-6)\n    all_metrics: List[Dict[str, float]] = []\n\n    for epoch in range(config.num_epochs):\n        rollouts = collect_rollouts(policy, ref_model, dataset, config, config.batch_size)\n        loss, metrics = ppo_loss(policy, rollouts, config)\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n        optimizer.step()\n        metrics['epoch'] = epoch\n        all_metrics.append(metrics)\n        print(f'Epoch {epoch}: {metrics}')\n    return policy, all_metrics\n"
    },
    {
      "cell_type": "markdown",
      "id": "3c1a1816",
      "metadata": {},
      "source": "Even a single PPO epoch is computationally demanding because it needs to generate fresh samples. For larger experiments you would:\n\n* Use a separate **reward model** instead of the synthetic verifier.\n* Accumulate rollouts across multiple gradient steps.\n* Monitor the KL divergence to adjust `kl_coef` adaptively.\n\nNext we upgrade the algorithm to GRPO.\n"
    },
    {
      "cell_type": "markdown",
      "id": "d1337376",
      "metadata": {},
      "source": "## Implementing GRPO\n\nGroup Relative Preference Optimization (GRPO) modifies PPO by optimizing the policy against *grouped* preferences. For each prompt we draw several candidate completions, score them with the verifier, and compute advantages relative to the group statistics.\n"
    },
    {
      "cell_type": "markdown",
      "id": "a167ce52",
      "metadata": {},
      "source": "### GRPO utilities\n"
    },
    {
      "cell_type": "code",
      "id": "8d114874",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@dataclass\nclass GRPOConfig(PPOConfig):\n    group_size: int = 4\n\n\ndef collect_group_rollouts(policy: PreTrainedModel, ref_model: PreTrainedModel, dataset: Dataset, config: GRPOConfig, num_groups: int) -> List[List[PPORollout]]:\n    groups: List[List[PPORollout]] = []\n    for example in dataset.shuffle(seed=1337).select(range(num_groups)):\n        prompt = example['prompt']\n        group: List[PPORollout] = []\n        for _ in range(config.group_size):\n            response = generate_response(policy, prompt, config)\n            tokenized = tokenize_batch([prompt], [response])\n            logprobs = compute_logprobs(policy, tokenized)[0]\n            ref_logprobs = compute_logprobs(ref_model, tokenized)[0]\n            values = compute_values(policy, tokenized)[0]\n            reward = synthetic_verifier(prompt, response)\n            rollout = PPORollout(\n                prompt=prompt,\n                response=response,\n                reward=reward,\n                logprobs=logprobs,\n                ref_logprobs=ref_logprobs,\n                values=values,\n                masks=~tokenized.prompt_mask[:, 1:][0],\n            )\n            group.append(rollout)\n        groups.append(group)\n    return groups\n\n\ndef grpo_group_advantages(group: List[PPORollout]) -> torch.Tensor:\n    rewards = torch.tensor([r.reward for r in group], dtype=torch.float)\n    baseline = rewards.mean()\n    advantages = rewards - baseline\n    return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n"
    },
    {
      "cell_type": "markdown",
      "id": "f9ded5a5",
      "metadata": {},
      "source": "### GRPO loss\n"
    },
    {
      "cell_type": "code",
      "id": "d02979bc",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def grpo_loss(policy: PreTrainedModel, groups: List[List[PPORollout]], config: GRPOConfig) -> Tuple[torch.Tensor, Dict[str, float]]:\n    total_loss = torch.tensor(0.0, device=DEVICE)\n    metrics = {'policy_loss': 0.0, 'kl': 0.0, 'entropy': 0.0}\n    total_tokens = 0\n\n    for group in groups:\n        advantages = grpo_group_advantages(group).to(DEVICE)\n        batch = tokenize_batch([r.prompt for r in group], [r.response for r in group])\n        outputs = policy(\n            input_ids=batch.input_ids.to(DEVICE),\n            attention_mask=batch.attention_mask.to(DEVICE)\n        )\n        logits = outputs.logits[:, :-1]\n        labels = batch.input_ids[:, 1:].to(DEVICE)\n        log_probs = F.log_softmax(logits, dim=-1)\n        chosen = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n\n        old_logprobs = torch.stack([r.logprobs for r in group]).to(DEVICE)\n        logratio = chosen - old_logprobs\n        ratio = logratio.exp()\n        mask = torch.stack([r.masks for r in group]).to(DEVICE)\n\n        policy_loss = -((ratio * advantages.unsqueeze(-1)) * mask).sum() / mask.sum()\n        entropy = -(ratio * logratio * mask).sum() / mask.sum()\n\n        ref_logprobs = torch.stack([r.ref_logprobs for r in group]).to(DEVICE)\n        kl = ((old_logprobs - ref_logprobs) * mask).sum() / mask.sum()\n\n        group_loss = policy_loss - config.ent_coef * entropy + config.kl_coef * kl\n        total_loss = total_loss + group_loss\n        total_tokens += mask.sum().item()\n        metrics['policy_loss'] += policy_loss.item()\n        metrics['kl'] += kl.item()\n        metrics['entropy'] += entropy.item()\n\n    total_loss = total_loss / len(groups)\n    for key in metrics:\n        metrics[key] /= len(groups)\n    metrics['tokens_per_step'] = total_tokens / len(groups)\n    return total_loss, metrics\n"
    },
    {
      "cell_type": "markdown",
      "id": "186a9d29",
      "metadata": {},
      "source": "### GRPO training loop\n"
    },
    {
      "cell_type": "code",
      "id": "d80cabd5",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def train_grpo(dataset: Dataset, config: GRPOConfig) -> Tuple[PreTrainedModel, List[Dict[str, float]]]:\n    policy, ref_model = build_models()\n    optimizer = torch.optim.AdamW(policy.parameters(), lr=5e-6)\n    history: List[Dict[str, float]] = []\n\n    for epoch in range(config.num_epochs):\n        groups = collect_group_rollouts(policy, ref_model, dataset, config, num_groups=config.batch_size)\n        loss, metrics = grpo_loss(policy, groups, config)\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n        optimizer.step()\n        metrics['epoch'] = epoch\n        history.append(metrics)\n        print(f'Epoch {epoch}: {metrics}')\n    return policy, history\n"
    },
    {
      "cell_type": "markdown",
      "id": "ddef9ace",
      "metadata": {},
      "source": "## Evaluation and verification\n\nIn verifiable domains you can often rely on automated scoring, which greatly simplifies evaluation compared with subjective tasks. Consider:\n\n* **Hold-out prompts** with deterministic solutions, allowing exact accuracy measurement.\n* **Programmatic validators** that can execute generated code, check unit tests, or validate mathematical proofs.\n* **Safety filters** that refuse to answer when verification fails or the prompt is unsafe.\n\nBelow is a simple evaluation helper that reuses the synthetic verifier but can be swapped out for richer test harnesses.\n"
    },
    {
      "cell_type": "code",
      "id": "0356a16a",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def evaluate_model(model: PreTrainedModel, prompts: Iterable[str], verifier: Callable[[str, str], float], config: PPOConfig) -> Dict[str, float]:\n    rewards = []\n    for prompt in prompts:\n        response = generate_response(model, prompt, config)\n        rewards.append(verifier(prompt, response))\n    return {\n        'avg_reward': sum(rewards) / len(rewards),\n        'success_rate': sum(r > 0 for r in rewards) / len(rewards),\n    }\n"
    },
    {
      "cell_type": "markdown",
      "id": "cff8e052",
      "metadata": {},
      "source": "## Next steps\n\n* Replace the synthetic verifier with a domain-specific scoring function.\n* Fine-tune a reward model on the `chosen` vs. `rejected` responses to approximate human preferences when automation is impossible.\n* Scale up batch sizes, gradient accumulation, and model sizes using distributed training utilities.\n* Track metrics like KL divergence, entropy, and accuracy to maintain alignment with the reference policy.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}